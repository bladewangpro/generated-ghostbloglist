<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=theme-color content="#FFFFFF"><meta http-equiv=x-ua-compatible content="IE=edge"><title>神经网络反向传播算法推导 | prometheus</title><meta name=description content="Explore in every moment of the hard thinking"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="神经网络反向传播算法推导"><meta property="og:description" content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Learning.ipynb
 一. Cost Function and Backpropagation 1. Cost Function 假设训练集中有 m 个训练样本，$\begin{Bmatrix} (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \cdots ,(x^{(m)},y^{(m)}) \end{Bmatrix}$，L 表示神经网络的总层数 Layer，用 $S_{l}$ 表示第 L 层的单元数(神经元的数量)，但是不包括第 L 层的偏差单元(常数项)。令 K 为输出层的单元数目，即 最后一层的单元数。
符号约定：
$z_i^{(j)}$ = 第 $j$ 层的第 $i$ 个节点（神经元）的“计算值” $a_i^{(j)}$ = 第 $j$ 层的第 $i$ 个节点（神经元）的“激活值” $\Theta^{(l)}{i,j}$ = 映射第 $l$ 层到第 $l+1$ 层的权值矩阵的第 $i$ 行第 $j$ 列的分量 $L$ = 神经网络总层数（包括输入层、隐层和输出层） $s_l$ = 第 $l$ 层节点（神经元）个数，不包括偏移量节点。 $K$ = 输出节点个数 $h{\theta}(x)_k$ = 第 $k$ 个预测输出结果 $x^{(i)}$ = 第 $i$ 个样本特征向量 $x^{(i)}_k$ = 第 $i$ 个样本的第 $k$ 个特征值 $y^{(i)}$ = 第 $i$ 个样本实际结果向量"><meta property="og:type" content="article"><meta property="og:url" content="https://new.halfrost.com/neural_networks_learning/"><meta property="article:published_time" content="2018-03-25T08:33:00+00:00"><meta property="article:modified_time" content="2018-03-25T08:33:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="神经网络反向传播算法推导"><meta name=twitter:description content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Learning.ipynb
 一. Cost Function and Backpropagation 1. Cost Function 假设训练集中有 m 个训练样本，$\begin{Bmatrix} (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \cdots ,(x^{(m)},y^{(m)}) \end{Bmatrix}$，L 表示神经网络的总层数 Layer，用 $S_{l}$ 表示第 L 层的单元数(神经元的数量)，但是不包括第 L 层的偏差单元(常数项)。令 K 为输出层的单元数目，即 最后一层的单元数。
符号约定：
$z_i^{(j)}$ = 第 $j$ 层的第 $i$ 个节点（神经元）的“计算值” $a_i^{(j)}$ = 第 $j$ 层的第 $i$ 个节点（神经元）的“激活值” $\Theta^{(l)}{i,j}$ = 映射第 $l$ 层到第 $l+1$ 层的权值矩阵的第 $i$ 行第 $j$ 列的分量 $L$ = 神经网络总层数（包括输入层、隐层和输出层） $s_l$ = 第 $l$ 层节点（神经元）个数，不包括偏移量节点。 $K$ = 输出节点个数 $h{\theta}(x)_k$ = 第 $k$ 个预测输出结果 $x^{(i)}$ = 第 $i$ 个样本特征向量 $x^{(i)}_k$ = 第 $i$ 个样本的第 $k$ 个特征值 $y^{(i)}$ = 第 $i$ 个样本实际结果向量"><link rel=stylesheet href=/css/style-white.min.css><link rel=manifest href=/manifest.json><link rel=stylesheet href=/prism.css><link href=/images/apple-touch-icon-60x60.png rel=apple-touch-icon sizes=60x60><link href=/images/apple-touch-icon-76x76.png rel=apple-touch-icon sizes=76x76><link href=/images/apple-touch-icon-120x120.png rel=apple-touch-icon sizes=120x120><link href=/images/apple-touch-icon-152x152.png rel=apple-touch-icon sizes=152x152><link href=/images/apple-touch-icon-180x180.png rel=apple-touch-icon sizes=180x180><link href=/images/apple-touch-icon-512x512.png rel=apple-touch-icon sizes=512x512><link href=/images/apple-touch-icon-1024x1024.png rel=apple-touch-icon sizes=1024x1024><script async>if('serviceWorker'in navigator){navigator.serviceWorker.register("\/serviceworker-v1.min.a64912b78d282eab1ad3715a0943da21616e5f326f8afea27034784ad445043b.js").then(function(){if(navigator.serviceWorker.controller){console.log('Assets cached by the controlling service worker.');}else{console.log('Please reload this page to allow the service worker to handle network operations.');}}).catch(function(error){console.log('ERROR: '+error);});}else{console.log('Service workers are not supported in the current browser.');}</script><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://new.halfrost.com/images/favicon.ico><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-82753806-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class="single-max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a><a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a><a id=top-icon-tablet href=# onclick="$('html, body').animate({scrollTop:0},'fast');" style=display:none><i class="fas fa-chevron-up fa-lg"></i></a><span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://new.halfrost.com/neural_networks_representation/><i class="fas fa-chevron-left" aria-hidden=true onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li><li><a class=icon href=https://new.halfrost.com/backpropagation_in_practice/><i class="fas fa-chevron-right" aria-hidden=true onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li><li><a class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up" aria-hidden=true onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li><li><a class=icon href=#><i class="fas fa-share-alt" aria-hidden=true onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f"><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&text=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&is_video=false&description=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f"><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-stumbleupon" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-digg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&name=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fNeural_Networks_Learning.ipynb%0a%20%e4%b8%80.%20Cost%20Function%20and%20Backpropagation%201.%20Cost%20Function%20%e5%81%87%e8%ae%be%e8%ae%ad%e7%bb%83%e9%9b%86%e4%b8%ad%e6%9c%89%20m%20%e4%b8%aa%e8%ae%ad%e7%bb%83%e6%a0%b7%e6%9c%ac%ef%bc%8c%24%5cbegin%7bBmatrix%7d%20%28x%5e%7b%281%29%7d%2cy%5e%7b%281%29%7d%29%2c%28x%5e%7b%282%29%7d%2cy%5e%7b%282%29%7d%29%2c%20%5ccdots%20%2c%28x%5e%7b%28m%29%7d%2cy%5e%7b%28m%29%7d%29%20%5cend%7bBmatrix%7d%24%ef%bc%8cL%20%e8%a1%a8%e7%a4%ba%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e6%80%bb%e5%b1%82%e6%95%b0%20Layer%ef%bc%8c%e7%94%a8%20%24S_%7bl%7d%24%20%e8%a1%a8%e7%a4%ba%e7%ac%ac%20L%20%e5%b1%82%e7%9a%84%e5%8d%95%e5%85%83%e6%95%b0%28%e7%a5%9e%e7%bb%8f%e5%85%83%e7%9a%84%e6%95%b0%e9%87%8f%29%ef%bc%8c%e4%bd%86%e6%98%af%e4%b8%8d%e5%8c%85%e6%8b%ac%e7%ac%ac%20L%20%e5%b1%82%e7%9a%84%e5%81%8f%e5%b7%ae%e5%8d%95%e5%85%83%28%e5%b8%b8%e6%95%b0%e9%a1%b9%29%e3%80%82%e4%bb%a4%20K%20%e4%b8%ba%e8%be%93%e5%87%ba%e5%b1%82%e7%9a%84%e5%8d%95%e5%85%83%e6%95%b0%e7%9b%ae%ef%bc%8c%e5%8d%b3%20%e6%9c%80%e5%90%8e%e4%b8%80%e5%b1%82%e7%9a%84%e5%8d%95%e5%85%83%e6%95%b0%e3%80%82%0a%e7%ac%a6%e5%8f%b7%e7%ba%a6%e5%ae%9a%ef%bc%9a%0a%24z_i%5e%7b%28j%29%7d%24%20%3d%20%e7%ac%ac%20%24j%24%20%e5%b1%82%e7%9a%84%e7%ac%ac%20%24i%24%20%e4%b8%aa%e8%8a%82%e7%82%b9%ef%bc%88%e7%a5%9e%e7%bb%8f%e5%85%83%ef%bc%89%e7%9a%84%e2%80%9c%e8%ae%a1%e7%ae%97%e5%80%bc%e2%80%9d%20%24a_i%5e%7b%28j%29%7d%24%20%3d%20%e7%ac%ac%20%24j%24%20%e5%b1%82%e7%9a%84%e7%ac%ac%20%24i%24%20%e4%b8%aa%e8%8a%82%e7%82%b9%ef%bc%88%e7%a5%9e%e7%bb%8f%e5%85%83%ef%bc%89%e7%9a%84%e2%80%9c%e6%bf%80%e6%b4%bb%e5%80%bc%e2%80%9d%20%24%5cTheta%5e%7b%28l%29%7d%7bi%2cj%7d%24%20%3d%20%e6%98%a0%e5%b0%84%e7%ac%ac%20%24l%24%20%e5%b1%82%e5%88%b0%e7%ac%ac%20%24l%2b1%24%20%e5%b1%82%e7%9a%84%e6%9d%83%e5%80%bc%e7%9f%a9%e9%98%b5%e7%9a%84%e7%ac%ac%20%24i%24%20%e8%a1%8c%e7%ac%ac%20%24j%24%20%e5%88%97%e7%9a%84%e5%88%86%e9%87%8f%20%24L%24%20%3d%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%80%bb%e5%b1%82%e6%95%b0%ef%bc%88%e5%8c%85%e6%8b%ac%e8%be%93%e5%85%a5%e5%b1%82%e3%80%81%e9%9a%90%e5%b1%82%e5%92%8c%e8%be%93%e5%87%ba%e5%b1%82%ef%bc%89%20%24s_l%24%20%3d%20%e7%ac%ac%20%24l%24%20%e5%b1%82%e8%8a%82%e7%82%b9%ef%bc%88%e7%a5%9e%e7%bb%8f%e5%85%83%ef%bc%89%e4%b8%aa%e6%95%b0%ef%bc%8c%e4%b8%8d%e5%8c%85%e6%8b%ac%e5%81%8f%e7%a7%bb%e9%87%8f%e8%8a%82%e7%82%b9%e3%80%82%20%24K%24%20%3d%20%e8%be%93%e5%87%ba%e8%8a%82%e7%82%b9%e4%b8%aa%e6%95%b0%20%24h%7b%5ctheta%7d%28x%29_k%24%20%3d%20%e7%ac%ac%20%24k%24%20%e4%b8%aa%e9%a2%84%e6%b5%8b%e8%be%93%e5%87%ba%e7%bb%93%e6%9e%9c%20%24x%5e%7b%28i%29%7d%24%20%3d%20%e7%ac%ac%20%24i%24%20%e4%b8%aa%e6%a0%b7%e6%9c%ac%e7%89%b9%e5%be%81%e5%90%91%e9%87%8f%20%24x%5e%7b%28i%29%7d_k%24%20%3d%20%e7%ac%ac%20%24i%24%20%e4%b8%aa%e6%a0%b7%e6%9c%ac%e7%9a%84%e7%ac%ac%20%24k%24%20%e4%b8%aa%e7%89%b9%e5%be%81%e5%80%bc%20%24y%5e%7b%28i%29%7d%24%20%3d%20%e7%ac%ac%20%24i%24%20%e4%b8%aa%e6%a0%b7%e6%9c%ac%e5%ae%9e%e9%99%85%e7%bb%93%e6%9e%9c%e5%90%91%e9%87%8f"><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&t=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><nav id=TableOfContents><ul><li><a href=#一-cost-function-and-backpropagation>一. Cost Function and Backpropagation</a><ul><li><a href=#1-cost-function>1. Cost Function</a></li><li><a href=#2-backpropagation-algorithm-反向传播算法>2. Backpropagation Algorithm 反向传播算法</a></li><li><a href=#1-前向传播>(1) 前向传播</a></li><li><a href=#2-计算误差>(2) 计算误差</a></li><li><a href=#3-反向传播>(3) 反向传播</a></li><li><a href=#4-计算偏导数>(4) 计算偏导数</a></li><li><a href=#5-更新矩阵>(5) 更新矩阵</a></li></ul></li><li><a href=#二-推导>二. 推导</a><ul><li><a href=#1-目标>1. 目标</a></li><li><a href=#2-思路>2. 思路</a></li><li><a href=#3-推导过程>3. 推导过程</a></li><li><a href=#情况1-隐藏层--输出层>情况1 隐藏层 → 输出层</a></li><li><a href=#情况2-隐藏层--输入层--隐藏层>情况2 隐藏层 / 输入层 → 隐藏层</a></li><li><a href=#4-总结算法公式>4. 总结算法公式</a></li></ul></li><li><a href=#三-backpropagation-algorithm-反向传播算法过程>三. Backpropagation Algorithm 反向传播算法过程</a></li><li><a href=#四-backpropagation-algorithm-implementation-算法实现>四. Backpropagation Algorithm implementation 算法实现</a><ul><li><a href=#1-前馈阶段>1. 前馈阶段</a></li><li><a href=#2-代价函数>2. 代价函数</a></li><li><a href=#3-反向传播-1>3. 反向传播</a></li></ul></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">神经网络反向传播算法推导</h1><div class=meta><div class=postdate><time datetime="2018-03-25 08:33:00 +0000 UTC" itemprop=datePublished>Mar 25</time></div><div class=article-category><i class="fas fa-archive"></i><a class=category-link href=/categories/machine-learning>Machine Learning</a>
,
<a class=category-link href=/categories/ai>AI</a></div><div class=article-tag><i class="fas fa-tag"></i><a class=tag-link href=/tags/machine-learning rel=tag>Machine Learning</a>
,
<a class=tag-link href=/tags/ai rel=tag>AI</a></div></div></header><div class=content itemprop=articleBody><blockquote><p>由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/contents.md>Github</a> 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。</p><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a><br>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a><br>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Learning.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Learning.ipynb</a></p></blockquote><h2 id=一-cost-function-and-backpropagation>一. Cost Function and Backpropagation</h2><h3 id=1-cost-function>1. Cost Function</h3><p><img src=https://img.halfrost.com/Blog/ArticleImage/72_3.png alt></p><p>假设训练集中有 m 个训练样本，$\begin{Bmatrix} (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \cdots ,(x^{(m)},y^{(m)}) \end{Bmatrix}$，L 表示神经网络的总层数 Layer，用 $S_{l}$ 表示第 L 层的单元数(神经元的数量)，但是不包括第 L 层的偏差单元(常数项)。令 K 为输出层的单元数目，即 最后一层的单元数。</p><p><strong>符号约定</strong>：</p><p>$z_i^{(j)}$ = 第 $j$ 层的第 $i$ 个节点（神经元）的“计算值”<br>$a_i^{(j)}$ = 第 $j$ 层的第 $i$ 个节点（神经元）的“激活值”<br>$\Theta^{(l)}<em>{i,j}$ = 映射第 $l$ 层到第 $l+1$ 层的权值矩阵的第 $i$ 行第 $j$ 列的分量<br>$L$ = 神经网络总层数（包括输入层、隐层和输出层）<br>$s_l$ = 第 $l$ 层节点（神经元）个数，不包括偏移量节点。<br>$K$ = 输出节点个数<br>$h</em>{\theta}(x)_k$ = 第 $k$ 个预测输出结果<br>$x^{(i)}$ = 第 $i$ 个样本特征向量<br>$x^{(i)}_k$ = 第 $i$ 个样本的第 $k$ 个特征值<br>$y^{(i)}$ = 第 $i$ 个样本实际结果向量<br>$y^{(i)}_k$ = 第 $i$ 个样本结果向量的第 $k$ 个分量</p><p>之前讨论的逻辑回归中代价函数如下：</p><p>$$
\begin{align*}
\rm{CostFunction} = \rm{F}({\theta}) &= -\frac{1}{m}\left [ \sum_{i=1}^{m} y^{(i)}logh_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)})) \right ] +\frac{\lambda}{2m} \sum_{j=1}^{n}\theta_{j}^{2} \<br>\end{align*}
$$</p><p>扩展到神经网络中：</p><p>$$
\begin{align*}
\rm{CostFunction} = \rm{F}({\Theta}) &= -\frac{1}{m}\left [ \sum_{i=1}^{m} \sum_{k=1}^{K} y^{(i)}_{k} log(h_{\Theta}(x^{(i)}))_{k} + (1-y^{(i)}_{k})log(1-(h_{\Theta}(x^{(i)}))_{k}) \right ] +\frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{S_{l}}\sum_{j=1}^{S_{l} +1}(\Theta_{j,i}^{(l)})^{2} \<br>h_{\Theta}(x) &\in \mathbb{R}^{K} ;;;;;;;;; (h_{\Theta}(x))_{i} = i^{th} ;;output \<br>\end{align*}
$$</p><p>$h_{\Theta}(x)$ 是一个 K 维向量，$ i $ 表示选择输出神经网络输出向量中的第 i 个元素。</p><p>神经网络的代价函数相比逻辑回归的代价函数，前一项的求和过程中多了一个 $ \sum_{k=1}^{K} $ ,由于 K 代表了最后一层的单元数，所以这里就是累加了 k 个输出层的代价函数。</p><p>后一项是正则化项，神经网络的正则化项看起来特别复杂，其实就是对 $ (\Theta_{j,i}^{(l)})^{2} $ 项对所有的 i，j，l的值求和。正如在逻辑回归中的一样，这里要除去那些对应于偏差值的项，因为我们不对它们进行求和，即不对 $ (\Theta_{j,0}^{(l)})^{2} ;;;;(i=0) $ 项求和。</p><h3 id=2-backpropagation-algorithm-反向传播算法>2. Backpropagation Algorithm 反向传播算法</h3><p>令 $ \delta_{j}^{(l)} $ 表示第 $l$ 层第 $j$ 个结点的误差。</p><p>反向传播从最后一层开始往前推：</p><p>$$
\begin{align*}
\delta_{j}^{(L)} &= a_{j}^{(L)} - y_{j} \<br>&=(h_{\theta}(x))_{j} - y_{j} \<br>\end{align*}
$$</p><p>往前计算几步：</p><p>$$
\begin{align*}
\delta^{(3)} &= (\Theta^{(3)})^{T}\delta^{(4)} . * g^{'}(z^{(3)}) \<br>\delta^{(2)} &= (\Theta^{(2)})^{T}\delta^{(3)} . * g^{'}(z^{(2)}) \<br>\end{align*}
$$</p><p>逻辑函数（Sigmoid函数）求导：</p><p>$$
\begin{align*}
\sigma(x)'&=\left(\frac{1}{1+e^{-x}}\right)&lsquo;=\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\frac{-1&rsquo;-(e^{-x})'}{(1+e^{-x})^2}=\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\frac{e^{-x}}{(1+e^{-x})^2} \newline &=\left(\frac{1}{1+e^{-x}}\right)\left(\frac{e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{+1-1 + e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{1 + e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}}\right)\<br>&=\sigma(x)(1 - \sigma(x))\<br>\end{align*}
$$</p><p>可以算出 $g^{'}(z^{(3)}) = a^{(3)} . * (1-a^{(3)})$ ， $g^{'}(z^{(2)}) = a^{(2)} . * (1-a^{(2)})$。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/72_4.png alt></p><p>于是可以给出反向传播的算法步骤：</p><p>首先有一个训练集 $\begin{Bmatrix} (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \cdots ,(x^{(m)},y^{(m)}) \end{Bmatrix}$，初始值对每一个 $(l,i,j)$ 都设置 $\Delta^{(l)}_{i,j} := 0$ ，即初始矩阵是全零矩阵。</p><p>针对 $1-m$ 训练集开始以下步骤的训练：</p><h3 id=1-前向传播>(1) 前向传播</h3><p>设置 $ a^{(1)} := x^{(t)} $，并按照前向传播的方法，计算出每一层的激励 $a^{(l)}$ 。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/72_5.png alt></p><h3 id=2-计算误差>(2) 计算误差</h3><p>利用 $y^{(t)}$，计算 $\delta^{(L)} = a^{(L)} - y^{t}$</p><p>其中 $L$ 是我们的总层数，$a^{(L)}$ 是最后一层激活单元输出的向量。所以我们最后一层的“误差值”仅仅是我们在最后一层的实际结果和 y 中的正确输出的差异。为了获得最后一层之前的图层的增量值，我们可以使用下面步骤中的方程，让我们从右向左前进：</p><h3 id=3-反向传播>(3) 反向传播</h3><p>通过 $\delta^{(l)} = ((\Theta^{(l)})^{(T)}\delta^{(l+1)}).* a^{(l)} .*(1-a^{(l)})$，计算 $\delta^{(L-1)},\delta^{(L-2)},\cdots,\delta^{(2)}$ 计算出每一层神经节点的误差。</p><h3 id=4-计算偏导数>(4) 计算偏导数</h3><p>最后利用 $\Delta^{(l)}<em>{i,j} := \Delta^{(l)}</em>{i,j} + a_{j}^{(l)}\delta_{i}^{(l+1)}$，或者矢量表示为 $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^{T}$。</p><p>$$
\frac{\partial }{\partial \Theta_{i,j}^{(l)} }F(\Theta) = D_{i,j}^{(l)} := \left{\begin{matrix}
\frac{1}{m} \left( \Delta_{i,j}^{(l)} + \lambda\Theta_{i,j}^{(l)} \right) ;;;;;;;; j\neq 0\
\frac{1}{m}\Delta_{i,j}^{(l)} ;;;;;;;;;;;;;;;;;;;;;;; j = 0
\end{matrix}\right.$$</p><h3 id=5-更新矩阵>(5) 更新矩阵</h3><p>更新各层的权值矩阵 $\Theta^{(l)}$ ，其中 $\alpha$ 为学习率：</p><p>$$\Theta^{(l)} = \Theta^{(l)} - \alpha D^{(l)}$$</p><hr><h2 id=二-推导>二. 推导</h2><h3 id=1-目标>1. 目标</h3><p>求 $\min_\Theta F(\Theta)$</p><h3 id=2-思路>2. 思路</h3><p>类似梯度下降法，给定一个初值后，计算出所有节点的计算值和激活值，然后根据代价函数的变化不断调整参数值（权值），最终不断逼近最优结果，使代价函数值最小。</p><h3 id=3-推导过程>3. 推导过程</h3><p>为了实现上述思路，我们必须首先计算代价函数的偏导数：</p><p>$$\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}F(\Theta)$$</p><p>这个偏导并不好求，为了方便推导，我们假设只有一个样本（$m=1$，可忽略代价函数中的外部求和），并舍弃正规化部分，然后分为两种情况来求。</p><h3 id=情况1-隐藏层--输出层>情况1 隐藏层 → 输出层</h3><p>我们知道：</p><p>$$
\begin{align*}
h_\Theta(x) &= a^{(j+1)} = g(z^{(j+1)}) \<br>z^{(j)} &= \Theta^{(j-1)}a^{(j-1)} \<br>\end{align*}
$$</p><p>另外，输出层即第$L$层。</p><p>所以：</p><p>$$\dfrac{\partial}{\partial \Theta_{i,j}^{(L)}}F(\Theta)
= \dfrac{\partial F(\Theta)}{\partial h_{\Theta}(x)_i} \dfrac{\partial h_{\Theta}(x)_i}{\partial z_i^{(L)}} \dfrac{\partial z_i^{(L)}}{\partial \Theta_{i,j}^{(L)}}
= \dfrac{\partial F(\Theta)}{\partial a_i^{(L)}} \dfrac{\partial a_i^{(L)}}{\partial z_i^{(L)}} \dfrac{\partial z_i^{(L)}}{\partial \Theta_{i,j}^{(L)}}$$</p><p>其中：</p><p>$$
\begin{align*}
\dfrac{\partial F(\Theta)}{\partial a_i^{(L)}} &= \dfrac{a_i^{(L)} - y_i}{(1 - a_i^{(L)})a_i^{(L)}} \<br>\dfrac{\partial a_i^{(L)}}{\partial z_i^{(L)}} &= \dfrac{\partial g(z_i^{(L)})}{\partial z_i^{(L)}} = \dfrac{e^{z_i^{(L)}}}{(e^{z_i^{(L)}}+1)^2} = a_i^{(L)} (1 - a_i^{(L)}) \<br>\dfrac{\partial z_i^{(L)}}{\partial \Theta_{i,j}^{(L)}} &= \dfrac{\partial ( \sum_{k=0}^{s_{(L-1)}}; \Theta_{i,k}^{(L)} a_k^{(L-1)})}{\partial \Theta_{i,j}^{(L)}} = a_j^{(L-1)} \<br>\end{align*}
$$</p><p>综上：</p><p>$$
\begin{split}
\dfrac{\partial}{\partial \Theta_{i,j}^{(L)}}F(\Theta)
=& \dfrac{\partial F(\Theta)}{\partial a_i^{(L)}} \dfrac{\partial a_i^{(L)}}{\partial z_i^{(L)}} \dfrac{\partial z_i^{(L)}}{\partial \Theta_{i,j}^{(L)}} \newline<br>=& \dfrac{a_i^{(L)} - y_i}{(1 - a_i^{(L)})a_i^{(L)}} a_i^{(L)} (1 - a_i^{(L)}) a_j^{(L-1)} \newline<br>=& (a_i^{(L)} - y_i)a_j^{(L-1)}
\end{split}
$$</p><h3 id=情况2-隐藏层--输入层--隐藏层>情况2 隐藏层 / 输入层 → 隐藏层</h3><p>因为 $a^{(1)}=x$，所以可以将输入层和隐藏层同样对待。</p><p>$$\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}F(\Theta)
=\dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} \dfrac{\partial a_i^{(l)}}{\partial z_i^{(l)}} \dfrac{\partial z_i^{(l)}}{\partial \Theta_{i,j}^{(l)}}\ (l = 1, 2, &mldr;, L-1)$$</p><p>其中后两部分偏导很容易根据前面所得类推出来：</p><p>$$
\begin{align*}
\dfrac{\partial a_i^{(l)}}{\partial z_i^{(l)}} &= \dfrac{e^{z_i^{(l)}}}{(e^{z_i^{(l)}}+1)^2} = a_i^{(l)} (1 - a_i^{(l)}) \<br>\dfrac{\partial z_i^{(l)}}{\partial \Theta_{i,j}^{(l)}} &= a_j^{(l-1)} \<br>\end{align*}
$$</p><p>第一部分偏导是不好求解的，或者说是没法直接求解的，我们可以得到一个递推式：</p><p>$$\dfrac{\partial F(\Theta)}{\partial a_i^{(l)}}
= \sum_{k=1}^{s_{(l+1)}} \Bigg[\dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \dfrac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}}\Bigg]$$</p><blockquote><p>因为该层的激活值与下一层各节点都有关，链式法则求导时需一一求导，所以有上式中的求和。</p></blockquote><p>递推式中第一部分是递推项，后两部分同样易求：</p><p>$$
\begin{align*}
\dfrac{\partial a_k^{(l+1)}}{\partial z_{k}^{(l+1)}} &= \dfrac{e^{z_{k}^{(l+1)}}}{(e^{z_{k}^{(l+1)}}+1)^2} = a_k^{(l+1)} (1 - a_k^{(l+1)}) \<br>\dfrac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}} &= \dfrac{\partial ( \sum_{j=0}^{s_l} \Theta_{k,j}^{(l+1)} a_j^{(l)})}{\partial a_i^{(l)}} = \Theta_{k,i}^{(l+1)} \<br>\end{align*}
$$</p><p>所以，递推式为：</p><p>$$
\begin{split}
\dfrac{\partial F(\Theta)}{\partial a_i^{(l)}}
=& \sum_{k=1}^{s_{(l+1)}} \Bigg[\dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \dfrac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}}\Bigg] \newline<br>=& \sum_{k=1}^{s_{(l+1)}} \Bigg[ \dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \Theta_{k,i}^{(l+1)} \Bigg] \newline<br>=& \sum_{k=1}^{s_{(l+1)}} \Bigg[ \dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} a_k^{(l+1)} (1 - a_k^{(l+1)}) \Theta_{k,i}^{(l+1)} \Bigg]
\end{split}
$$</p><p>为了简化表达式，定义第 $l$ 层第 $i$ 个节点的误差：</p><p>$$\begin{split}
\delta^{(l)}<em>i
=& \dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} \dfrac{\partial a_i^{(l)}}{\partial z_i^{(l)}} \newline<br>=& \dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} a_i^{(l)} (1 - a_i^{(l)}) \newline<br>=& \sum</em>{k=1}^{s_{(l+1)}} \Bigg[ \dfrac{\partial F(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \Theta_{k,i}^{(l+1)} \Bigg] a_i^{(l)} (1 - a_i^{(l)}) \newline<br>=& \sum_{k=1}^{s_{(l+1)}} \Big[\delta^{(l+1)}_k \Theta_{k,i}^{(l+1)} \Big] a_i^{(l)} (1 - a_i^{(l)})
\end{split}$$</p><p>可知，<strong>情况1</strong>的误差为：</p><p>$$\begin{split}
\delta^{(L)}_i
=& \dfrac{\partial F(\Theta)}{\partial a_i^{(L)}} \dfrac{\partial a_i^{(L)}}{\partial z_i^{(L)}} \newline<br>=& \dfrac{a_i^{(L)} - y_i}{(1 - a_i^{(L)})a_i^{(L)}} a_i^{(L)} (1 - a_i^{(L)}) \newline<br>=& a_i^{(L)} - y_i
\end{split}$$</p><p>最终的代价函数的偏导为：</p><p>$$\begin{split}
\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}F(\Theta)
=& \dfrac{\partial F(\Theta)}{\partial a_i^{(l)}} \dfrac{\partial a_i^{(l)}}{\partial z_i^{(l)}} \dfrac{\partial z_i^{(l)}}{\partial \Theta_{i,j}^{(l)}} \newline<br>=& \delta^{(l)}_i \dfrac{\partial z_i^{(l)}}{\partial \Theta_{i,j}^{(l)}} \newline<br>=& \delta^{(l)}_i a_j^{(l-1)}
\end{split}$$</p><p>我们发现，引入误差 $\delta^{(l)}_i$ 后，这个公式可以通用于<strong>情况1</strong>和<strong>情况2</strong>。</p><p>可以看出，当前层的代价函数偏导，需要依赖于后一层的计算结果。这也是为什么这个算法的名称叫做“反向传播算法”。</p><h3 id=4-总结算法公式>4. 总结算法公式</h3><ul><li>输出层误差</li></ul><p>$$\delta^{(L)}_i = a_i^{(L)} - y_i$$</p><ul><li>隐藏层误差（反向传播计算）</li></ul><p>$$\delta^{(l)}<em>i = \sum</em>{k=1}^{s_{(l+1)}} \Big[\delta^{(l+1)}_k \Theta_{k,i}^{(l+1)} \Big] a_i^{(l)} (1 - a_i^{(l)})$$</p><ul><li>代价函数偏导计算（通用）</li></ul><p>$$\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}F(\Theta) = \delta^{(l)}_i a_j^{(l-1)}$$</p><hr><h2 id=三-backpropagation-algorithm-反向传播算法过程>三. Backpropagation Algorithm 反向传播算法过程</h2><p><img src=https://img.halfrost.com/Blog/ArticleImage/72_2_.png alt></p><p>有了上述推导，我们描述一下算法具体的操作流程：</p><ul><li>输入：输入样本数据，初始化权值参数（建议随机生成较小的数）。</li><li>前馈：计算各层（$l=2, 3, &mldr;, L$）各节点的计算值（$z^{(l)}=\Theta^{(l-1)}a^{(l-1)}$）和激活值（$a^{(l)}=g(z^{(l)})$）。</li><li>输出层误差：计算输出层误差\delta^{(L)}（公式见前文）。</li><li>反向传播误差：计算各层（$l=L-1, L-2, &mldr;, 2$）的误差 $\delta^{(l)}$（公式见前文）。</li><li>输出：得到代价函数的梯度 $\nabla F(\Theta)$（参考前文偏导计算公式）。</li></ul><p>反向传播算法帮助我们得到了代价函数的梯度，我们就可以借助梯度下降法训练神经网络了。</p><p>$$\Theta := \Theta - \alpha \nabla F(\Theta)$$</p><p>$\alpha $ 为学习速率。</p><hr><h2 id=四-backpropagation-algorithm-implementation-算法实现>四. Backpropagation Algorithm implementation 算法实现</h2><p>以3层神经网络（输入层、隐层、输出层各一）为例。</p><ul><li>X 为大小为样本数∗特征数的样本特征矩阵</li><li>Y 为大小为样本数∗输出节点数的样本类别（结果）矩阵</li><li>Theta1 为输入层→隐层的权值矩阵</li><li>Theta2 为隐藏层→输出层的权值矩阵</li><li>m 为样本数</li><li>K 为输出层节点数</li><li>H 为隐藏层节点数</li><li>sigmoid 函数即逻辑函数（S型函数，Sigmoid函数）</li><li>sigmoidGradient 函数即 Sigmoid 函数的导函数</li><li>代码实现中，考虑了正规化，避免出现过拟合问题。</li></ul><h3 id=1-前馈阶段>1. 前馈阶段</h3><p>逐层计算各节点值和激活值。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>
a1 <span style=color:#f92672>=</span> X;
z2 <span style=color:#f92672>=</span> [ones(m, <span style=color:#ae81ff>1</span>), a1] <span style=color:#f92672>*</span> Theta1<span style=color:#960050;background-color:#1e0010>&#39;</span>;
a2 <span style=color:#f92672>=</span> sigmoid(z2);
z3 <span style=color:#f92672>=</span> [ones(m, <span style=color:#ae81ff>1</span>), a2] <span style=color:#f92672>*</span> Theta2<span style=color:#960050;background-color:#1e0010>&#39;</span>;
a3 <span style=color:#f92672>=</span> sigmoid(z3);

</code></pre></div><h3 id=2-代价函数>2. 代价函数</h3><p>正规化部分需注意代价函数不惩罚偏移参数，即 $\Theta_{i,0}$（代码表示为 $Theta(:,1)$）。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>
F <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> m <span style=color:#f92672>*</span> sum((<span style=color:#f92672>-</span>log(a3) .<span style=color:#f92672>*</span> Y <span style=color:#f92672>-</span> log(<span style=color:#ae81ff>1</span> .<span style=color:#f92672>-</span> a3) .<span style=color:#f92672>*</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> Y))(<span style=color:#f92672>:</span>)) <span style=color:#f92672>+</span> ... <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#960050;background-color:#1e0010>代价部分</span>
 lambda <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>/</span> m <span style=color:#f92672>*</span> (sum((Theta1(<span style=color:#f92672>:</span>, <span style=color:#ae81ff>2</span><span style=color:#f92672>:</span>end) .<span style=color:#f92672>^</span> <span style=color:#ae81ff>2</span>)(<span style=color:#f92672>:</span>)) <span style=color:#f92672>+</span> sum((Theta2(<span style=color:#f92672>:</span>, <span style=color:#ae81ff>2</span><span style=color:#f92672>:</span>end) .<span style=color:#f92672>^</span> <span style=color:#ae81ff>2</span>)(<span style=color:#f92672>:</span>))); 
 <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#960050;background-color:#1e0010>正规化部分，</span>lambda为正规参数<span style=color:#960050;background-color:#1e0010>，需除去偏移参数</span>Theta<span style=color:#f92672>*</span>(<span style=color:#f92672>:</span>,<span style=color:#ae81ff>1</span>)

</code></pre></div><h3 id=3-反向传播-1>3. 反向传播</h3><p>输出层误差和 $\Theta^{(2)}$ 梯度计算，反向传播计算隐层误差和 $\Theta^{(1)}$ 梯度。</p><p>仍需注意正规化时排除偏移参数，另外注意为激活值补一个偏移量 $1$。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>
function g <span style=color:#f92672>=</span> sigmoid(z)
    g <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span> .<span style=color:#f92672>/</span> (<span style=color:#ae81ff>1.0</span> <span style=color:#f92672>+</span> exp(<span style=color:#f92672>-</span>z));
end

function g <span style=color:#f92672>=</span> sigmoidGradient(z)
    g <span style=color:#f92672>=</span> sigmoid(z) .<span style=color:#f92672>*</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> sigmoid(z));
end

delta3 <span style=color:#f92672>=</span> a3 <span style=color:#f92672>-</span> Y;

Theta2_grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> m <span style=color:#f92672>*</span> delta3<span style=color:#960050;background-color:#1e0010>&#39;</span> <span style=color:#f92672>*</span> [ones(m, <span style=color:#ae81ff>1</span>), a2] <span style=color:#f92672>+</span> ...
  lambda <span style=color:#f92672>/</span> m <span style=color:#f92672>*</span> [zeros(K, <span style=color:#ae81ff>1</span>), Theta2(<span style=color:#f92672>:</span>, <span style=color:#ae81ff>2</span><span style=color:#f92672>:</span>end)]; <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#960050;background-color:#1e0010>正规化部分</span>

delta2 <span style=color:#f92672>=</span> (delta3 <span style=color:#f92672>*</span> Theta2 .<span style=color:#f92672>*</span> sigmoidGradient([ones(m, <span style=color:#ae81ff>1</span>), z2]));
delta2 <span style=color:#f92672>=</span> delta2(<span style=color:#f92672>:</span>, <span style=color:#ae81ff>2</span><span style=color:#f92672>:</span>end); <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#960050;background-color:#1e0010>反向计算多一个偏移参数误差，除去</span>

Theta1_grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> m <span style=color:#f92672>*</span>  delta2<span style=color:#960050;background-color:#1e0010>&#39;</span> <span style=color:#f92672>*</span> [ones(m, <span style=color:#ae81ff>1</span>), a1] <span style=color:#f92672>+</span> ...
  lambda <span style=color:#f92672>/</span> m <span style=color:#f92672>*</span> [zeros(H, <span style=color:#ae81ff>1</span>), Theta1(<span style=color:#f92672>:</span>, <span style=color:#ae81ff>2</span><span style=color:#f92672>:</span>end)]; <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#960050;background-color:#1e0010>正规化部分</span>

</code></pre></div><hr><p>推荐阅读：</p><p><a href=http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html>Principles of training multi-layer neural network using backpropagation</a></p><p><a href=https://www.zhihu.com/question/27239198>如何直观地解释 back propagation 算法？</a></p><hr><blockquote><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a></p><p>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a></p><p>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Learning.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Learning.ipynb</a></p></blockquote><img src=https://img.halfrost.com/wechat-qr-code.png></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#一-cost-function-and-backpropagation>一. Cost Function and Backpropagation</a><ul><li><a href=#1-cost-function>1. Cost Function</a></li><li><a href=#2-backpropagation-algorithm-反向传播算法>2. Backpropagation Algorithm 反向传播算法</a></li><li><a href=#1-前向传播>(1) 前向传播</a></li><li><a href=#2-计算误差>(2) 计算误差</a></li><li><a href=#3-反向传播>(3) 反向传播</a></li><li><a href=#4-计算偏导数>(4) 计算偏导数</a></li><li><a href=#5-更新矩阵>(5) 更新矩阵</a></li></ul></li><li><a href=#二-推导>二. 推导</a><ul><li><a href=#1-目标>1. 目标</a></li><li><a href=#2-思路>2. 思路</a></li><li><a href=#3-推导过程>3. 推导过程</a></li><li><a href=#情况1-隐藏层--输出层>情况1 隐藏层 → 输出层</a></li><li><a href=#情况2-隐藏层--输入层--隐藏层>情况2 隐藏层 / 输入层 → 隐藏层</a></li><li><a href=#4-总结算法公式>4. 总结算法公式</a></li></ul></li><li><a href=#三-backpropagation-algorithm-反向传播算法过程>三. Backpropagation Algorithm 反向传播算法过程</a></li><li><a href=#四-backpropagation-algorithm-implementation-算法实现>四. Backpropagation Algorithm implementation 算法实现</a><ul><li><a href=#1-前馈阶段>1. 前馈阶段</a></li><li><a href=#2-代价函数>2. 代价函数</a></li><li><a href=#3-反向传播-1>3. 反向传播</a></li></ul></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f"><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&text=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&is_video=false&description=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f"><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-stumbleupon fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-digg fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&name=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fNeural_Networks_Learning.ipynb%0a%20%e4%b8%80.%20Cost%20Function%20and%20Backpropagation%201.%20Cost%20Function%20%e5%81%87%e8%ae%be%e8%ae%ad%e7%bb%83%e9%9b%86%e4%b8%ad%e6%9c%89%20m%20%e4%b8%aa%e8%ae%ad%e7%bb%83%e6%a0%b7%e6%9c%ac%ef%bc%8c%24%5cbegin%7bBmatrix%7d%20%28x%5e%7b%281%29%7d%2cy%5e%7b%281%29%7d%29%2c%28x%5e%7b%282%29%7d%2cy%5e%7b%282%29%7d%29%2c%20%5ccdots%20%2c%28x%5e%7b%28m%29%7d%2cy%5e%7b%28m%29%7d%29%20%5cend%7bBmatrix%7d%24%ef%bc%8cL%20%e8%a1%a8%e7%a4%ba%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e6%80%bb%e5%b1%82%e6%95%b0%20Layer%ef%bc%8c%e7%94%a8%20%24S_%7bl%7d%24%20%e8%a1%a8%e7%a4%ba%e7%ac%ac%20L%20%e5%b1%82%e7%9a%84%e5%8d%95%e5%85%83%e6%95%b0%28%e7%a5%9e%e7%bb%8f%e5%85%83%e7%9a%84%e6%95%b0%e9%87%8f%29%ef%bc%8c%e4%bd%86%e6%98%af%e4%b8%8d%e5%8c%85%e6%8b%ac%e7%ac%ac%20L%20%e5%b1%82%e7%9a%84%e5%81%8f%e5%b7%ae%e5%8d%95%e5%85%83%28%e5%b8%b8%e6%95%b0%e9%a1%b9%29%e3%80%82%e4%bb%a4%20K%20%e4%b8%ba%e8%be%93%e5%87%ba%e5%b1%82%e7%9a%84%e5%8d%95%e5%85%83%e6%95%b0%e7%9b%ae%ef%bc%8c%e5%8d%b3%20%e6%9c%80%e5%90%8e%e4%b8%80%e5%b1%82%e7%9a%84%e5%8d%95%e5%85%83%e6%95%b0%e3%80%82%0a%e7%ac%a6%e5%8f%b7%e7%ba%a6%e5%ae%9a%ef%bc%9a%0a%24z_i%5e%7b%28j%29%7d%24%20%3d%20%e7%ac%ac%20%24j%24%20%e5%b1%82%e7%9a%84%e7%ac%ac%20%24i%24%20%e4%b8%aa%e8%8a%82%e7%82%b9%ef%bc%88%e7%a5%9e%e7%bb%8f%e5%85%83%ef%bc%89%e7%9a%84%e2%80%9c%e8%ae%a1%e7%ae%97%e5%80%bc%e2%80%9d%20%24a_i%5e%7b%28j%29%7d%24%20%3d%20%e7%ac%ac%20%24j%24%20%e5%b1%82%e7%9a%84%e7%ac%ac%20%24i%24%20%e4%b8%aa%e8%8a%82%e7%82%b9%ef%bc%88%e7%a5%9e%e7%bb%8f%e5%85%83%ef%bc%89%e7%9a%84%e2%80%9c%e6%bf%80%e6%b4%bb%e5%80%bc%e2%80%9d%20%24%5cTheta%5e%7b%28l%29%7d%7bi%2cj%7d%24%20%3d%20%e6%98%a0%e5%b0%84%e7%ac%ac%20%24l%24%20%e5%b1%82%e5%88%b0%e7%ac%ac%20%24l%2b1%24%20%e5%b1%82%e7%9a%84%e6%9d%83%e5%80%bc%e7%9f%a9%e9%98%b5%e7%9a%84%e7%ac%ac%20%24i%24%20%e8%a1%8c%e7%ac%ac%20%24j%24%20%e5%88%97%e7%9a%84%e5%88%86%e9%87%8f%20%24L%24%20%3d%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%80%bb%e5%b1%82%e6%95%b0%ef%bc%88%e5%8c%85%e6%8b%ac%e8%be%93%e5%85%a5%e5%b1%82%e3%80%81%e9%9a%90%e5%b1%82%e5%92%8c%e8%be%93%e5%87%ba%e5%b1%82%ef%bc%89%20%24s_l%24%20%3d%20%e7%ac%ac%20%24l%24%20%e5%b1%82%e8%8a%82%e7%82%b9%ef%bc%88%e7%a5%9e%e7%bb%8f%e5%85%83%ef%bc%89%e4%b8%aa%e6%95%b0%ef%bc%8c%e4%b8%8d%e5%8c%85%e6%8b%ac%e5%81%8f%e7%a7%bb%e9%87%8f%e8%8a%82%e7%82%b9%e3%80%82%20%24K%24%20%3d%20%e8%be%93%e5%87%ba%e8%8a%82%e7%82%b9%e4%b8%aa%e6%95%b0%20%24h%7b%5ctheta%7d%28x%29_k%24%20%3d%20%e7%ac%ac%20%24k%24%20%e4%b8%aa%e9%a2%84%e6%b5%8b%e8%be%93%e5%87%ba%e7%bb%93%e6%9e%9c%20%24x%5e%7b%28i%29%7d%24%20%3d%20%e7%ac%ac%20%24i%24%20%e4%b8%aa%e6%a0%b7%e6%9c%ac%e7%89%b9%e5%be%81%e5%90%91%e9%87%8f%20%24x%5e%7b%28i%29%7d_k%24%20%3d%20%e7%ac%ac%20%24i%24%20%e4%b8%aa%e6%a0%b7%e6%9c%ac%e7%9a%84%e7%ac%ac%20%24k%24%20%e4%b8%aa%e7%89%b9%e5%be%81%e5%80%bc%20%24y%5e%7b%28i%29%7d%24%20%3d%20%e7%ac%ac%20%24i%24%20%e4%b8%aa%e6%a0%b7%e6%9c%ac%e5%ae%9e%e9%99%85%e7%bb%93%e6%9e%9c%e5%90%91%e9%87%8f"><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2fneural_networks_learning%2f&t=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e6%8e%a8%e5%af%bc"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu class=icon href=# onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden=true></i>Menu</a>
<a id=toc class=icon href=# onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden=true></i>TOC</a>
<a id=share class=icon href=# onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden=true></i>share</a>
<a id=top style=display:none class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i>Top</a></div></div></div><footer id=footer><div class=footer-left><p class=copyright style=float:left;margin-bottom:0><a href=https://github.com/halfrost/Halfrost-Field class=github-repo style=height:18px><span class=gadget-github></span>Star</a>
Copyright &copy;halfrost 2016 - 2021
<a href=http://www.miit.gov.cn/>鄂ICP备16014744号</a></p><br><p class="copyright statistics" style=margin-bottom:20px><span id=busuanzi_container_site_pv>Cumulative Page Views <span id=busuanzi_value_site_pv></span>| Unique Visitors <span id=busuanzi_value_site_uv></span></span></p></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script><script src=/main.min.f870a4d110314b9e50e65f8ac982dc1c9c376c8f1a5083d39c62cfc49073f011.js></script><script async src=/prism.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}};</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>