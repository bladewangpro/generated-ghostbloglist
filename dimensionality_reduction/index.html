<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=theme-color content="#FFFFFF"><meta http-equiv=x-ua-compatible content="IE=edge"><title>PCA 与降维 | prometheus</title><meta name=description content="Explore in every moment of the hard thinking"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="PCA 与降维"><meta property="og:description" content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Dimensionality_Reduction.ipynb
 一. Motivation 我们很希望有足够多的特征（知识）来保准学习模型的训练效果，尤其在图像处理这类的任务中，高维特征是在所难免的，但是，高维的特征也有几个如下不好的地方：
 学习性能下降，知识越多，吸收知识（输入），并且精通知识（学习）的速度就越慢。 过多的特征难于分辨，你很难第一时间认识某个特征代表的意义。 特征冗余，如下图所示，厘米和英尺就是一对冗余特征，他们本身代表的意义是一样的，并且能够相互转换。  我们使用现在使用了一条绿色直线，将各个样本投影到该直线，那么，原来二维的特征 x=(厘米，英尺) 就被降低为了一维 x=(直线上的相对位置)
而在下面的例子中，我们又将三维特征投影到二位平面，从而将三维特征降到了二维：
特征降维的一般手段就是将高维特征投影到低维空间。
 二. Principal Component Analysis 主成分分析 PCA，Principle Component Analysis，即主成分分析法，是特征降维的最常用手段。顾名思义，PCA 能从冗余特征中提取主要成分，在不太损失模型质量的情况下，提升了模型训练速度。
如上图所示，我们将样本到红色向量的距离称作是投影误差（Projection Error）。以二维投影到一维为例，PCA 就是要找寻一条直线，使得各个特征的投影误差足够小，这样才能尽可能的保留原特征具有的信息。
假设我们要将特征从 n 维度降到 k 维：PCA 首先找寻 k 个 n 维向量，然后将特征投影到这些向量构成的 k 维空间，并保证投影误差足够小。下图中中，为了将特征维度从三维降低到二位，PCA 就会先找寻两个三维向量 $\mu^{(1)},\mu^{(2)}$ ，二者构成了一个二维平面，然后将原来的三维特征投影到该二维平面上：
1. 区别 PCA 和 线性回归的区别是：
线性回归找的是垂直于 X 轴距离最小值，PCA 找的是投影垂直距离最小值。"><meta property="og:type" content="article"><meta property="og:url" content="https://new.halfrost.com/dimensionality_reduction/"><meta property="article:published_time" content="2018-03-31T18:09:00+00:00"><meta property="article:modified_time" content="2018-03-31T18:09:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="PCA 与降维"><meta name=twitter:description content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Dimensionality_Reduction.ipynb
 一. Motivation 我们很希望有足够多的特征（知识）来保准学习模型的训练效果，尤其在图像处理这类的任务中，高维特征是在所难免的，但是，高维的特征也有几个如下不好的地方：
 学习性能下降，知识越多，吸收知识（输入），并且精通知识（学习）的速度就越慢。 过多的特征难于分辨，你很难第一时间认识某个特征代表的意义。 特征冗余，如下图所示，厘米和英尺就是一对冗余特征，他们本身代表的意义是一样的，并且能够相互转换。  我们使用现在使用了一条绿色直线，将各个样本投影到该直线，那么，原来二维的特征 x=(厘米，英尺) 就被降低为了一维 x=(直线上的相对位置)
而在下面的例子中，我们又将三维特征投影到二位平面，从而将三维特征降到了二维：
特征降维的一般手段就是将高维特征投影到低维空间。
 二. Principal Component Analysis 主成分分析 PCA，Principle Component Analysis，即主成分分析法，是特征降维的最常用手段。顾名思义，PCA 能从冗余特征中提取主要成分，在不太损失模型质量的情况下，提升了模型训练速度。
如上图所示，我们将样本到红色向量的距离称作是投影误差（Projection Error）。以二维投影到一维为例，PCA 就是要找寻一条直线，使得各个特征的投影误差足够小，这样才能尽可能的保留原特征具有的信息。
假设我们要将特征从 n 维度降到 k 维：PCA 首先找寻 k 个 n 维向量，然后将特征投影到这些向量构成的 k 维空间，并保证投影误差足够小。下图中中，为了将特征维度从三维降低到二位，PCA 就会先找寻两个三维向量 $\mu^{(1)},\mu^{(2)}$ ，二者构成了一个二维平面，然后将原来的三维特征投影到该二维平面上：
1. 区别 PCA 和 线性回归的区别是：
线性回归找的是垂直于 X 轴距离最小值，PCA 找的是投影垂直距离最小值。"><link rel=stylesheet href=/css/style-white.min.css><link rel=manifest href=/manifest.json><link rel=stylesheet href=/prism.css><link href=/images/apple-touch-icon-60x60.png rel=apple-touch-icon sizes=60x60><link href=/images/apple-touch-icon-76x76.png rel=apple-touch-icon sizes=76x76><link href=/images/apple-touch-icon-120x120.png rel=apple-touch-icon sizes=120x120><link href=/images/apple-touch-icon-152x152.png rel=apple-touch-icon sizes=152x152><link href=/images/apple-touch-icon-180x180.png rel=apple-touch-icon sizes=180x180><link href=/images/apple-touch-icon-512x512.png rel=apple-touch-icon sizes=512x512><link href=/images/apple-touch-icon-1024x1024.png rel=apple-touch-icon sizes=1024x1024><script async>if('serviceWorker'in navigator){navigator.serviceWorker.register("\/serviceworker-v1.min.a64912b78d282eab1ad3715a0943da21616e5f326f8afea27034784ad445043b.js").then(function(){if(navigator.serviceWorker.controller){console.log('Assets cached by the controlling service worker.');}else{console.log('Please reload this page to allow the service worker to handle network operations.');}}).catch(function(error){console.log('ERROR: '+error);});}else{console.log('Service workers are not supported in the current browser.');}</script><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://new.halfrost.com/images/favicon.ico><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-82753806-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class="single-max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a><a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a><a id=top-icon-tablet href=# onclick="$('html, body').animate({scrollTop:0},'fast');" style=display:none><i class="fas fa-chevron-up fa-lg"></i></a><span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://new.halfrost.com/unsupervised_learning/><i class="fas fa-chevron-left" aria-hidden=true onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li><li><a class=icon href=https://new.halfrost.com/anomaly_detection/><i class="fas fa-chevron-right" aria-hidden=true onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li><li><a class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up" aria-hidden=true onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li><li><a class=icon href=#><i class="fas fa-share-alt" aria-hidden=true onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f"><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&text=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&title=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&is_video=false&description=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f"><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&title=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&title=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&title=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-stumbleupon" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&title=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-digg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&name=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fDimensionality_Reduction.ipynb%0a%20%e4%b8%80.%20Motivation%20%e6%88%91%e4%bb%ac%e5%be%88%e5%b8%8c%e6%9c%9b%e6%9c%89%e8%b6%b3%e5%a4%9f%e5%a4%9a%e7%9a%84%e7%89%b9%e5%be%81%ef%bc%88%e7%9f%a5%e8%af%86%ef%bc%89%e6%9d%a5%e4%bf%9d%e5%87%86%e5%ad%a6%e4%b9%a0%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%ae%ad%e7%bb%83%e6%95%88%e6%9e%9c%ef%bc%8c%e5%b0%a4%e5%85%b6%e5%9c%a8%e5%9b%be%e5%83%8f%e5%a4%84%e7%90%86%e8%bf%99%e7%b1%bb%e7%9a%84%e4%bb%bb%e5%8a%a1%e4%b8%ad%ef%bc%8c%e9%ab%98%e7%bb%b4%e7%89%b9%e5%be%81%e6%98%af%e5%9c%a8%e6%89%80%e9%9a%be%e5%85%8d%e7%9a%84%ef%bc%8c%e4%bd%86%e6%98%af%ef%bc%8c%e9%ab%98%e7%bb%b4%e7%9a%84%e7%89%b9%e5%be%81%e4%b9%9f%e6%9c%89%e5%87%a0%e4%b8%aa%e5%a6%82%e4%b8%8b%e4%b8%8d%e5%a5%bd%e7%9a%84%e5%9c%b0%e6%96%b9%ef%bc%9a%0a%20%e5%ad%a6%e4%b9%a0%e6%80%a7%e8%83%bd%e4%b8%8b%e9%99%8d%ef%bc%8c%e7%9f%a5%e8%af%86%e8%b6%8a%e5%a4%9a%ef%bc%8c%e5%90%b8%e6%94%b6%e7%9f%a5%e8%af%86%ef%bc%88%e8%be%93%e5%85%a5%ef%bc%89%ef%bc%8c%e5%b9%b6%e4%b8%94%e7%b2%be%e9%80%9a%e7%9f%a5%e8%af%86%ef%bc%88%e5%ad%a6%e4%b9%a0%ef%bc%89%e7%9a%84%e9%80%9f%e5%ba%a6%e5%b0%b1%e8%b6%8a%e6%85%a2%e3%80%82%20%e8%bf%87%e5%a4%9a%e7%9a%84%e7%89%b9%e5%be%81%e9%9a%be%e4%ba%8e%e5%88%86%e8%be%a8%ef%bc%8c%e4%bd%a0%e5%be%88%e9%9a%be%e7%ac%ac%e4%b8%80%e6%97%b6%e9%97%b4%e8%ae%a4%e8%af%86%e6%9f%90%e4%b8%aa%e7%89%b9%e5%be%81%e4%bb%a3%e8%a1%a8%e7%9a%84%e6%84%8f%e4%b9%89%e3%80%82%20%e7%89%b9%e5%be%81%e5%86%97%e4%bd%99%ef%bc%8c%e5%a6%82%e4%b8%8b%e5%9b%be%e6%89%80%e7%a4%ba%ef%bc%8c%e5%8e%98%e7%b1%b3%e5%92%8c%e8%8b%b1%e5%b0%ba%e5%b0%b1%e6%98%af%e4%b8%80%e5%af%b9%e5%86%97%e4%bd%99%e7%89%b9%e5%be%81%ef%bc%8c%e4%bb%96%e4%bb%ac%e6%9c%ac%e8%ba%ab%e4%bb%a3%e8%a1%a8%e7%9a%84%e6%84%8f%e4%b9%89%e6%98%af%e4%b8%80%e6%a0%b7%e7%9a%84%ef%bc%8c%e5%b9%b6%e4%b8%94%e8%83%bd%e5%a4%9f%e7%9b%b8%e4%ba%92%e8%bd%ac%e6%8d%a2%e3%80%82%20%20%e6%88%91%e4%bb%ac%e4%bd%bf%e7%94%a8%e7%8e%b0%e5%9c%a8%e4%bd%bf%e7%94%a8%e4%ba%86%e4%b8%80%e6%9d%a1%e7%bb%bf%e8%89%b2%e7%9b%b4%e7%ba%bf%ef%bc%8c%e5%b0%86%e5%90%84%e4%b8%aa%e6%a0%b7%e6%9c%ac%e6%8a%95%e5%bd%b1%e5%88%b0%e8%af%a5%e7%9b%b4%e7%ba%bf%ef%bc%8c%e9%82%a3%e4%b9%88%ef%bc%8c%e5%8e%9f%e6%9d%a5%e4%ba%8c%e7%bb%b4%e7%9a%84%e7%89%b9%e5%be%81%20x%3d%28%e5%8e%98%e7%b1%b3%ef%bc%8c%e8%8b%b1%e5%b0%ba%29%20%e5%b0%b1%e8%a2%ab%e9%99%8d%e4%bd%8e%e4%b8%ba%e4%ba%86%e4%b8%80%e7%bb%b4%20x%3d%28%e7%9b%b4%e7%ba%bf%e4%b8%8a%e7%9a%84%e7%9b%b8%e5%af%b9%e4%bd%8d%e7%bd%ae%29%0a%e8%80%8c%e5%9c%a8%e4%b8%8b%e9%9d%a2%e7%9a%84%e4%be%8b%e5%ad%90%e4%b8%ad%ef%bc%8c%e6%88%91%e4%bb%ac%e5%8f%88%e5%b0%86%e4%b8%89%e7%bb%b4%e7%89%b9%e5%be%81%e6%8a%95%e5%bd%b1%e5%88%b0%e4%ba%8c%e4%bd%8d%e5%b9%b3%e9%9d%a2%ef%bc%8c%e4%bb%8e%e8%80%8c%e5%b0%86%e4%b8%89%e7%bb%b4%e7%89%b9%e5%be%81%e9%99%8d%e5%88%b0%e4%ba%86%e4%ba%8c%e7%bb%b4%ef%bc%9a%0a%e7%89%b9%e5%be%81%e9%99%8d%e7%bb%b4%e7%9a%84%e4%b8%80%e8%88%ac%e6%89%8b%e6%ae%b5%e5%b0%b1%e6%98%af%e5%b0%86%e9%ab%98%e7%bb%b4%e7%89%b9%e5%be%81%e6%8a%95%e5%bd%b1%e5%88%b0%e4%bd%8e%e7%bb%b4%e7%a9%ba%e9%97%b4%e3%80%82%0a%20%e4%ba%8c.%20Principal%20Component%20Analysis%20%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90%20PCA%ef%bc%8cPrinciple%20Component%20Analysis%ef%bc%8c%e5%8d%b3%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90%e6%b3%95%ef%bc%8c%e6%98%af%e7%89%b9%e5%be%81%e9%99%8d%e7%bb%b4%e7%9a%84%e6%9c%80%e5%b8%b8%e7%94%a8%e6%89%8b%e6%ae%b5%e3%80%82%e9%a1%be%e5%90%8d%e6%80%9d%e4%b9%89%ef%bc%8cPCA%20%e8%83%bd%e4%bb%8e%e5%86%97%e4%bd%99%e7%89%b9%e5%be%81%e4%b8%ad%e6%8f%90%e5%8f%96%e4%b8%bb%e8%a6%81%e6%88%90%e5%88%86%ef%bc%8c%e5%9c%a8%e4%b8%8d%e5%a4%aa%e6%8d%9f%e5%a4%b1%e6%a8%a1%e5%9e%8b%e8%b4%a8%e9%87%8f%e7%9a%84%e6%83%85%e5%86%b5%e4%b8%8b%ef%bc%8c%e6%8f%90%e5%8d%87%e4%ba%86%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%e9%80%9f%e5%ba%a6%e3%80%82%0a%e5%a6%82%e4%b8%8a%e5%9b%be%e6%89%80%e7%a4%ba%ef%bc%8c%e6%88%91%e4%bb%ac%e5%b0%86%e6%a0%b7%e6%9c%ac%e5%88%b0%e7%ba%a2%e8%89%b2%e5%90%91%e9%87%8f%e7%9a%84%e8%b7%9d%e7%a6%bb%e7%a7%b0%e4%bd%9c%e6%98%af%e6%8a%95%e5%bd%b1%e8%af%af%e5%b7%ae%ef%bc%88Projection%20Error%ef%bc%89%e3%80%82%e4%bb%a5%e4%ba%8c%e7%bb%b4%e6%8a%95%e5%bd%b1%e5%88%b0%e4%b8%80%e7%bb%b4%e4%b8%ba%e4%be%8b%ef%bc%8cPCA%20%e5%b0%b1%e6%98%af%e8%a6%81%e6%89%be%e5%af%bb%e4%b8%80%e6%9d%a1%e7%9b%b4%e7%ba%bf%ef%bc%8c%e4%bd%bf%e5%be%97%e5%90%84%e4%b8%aa%e7%89%b9%e5%be%81%e7%9a%84%e6%8a%95%e5%bd%b1%e8%af%af%e5%b7%ae%e8%b6%b3%e5%a4%9f%e5%b0%8f%ef%bc%8c%e8%bf%99%e6%a0%b7%e6%89%8d%e8%83%bd%e5%b0%bd%e5%8f%af%e8%83%bd%e7%9a%84%e4%bf%9d%e7%95%99%e5%8e%9f%e7%89%b9%e5%be%81%e5%85%b7%e6%9c%89%e7%9a%84%e4%bf%a1%e6%81%af%e3%80%82%0a%e5%81%87%e8%ae%be%e6%88%91%e4%bb%ac%e8%a6%81%e5%b0%86%e7%89%b9%e5%be%81%e4%bb%8e%20n%20%e7%bb%b4%e5%ba%a6%e9%99%8d%e5%88%b0%20k%20%e7%bb%b4%ef%bc%9aPCA%20%e9%a6%96%e5%85%88%e6%89%be%e5%af%bb%20k%20%e4%b8%aa%20n%20%e7%bb%b4%e5%90%91%e9%87%8f%ef%bc%8c%e7%84%b6%e5%90%8e%e5%b0%86%e7%89%b9%e5%be%81%e6%8a%95%e5%bd%b1%e5%88%b0%e8%bf%99%e4%ba%9b%e5%90%91%e9%87%8f%e6%9e%84%e6%88%90%e7%9a%84%20k%20%e7%bb%b4%e7%a9%ba%e9%97%b4%ef%bc%8c%e5%b9%b6%e4%bf%9d%e8%af%81%e6%8a%95%e5%bd%b1%e8%af%af%e5%b7%ae%e8%b6%b3%e5%a4%9f%e5%b0%8f%e3%80%82%e4%b8%8b%e5%9b%be%e4%b8%ad%e4%b8%ad%ef%bc%8c%e4%b8%ba%e4%ba%86%e5%b0%86%e7%89%b9%e5%be%81%e7%bb%b4%e5%ba%a6%e4%bb%8e%e4%b8%89%e7%bb%b4%e9%99%8d%e4%bd%8e%e5%88%b0%e4%ba%8c%e4%bd%8d%ef%bc%8cPCA%20%e5%b0%b1%e4%bc%9a%e5%85%88%e6%89%be%e5%af%bb%e4%b8%a4%e4%b8%aa%e4%b8%89%e7%bb%b4%e5%90%91%e9%87%8f%20%24%5cmu%5e%7b%281%29%7d%2c%5cmu%5e%7b%282%29%7d%24%20%ef%bc%8c%e4%ba%8c%e8%80%85%e6%9e%84%e6%88%90%e4%ba%86%e4%b8%80%e4%b8%aa%e4%ba%8c%e7%bb%b4%e5%b9%b3%e9%9d%a2%ef%bc%8c%e7%84%b6%e5%90%8e%e5%b0%86%e5%8e%9f%e6%9d%a5%e7%9a%84%e4%b8%89%e7%bb%b4%e7%89%b9%e5%be%81%e6%8a%95%e5%bd%b1%e5%88%b0%e8%af%a5%e4%ba%8c%e7%bb%b4%e5%b9%b3%e9%9d%a2%e4%b8%8a%ef%bc%9a%0a1.%20%e5%8c%ba%e5%88%ab%20PCA%20%e5%92%8c%20%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e7%9a%84%e5%8c%ba%e5%88%ab%e6%98%af%ef%bc%9a%0a%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e6%89%be%e7%9a%84%e6%98%af%e5%9e%82%e7%9b%b4%e4%ba%8e%20X%20%e8%bd%b4%e8%b7%9d%e7%a6%bb%e6%9c%80%e5%b0%8f%e5%80%bc%ef%bc%8cPCA%20%e6%89%be%e7%9a%84%e6%98%af%e6%8a%95%e5%bd%b1%e5%9e%82%e7%9b%b4%e8%b7%9d%e7%a6%bb%e6%9c%80%e5%b0%8f%e5%80%bc%e3%80%82"><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&t=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><nav id=TableOfContents><ul><li><a href=#一-motivation>一. Motivation</a></li><li><a href=#二-principal-component-analysis-主成分分析>二. Principal Component Analysis 主成分分析</a><ul><li><a href=#1-区别>1. 区别</a></li><li><a href=#2-算法流程>2. 算法流程</a></li><li><a href=#3-特征还原>3. 特征还原</a></li><li><a href=#4-降维多少才合适>4. 降维多少才合适？</a></li><li><a href=#5-不要提前优化>5. 不要提前优化</a></li></ul></li><li><a href=#三-principal-component-analysis-测试>三. Principal Component Analysis 测试</a><ul><li><a href=#1-question-1>1. Question 1</a></li><li><a href=#2-question-2>2. Question 2</a></li><li><a href=#3-question-3>3. Question 3</a></li><li><a href=#4-question-4>4. Question 4</a></li><li><a href=#5-question-5>5. Question 5</a></li></ul></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">PCA 与降维</h1><div class=meta><div class=postdate><time datetime="2018-03-31 18:09:00 +0000 UTC" itemprop=datePublished>Mar 31</time></div><div class=article-category><i class="fas fa-archive"></i><a class=category-link href=/categories/machine-learning>Machine Learning</a>
,
<a class=category-link href=/categories/ai>AI</a></div><div class=article-tag><i class="fas fa-tag"></i><a class=tag-link href=/tags/machine-learning rel=tag>Machine Learning</a>
,
<a class=tag-link href=/tags/ai rel=tag>AI</a></div></div></header><div class=content itemprop=articleBody><blockquote><p>由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/contents.md>Github</a> 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。</p><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a><br>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a><br>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Dimensionality_Reduction.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Dimensionality_Reduction.ipynb</a></p></blockquote><h2 id=一-motivation>一. Motivation</h2><p>我们很希望有足够多的特征（知识）来保准学习模型的训练效果，尤其在图像处理这类的任务中，高维特征是在所难免的，但是，高维的特征也有几个如下不好的地方：</p><ol><li>学习性能下降，知识越多，吸收知识（输入），并且精通知识（学习）的速度就越慢。</li><li>过多的特征难于分辨，你很难第一时间认识某个特征代表的意义。</li><li>特征冗余，如下图所示，厘米和英尺就是一对冗余特征，他们本身代表的意义是一样的，并且能够相互转换。</li></ol><p><img src=https://img.halfrost.com/Blog/ArticleImage/78_1.png alt></p><p>我们使用现在使用了一条绿色直线，将各个样本投影到该直线，那么，原来二维的特征 x=(厘米，英尺) 就被降低为了一维 x=(直线上的相对位置)</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/78_2.png alt></p><p>而在下面的例子中，我们又将三维特征投影到二位平面，从而将三维特征降到了二维：</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/78_3.png alt></p><p>特征降维的一般手段就是将高维特征投影到低维空间。</p><hr><h2 id=二-principal-component-analysis-主成分分析>二. Principal Component Analysis 主成分分析</h2><p>PCA，Principle Component Analysis，即主成分分析法，是特征降维的最常用手段。顾名思义，PCA 能从冗余特征中提取主要成分，在不太损失模型质量的情况下，提升了模型训练速度。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/78_4.png alt></p><p>如上图所示，我们将样本到红色向量的距离称作是投影误差（Projection Error）。以二维投影到一维为例，PCA 就是要找寻一条直线，使得各个特征的投影误差足够小，这样才能尽可能的保留原特征具有的信息。</p><p>假设我们要将特征从 n 维度降到 k 维：PCA 首先找寻 k 个 n 维向量，然后将特征投影到这些向量构成的 k 维空间，并保证投影误差足够小。下图中中，为了将特征维度从三维降低到二位，PCA 就会先找寻两个三维向量 $\mu^{(1)},\mu^{(2)}$ ，二者构成了一个二维平面，然后将原来的三维特征投影到该二维平面上：</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/78_5.png alt></p><h3 id=1-区别>1. 区别</h3><p>PCA 和 线性回归的区别是：</p><p>线性回归找的是垂直于 X 轴距离最小值，PCA 找的是投影垂直距离最小值。</p><h3 id=2-算法流程>2. 算法流程</h3><p>假定我们需要将特征维度从 n 维降到 k 维。则 PCA 的执行流程如下：</p><p>特征标准化，平衡各个特征尺度：</p><p>$$x^{(i)}_j=\frac{x^{(i)}_j-\mu_j}{s_j}$$</p><p>$\mu_j$ 为特征 j 的均值，sj 为特征 j 的标准差。</p><p>计算协方差矩阵 $\Sigma $ ：</p><p>$$\Sigma =\frac{1}{m}\sum_{i=1}{m}(x^{(i)})(x^{(i)})^T=\frac{1}{m} \cdot X^TX$$</p><p>通过奇异值分解（SVD），求取 $\Sigma $ 的特征向量（eigenvectors）：</p><p>$$(U,S,V^T)=SVD(\Sigma )$$</p><p>从 U 中取出前 k 个左奇异向量，构成一个约减矩阵 Ureduce :</p><p>$$U_{reduce}=(\mu^{(1)},\mu^{(2)},\cdots,\mu^{(k)})$$</p><p>计算新的特征向量： $z^{(i)}$</p><p>$$z^{(i)}=U^{T}_{reduce} \cdot x^{(i)}$$</p><h3 id=3-特征还原>3. 特征还原</h3><p>因为 PCA 仅保留了特征的主成分，所以 PCA 是一种有损的压缩方式，假定我们获得新特征向量为：</p><p>$$z=U^T_{reduce}x$$</p><p>那么，还原后的特征 $x_{approx}$ 为：</p><p>$$x_{approx}=U_{reduce}z$$</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/78_6.png alt></p><h3 id=4-降维多少才合适>4. 降维多少才合适？</h3><p>从 PCA 的执行流程中，我们知道，需要为 PCA 指定目的维度 k 。如果降维不多，则性能提升不大；如果目标维度太小，则又丢失了许多信息。通常，使用如下的流程的来评估 k 值选取优异：</p><p>求各样本的投影均方误差:</p><p>$$\min \frac{1}{m}\sum_{j=1}^{m}\left | x^{(i)}-x^{(i)}_{approx} \right |^2$$</p><p>求数据的总变差：</p><p>$$\frac{1}{m}\sum_{j=1}^{m}\left | x^{(i)} \right |^2$$</p><p>评估下式是否成立:</p><p>$$\frac{\min \frac{1}{m}\sum_{j=1}^{m}\left | x^{(i)}-x^{(i)}_{approx} \right |^2}{\frac{1}{m}\sum_{j=1}^{m}\left | x^{(i)} \right |^2} \leqslant \epsilon $$</p><p>其中， $\epsilon $ 的取值可以为 0.01,0.05,0.10,⋯0.01,0.05,0.10,⋯ ，假设 $\epsilon = 0.01 $ ，我们就说“特征间 99% 的差异性得到保留”。</p><h3 id=5-不要提前优化>5. 不要提前优化</h3><p>由于 PCA 减小了特征维度，因而也有可能带来过拟合的问题。PCA 不是必须的，在机器学习中，一定谨记不要提前优化，只有当算法运行效率不尽如如人意时，再考虑使用 PCA 或者其他特征降维手段来提升训练速度。</p><p>当你在保留99% 或者95% 或者其它百分比的方差时 结果表明 就只使用正则化将会给你 一种避免过拟合 绝对好的方法 ，同时正则化 效果也会比 PCA 更好 因为当你使用线性回归或者逻辑回归 或其他的方法 配合正则化时 这个最小化问题 实际就变成了 y 值是什么 才不至于 将有用的信息舍弃掉 然而 PCA 不需要使用到 这些标签 更容易将有价值信息舍弃 总之 使用 PCA 的目的是 加速 学习算法的时候是好的 但是用它来避免过拟合 却并不是一个好的 PCA 应用 我们使用正则化的方法来代替 PCA 方法 是很多人 建议的 。</p><p>你的学习算法 收敛地非常缓慢 占用内存 或者硬盘空间非常大 所以你想来压缩 数据 只有当你的$x^{(i)}$ 效果不好 只有当你有证据或者 充足的理由来确定 $x^{(i)}$ 效果不好的时候 那么就考虑用PCA来进行压缩数据 。</p><p>PCA通常都是 被用来 压缩数据的 以减少内存使用 或硬盘空间占用 或者用来可视化数据</p><hr><h2 id=三-principal-component-analysis-测试>三. Principal Component Analysis 测试</h2><h3 id=1-question-1>1. Question 1</h3><p>Consider the following 2D dataset:</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/7X_1.png alt></p><p>Which of the following figures correspond to possible values that PCA may return for u(1) (the first eigenvector / first principal component)? Check all that apply (you may have to check more than one figure).</p><p>A. <img src=https://img.halfrost.com/Blog/ArticleImage/7X_1A.png alt></p><p>B. <img src=https://img.halfrost.com/Blog/ArticleImage/7X_1B.png alt></p><p>C. <img src=https://img.halfrost.com/Blog/ArticleImage/7X_1C.png alt></p><p>D. <img src=https://img.halfrost.com/Blog/ArticleImage/7X_1D.png alt></p><p>解答：A、B</p><h3 id=2-question-2>2. Question 2</h3><p>Which of the following is a reasonable way to select the number of principal components k?</p><p>(Recall that n is the dimensionality of the input data and m is the number of input examples.)</p><p>A. Choose k to be the smallest value so that at least 1% of the variance is retained.</p><p>B. Choose k to be the smallest value so that at least 99% of the variance is retained.</p><p>C. Choose the value of k that minimizes the approximation error $\frac{1}{m}\sum^{m}<em>{i=1}\left | x^{(i)} - x</em>{approx}^{(i)} \right |^{2}$.</p><p>D. Choose k to be 99% of n (i.e., k=0.99∗n, rounded to the nearest integer).</p><p>解答： B</p><h3 id=3-question-3>3. Question 3</h3><p>Suppose someone tells you that they ran PCA in such a way that &ldquo;95% of the variance was retained.&rdquo; What is an equivalent statement to this?</p><p>A. $\frac{\frac{1}{m}\sum^{m}<em>{i=1}\left | x^{(i)} \right |^{2}}{\frac{1}{m}\sum^{m}</em>{i=1}\left | x^{(i)} - x_{approx}^{(i)} \right |^{2}} \geqslant 0.05$</p><p>B. $\frac{\frac{1}{m}\sum^{m}<em>{i=1}\left | x^{(i)} \right |^{2}}{\frac{1}{m}\sum^{m}</em>{i=1}\left | x^{(i)} - x_{approx}^{(i)} \right |^{2}} \leqslant 0.95$</p><p>C. $\frac{\frac{1}{m}\sum^{m}<em>{i=1}\left | x^{(i)} - x</em>{approx}^{(i)} \right |^{2}}{\frac{1}{m}\sum^{m}_{i=1}\left | x^{(i)} \right |^{2}} \leqslant 0.05$</p><p>D. $\frac{\frac{1}{m}\sum^{m}<em>{i=1}\left | x^{(i)} \right |^{2}}{\frac{1}{m}\sum^{m}</em>{i=1}\left | x^{(i)} - x_{approx}^{(i)} \right |^{2}} \leqslant 0.05$</p><p>解答： C</p><h3 id=4-question-4>4. Question 4</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. If the input features are on very different scales, it is a good idea to perform feature scaling before applying PCA.</p><p>B. Feature scaling is not useful for PCA, since the eigenvector calculation (such as using Octave&rsquo;s svd(Sigma) routine) takes care of this automatically.</p><p>C. Given an input $x \in \mathbb{R}^{n}$, PCA compresses it to a lower-dimensional vector $z \in \mathbb{R}^{k}$.</p><p>D. PCA can be used only to reduce the dimensionality of data by 1 (such as 3D to 2D, or 2D to 1D).</p><p>解答：A、C</p><h3 id=5-question-5>5. Question 5</h3><p>Which of the following are recommended applications of PCA? Select all that apply.</p><p>A. To get more features to feed into a learning algorithm.</p><p>B. Data compression: Reduce the dimension of your data, so that it takes up less memory / disk space.</p><p>C. Data visualization: Reduce data to 2D (or 3D) so that it can be plotted.</p><p>D. Data compression: Reduce the dimension of your input data $x^{(i)}$, which will be used in a supervised learning algorithm (i.e., use PCA so that your supervised learning algorithm runs faster).</p><p>解答：B、C</p><hr><blockquote><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a></p><p>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a></p><p>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Dimensionality_Reduction.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Dimensionality_Reduction.ipynb</a></p></blockquote><img src=https://img.halfrost.com/wechat-qr-code.png></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#一-motivation>一. Motivation</a></li><li><a href=#二-principal-component-analysis-主成分分析>二. Principal Component Analysis 主成分分析</a><ul><li><a href=#1-区别>1. 区别</a></li><li><a href=#2-算法流程>2. 算法流程</a></li><li><a href=#3-特征还原>3. 特征还原</a></li><li><a href=#4-降维多少才合适>4. 降维多少才合适？</a></li><li><a href=#5-不要提前优化>5. 不要提前优化</a></li></ul></li><li><a href=#三-principal-component-analysis-测试>三. Principal Component Analysis 测试</a><ul><li><a href=#1-question-1>1. Question 1</a></li><li><a href=#2-question-2>2. Question 2</a></li><li><a href=#3-question-3>3. Question 3</a></li><li><a href=#4-question-4>4. Question 4</a></li><li><a href=#5-question-5>5. Question 5</a></li></ul></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f"><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&text=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&title=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&is_video=false&description=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f"><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&title=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&title=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&title=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-stumbleupon fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&title=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-digg fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&name=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fDimensionality_Reduction.ipynb%0a%20%e4%b8%80.%20Motivation%20%e6%88%91%e4%bb%ac%e5%be%88%e5%b8%8c%e6%9c%9b%e6%9c%89%e8%b6%b3%e5%a4%9f%e5%a4%9a%e7%9a%84%e7%89%b9%e5%be%81%ef%bc%88%e7%9f%a5%e8%af%86%ef%bc%89%e6%9d%a5%e4%bf%9d%e5%87%86%e5%ad%a6%e4%b9%a0%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%ae%ad%e7%bb%83%e6%95%88%e6%9e%9c%ef%bc%8c%e5%b0%a4%e5%85%b6%e5%9c%a8%e5%9b%be%e5%83%8f%e5%a4%84%e7%90%86%e8%bf%99%e7%b1%bb%e7%9a%84%e4%bb%bb%e5%8a%a1%e4%b8%ad%ef%bc%8c%e9%ab%98%e7%bb%b4%e7%89%b9%e5%be%81%e6%98%af%e5%9c%a8%e6%89%80%e9%9a%be%e5%85%8d%e7%9a%84%ef%bc%8c%e4%bd%86%e6%98%af%ef%bc%8c%e9%ab%98%e7%bb%b4%e7%9a%84%e7%89%b9%e5%be%81%e4%b9%9f%e6%9c%89%e5%87%a0%e4%b8%aa%e5%a6%82%e4%b8%8b%e4%b8%8d%e5%a5%bd%e7%9a%84%e5%9c%b0%e6%96%b9%ef%bc%9a%0a%20%e5%ad%a6%e4%b9%a0%e6%80%a7%e8%83%bd%e4%b8%8b%e9%99%8d%ef%bc%8c%e7%9f%a5%e8%af%86%e8%b6%8a%e5%a4%9a%ef%bc%8c%e5%90%b8%e6%94%b6%e7%9f%a5%e8%af%86%ef%bc%88%e8%be%93%e5%85%a5%ef%bc%89%ef%bc%8c%e5%b9%b6%e4%b8%94%e7%b2%be%e9%80%9a%e7%9f%a5%e8%af%86%ef%bc%88%e5%ad%a6%e4%b9%a0%ef%bc%89%e7%9a%84%e9%80%9f%e5%ba%a6%e5%b0%b1%e8%b6%8a%e6%85%a2%e3%80%82%20%e8%bf%87%e5%a4%9a%e7%9a%84%e7%89%b9%e5%be%81%e9%9a%be%e4%ba%8e%e5%88%86%e8%be%a8%ef%bc%8c%e4%bd%a0%e5%be%88%e9%9a%be%e7%ac%ac%e4%b8%80%e6%97%b6%e9%97%b4%e8%ae%a4%e8%af%86%e6%9f%90%e4%b8%aa%e7%89%b9%e5%be%81%e4%bb%a3%e8%a1%a8%e7%9a%84%e6%84%8f%e4%b9%89%e3%80%82%20%e7%89%b9%e5%be%81%e5%86%97%e4%bd%99%ef%bc%8c%e5%a6%82%e4%b8%8b%e5%9b%be%e6%89%80%e7%a4%ba%ef%bc%8c%e5%8e%98%e7%b1%b3%e5%92%8c%e8%8b%b1%e5%b0%ba%e5%b0%b1%e6%98%af%e4%b8%80%e5%af%b9%e5%86%97%e4%bd%99%e7%89%b9%e5%be%81%ef%bc%8c%e4%bb%96%e4%bb%ac%e6%9c%ac%e8%ba%ab%e4%bb%a3%e8%a1%a8%e7%9a%84%e6%84%8f%e4%b9%89%e6%98%af%e4%b8%80%e6%a0%b7%e7%9a%84%ef%bc%8c%e5%b9%b6%e4%b8%94%e8%83%bd%e5%a4%9f%e7%9b%b8%e4%ba%92%e8%bd%ac%e6%8d%a2%e3%80%82%20%20%e6%88%91%e4%bb%ac%e4%bd%bf%e7%94%a8%e7%8e%b0%e5%9c%a8%e4%bd%bf%e7%94%a8%e4%ba%86%e4%b8%80%e6%9d%a1%e7%bb%bf%e8%89%b2%e7%9b%b4%e7%ba%bf%ef%bc%8c%e5%b0%86%e5%90%84%e4%b8%aa%e6%a0%b7%e6%9c%ac%e6%8a%95%e5%bd%b1%e5%88%b0%e8%af%a5%e7%9b%b4%e7%ba%bf%ef%bc%8c%e9%82%a3%e4%b9%88%ef%bc%8c%e5%8e%9f%e6%9d%a5%e4%ba%8c%e7%bb%b4%e7%9a%84%e7%89%b9%e5%be%81%20x%3d%28%e5%8e%98%e7%b1%b3%ef%bc%8c%e8%8b%b1%e5%b0%ba%29%20%e5%b0%b1%e8%a2%ab%e9%99%8d%e4%bd%8e%e4%b8%ba%e4%ba%86%e4%b8%80%e7%bb%b4%20x%3d%28%e7%9b%b4%e7%ba%bf%e4%b8%8a%e7%9a%84%e7%9b%b8%e5%af%b9%e4%bd%8d%e7%bd%ae%29%0a%e8%80%8c%e5%9c%a8%e4%b8%8b%e9%9d%a2%e7%9a%84%e4%be%8b%e5%ad%90%e4%b8%ad%ef%bc%8c%e6%88%91%e4%bb%ac%e5%8f%88%e5%b0%86%e4%b8%89%e7%bb%b4%e7%89%b9%e5%be%81%e6%8a%95%e5%bd%b1%e5%88%b0%e4%ba%8c%e4%bd%8d%e5%b9%b3%e9%9d%a2%ef%bc%8c%e4%bb%8e%e8%80%8c%e5%b0%86%e4%b8%89%e7%bb%b4%e7%89%b9%e5%be%81%e9%99%8d%e5%88%b0%e4%ba%86%e4%ba%8c%e7%bb%b4%ef%bc%9a%0a%e7%89%b9%e5%be%81%e9%99%8d%e7%bb%b4%e7%9a%84%e4%b8%80%e8%88%ac%e6%89%8b%e6%ae%b5%e5%b0%b1%e6%98%af%e5%b0%86%e9%ab%98%e7%bb%b4%e7%89%b9%e5%be%81%e6%8a%95%e5%bd%b1%e5%88%b0%e4%bd%8e%e7%bb%b4%e7%a9%ba%e9%97%b4%e3%80%82%0a%20%e4%ba%8c.%20Principal%20Component%20Analysis%20%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90%20PCA%ef%bc%8cPrinciple%20Component%20Analysis%ef%bc%8c%e5%8d%b3%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90%e6%b3%95%ef%bc%8c%e6%98%af%e7%89%b9%e5%be%81%e9%99%8d%e7%bb%b4%e7%9a%84%e6%9c%80%e5%b8%b8%e7%94%a8%e6%89%8b%e6%ae%b5%e3%80%82%e9%a1%be%e5%90%8d%e6%80%9d%e4%b9%89%ef%bc%8cPCA%20%e8%83%bd%e4%bb%8e%e5%86%97%e4%bd%99%e7%89%b9%e5%be%81%e4%b8%ad%e6%8f%90%e5%8f%96%e4%b8%bb%e8%a6%81%e6%88%90%e5%88%86%ef%bc%8c%e5%9c%a8%e4%b8%8d%e5%a4%aa%e6%8d%9f%e5%a4%b1%e6%a8%a1%e5%9e%8b%e8%b4%a8%e9%87%8f%e7%9a%84%e6%83%85%e5%86%b5%e4%b8%8b%ef%bc%8c%e6%8f%90%e5%8d%87%e4%ba%86%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%e9%80%9f%e5%ba%a6%e3%80%82%0a%e5%a6%82%e4%b8%8a%e5%9b%be%e6%89%80%e7%a4%ba%ef%bc%8c%e6%88%91%e4%bb%ac%e5%b0%86%e6%a0%b7%e6%9c%ac%e5%88%b0%e7%ba%a2%e8%89%b2%e5%90%91%e9%87%8f%e7%9a%84%e8%b7%9d%e7%a6%bb%e7%a7%b0%e4%bd%9c%e6%98%af%e6%8a%95%e5%bd%b1%e8%af%af%e5%b7%ae%ef%bc%88Projection%20Error%ef%bc%89%e3%80%82%e4%bb%a5%e4%ba%8c%e7%bb%b4%e6%8a%95%e5%bd%b1%e5%88%b0%e4%b8%80%e7%bb%b4%e4%b8%ba%e4%be%8b%ef%bc%8cPCA%20%e5%b0%b1%e6%98%af%e8%a6%81%e6%89%be%e5%af%bb%e4%b8%80%e6%9d%a1%e7%9b%b4%e7%ba%bf%ef%bc%8c%e4%bd%bf%e5%be%97%e5%90%84%e4%b8%aa%e7%89%b9%e5%be%81%e7%9a%84%e6%8a%95%e5%bd%b1%e8%af%af%e5%b7%ae%e8%b6%b3%e5%a4%9f%e5%b0%8f%ef%bc%8c%e8%bf%99%e6%a0%b7%e6%89%8d%e8%83%bd%e5%b0%bd%e5%8f%af%e8%83%bd%e7%9a%84%e4%bf%9d%e7%95%99%e5%8e%9f%e7%89%b9%e5%be%81%e5%85%b7%e6%9c%89%e7%9a%84%e4%bf%a1%e6%81%af%e3%80%82%0a%e5%81%87%e8%ae%be%e6%88%91%e4%bb%ac%e8%a6%81%e5%b0%86%e7%89%b9%e5%be%81%e4%bb%8e%20n%20%e7%bb%b4%e5%ba%a6%e9%99%8d%e5%88%b0%20k%20%e7%bb%b4%ef%bc%9aPCA%20%e9%a6%96%e5%85%88%e6%89%be%e5%af%bb%20k%20%e4%b8%aa%20n%20%e7%bb%b4%e5%90%91%e9%87%8f%ef%bc%8c%e7%84%b6%e5%90%8e%e5%b0%86%e7%89%b9%e5%be%81%e6%8a%95%e5%bd%b1%e5%88%b0%e8%bf%99%e4%ba%9b%e5%90%91%e9%87%8f%e6%9e%84%e6%88%90%e7%9a%84%20k%20%e7%bb%b4%e7%a9%ba%e9%97%b4%ef%bc%8c%e5%b9%b6%e4%bf%9d%e8%af%81%e6%8a%95%e5%bd%b1%e8%af%af%e5%b7%ae%e8%b6%b3%e5%a4%9f%e5%b0%8f%e3%80%82%e4%b8%8b%e5%9b%be%e4%b8%ad%e4%b8%ad%ef%bc%8c%e4%b8%ba%e4%ba%86%e5%b0%86%e7%89%b9%e5%be%81%e7%bb%b4%e5%ba%a6%e4%bb%8e%e4%b8%89%e7%bb%b4%e9%99%8d%e4%bd%8e%e5%88%b0%e4%ba%8c%e4%bd%8d%ef%bc%8cPCA%20%e5%b0%b1%e4%bc%9a%e5%85%88%e6%89%be%e5%af%bb%e4%b8%a4%e4%b8%aa%e4%b8%89%e7%bb%b4%e5%90%91%e9%87%8f%20%24%5cmu%5e%7b%281%29%7d%2c%5cmu%5e%7b%282%29%7d%24%20%ef%bc%8c%e4%ba%8c%e8%80%85%e6%9e%84%e6%88%90%e4%ba%86%e4%b8%80%e4%b8%aa%e4%ba%8c%e7%bb%b4%e5%b9%b3%e9%9d%a2%ef%bc%8c%e7%84%b6%e5%90%8e%e5%b0%86%e5%8e%9f%e6%9d%a5%e7%9a%84%e4%b8%89%e7%bb%b4%e7%89%b9%e5%be%81%e6%8a%95%e5%bd%b1%e5%88%b0%e8%af%a5%e4%ba%8c%e7%bb%b4%e5%b9%b3%e9%9d%a2%e4%b8%8a%ef%bc%9a%0a1.%20%e5%8c%ba%e5%88%ab%20PCA%20%e5%92%8c%20%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e7%9a%84%e5%8c%ba%e5%88%ab%e6%98%af%ef%bc%9a%0a%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e6%89%be%e7%9a%84%e6%98%af%e5%9e%82%e7%9b%b4%e4%ba%8e%20X%20%e8%bd%b4%e8%b7%9d%e7%a6%bb%e6%9c%80%e5%b0%8f%e5%80%bc%ef%bc%8cPCA%20%e6%89%be%e7%9a%84%e6%98%af%e6%8a%95%e5%bd%b1%e5%9e%82%e7%9b%b4%e8%b7%9d%e7%a6%bb%e6%9c%80%e5%b0%8f%e5%80%bc%e3%80%82"><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2fdimensionality_reduction%2f&t=PCA%20%e4%b8%8e%e9%99%8d%e7%bb%b4"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu class=icon href=# onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden=true></i>Menu</a>
<a id=toc class=icon href=# onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden=true></i>TOC</a>
<a id=share class=icon href=# onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden=true></i>share</a>
<a id=top style=display:none class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i>Top</a></div></div></div><footer id=footer><div class=footer-left><p class=copyright style=float:left;margin-bottom:0><a href=https://github.com/halfrost/Halfrost-Field class=github-repo style=height:18px><span class=gadget-github></span>Star</a>
Copyright &copy;halfrost 2016 - 2021
<a href=http://www.miit.gov.cn/>鄂ICP备16014744号</a></p><br><p class="copyright statistics" style=margin-bottom:20px><span id=busuanzi_container_site_pv>Cumulative Page Views <span id=busuanzi_value_site_pv></span>| Unique Visitors <span id=busuanzi_value_site_uv></span></span></p></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script><script src=/main.min.f870a4d110314b9e50e65f8ac982dc1c9c376c8f1a5083d39c62cfc49073f011.js></script><script async src=/prism.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}};</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>