<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=theme-color content="#FFFFFF"><meta http-equiv=x-ua-compatible content="IE=edge"><title>大规模机器学习中如何优化算法？ | prometheus</title><meta name=description content="Explore in every moment of the hard thinking"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="大规模机器学习中如何优化算法？"><meta property="og:description" content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Large_Scale_Machine_Learning.ipynb
 一. Gradient Descent with Large Datasets 如果我们有一个低方差的模型，增加数据集的规模可以帮助你获得更好的结果。我们应该怎样应对一个有100万条记录的训练集？
以线性回归模型为例，每一次梯度下降迭代，我们都需要计算训练集的误差的平方和，如果我们的学习算法需要有20次迭代，这便已经是非常大的计算代价。
首先应该做的事是去检查一个这么大规模的训练集是否真的必要，也许我们只用1000个训练集也能获得较好的效果，我们可以绘制学习曲线来帮助判断。
 二. Advanced Topics 1. 批量梯度下降法（Batch gradient descent） 拥有了大数据，就意味着，我们的算法模型中得面临一个很大的 m 值。回顾到我们的批量梯度下降法：
重复直到收敛：
$$\theta_j=\theta_j-\alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j,\;\;\;\;for\;\;j=0,\cdots,n$$
可以看到，每更新一个参数 $\theta_j$ ，我们都不得不遍历一遍样本集，在 m 很大时，该算法就显得比较低效。但是，批量梯度下降法能找到全局最优解：
2. 随机梯度下降法（Stochastic gradient descent） 针对大数据集，又引入了随机梯度下降法，该算法的执行过程为：
重复直到收敛：
$$ \begin{align*} for\;\;\;i&=1,\cdots,m:\
\theta_j&=\theta_j-\alpha(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j,\;\;\;\;for\;\;j=0,\cdots,n\
\end{align*} $$
相较于批量梯度下降法，随机梯度下降法每次更新 $\theta_j$ 只会用当前遍历的样本。虽然外层循环仍需要遍历所有样本，但是，往往我们能在样本尚未遍历完时就已经收敛，因此，面临大数据集时，随机梯度下降法性能卓越。
上图反映了随机梯度下降法找寻最优解的过程，相较于批量梯度下降法，随机梯度下降法的曲线就显得不是那么平滑，而是很曲折了，其也倾向于找到局部最优解而不是全局最优解。因此，我们通常需要绘制调试曲线来监控随机梯度的工作过程是否正确。例如，假定误差定义为 $cost(\theta,(x^{(i)},y^{(i)}))=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$，则每完成 1000 次迭代，即遍历了 1000 个样本，我们求取平均误差并进行绘制，得到误差随迭代次数的变化曲线："><meta property="og:type" content="article"><meta property="og:url" content="https://new.halfrost.com/large_scale_machine_learning/"><meta property="article:published_time" content="2018-04-03T18:28:00+00:00"><meta property="article:modified_time" content="2018-04-03T18:28:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="大规模机器学习中如何优化算法？"><meta name=twitter:description content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Large_Scale_Machine_Learning.ipynb
 一. Gradient Descent with Large Datasets 如果我们有一个低方差的模型，增加数据集的规模可以帮助你获得更好的结果。我们应该怎样应对一个有100万条记录的训练集？
以线性回归模型为例，每一次梯度下降迭代，我们都需要计算训练集的误差的平方和，如果我们的学习算法需要有20次迭代，这便已经是非常大的计算代价。
首先应该做的事是去检查一个这么大规模的训练集是否真的必要，也许我们只用1000个训练集也能获得较好的效果，我们可以绘制学习曲线来帮助判断。
 二. Advanced Topics 1. 批量梯度下降法（Batch gradient descent） 拥有了大数据，就意味着，我们的算法模型中得面临一个很大的 m 值。回顾到我们的批量梯度下降法：
重复直到收敛：
$$\theta_j=\theta_j-\alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j,\;\;\;\;for\;\;j=0,\cdots,n$$
可以看到，每更新一个参数 $\theta_j$ ，我们都不得不遍历一遍样本集，在 m 很大时，该算法就显得比较低效。但是，批量梯度下降法能找到全局最优解：
2. 随机梯度下降法（Stochastic gradient descent） 针对大数据集，又引入了随机梯度下降法，该算法的执行过程为：
重复直到收敛：
$$ \begin{align*} for\;\;\;i&=1,\cdots,m:\
\theta_j&=\theta_j-\alpha(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j,\;\;\;\;for\;\;j=0,\cdots,n\
\end{align*} $$
相较于批量梯度下降法，随机梯度下降法每次更新 $\theta_j$ 只会用当前遍历的样本。虽然外层循环仍需要遍历所有样本，但是，往往我们能在样本尚未遍历完时就已经收敛，因此，面临大数据集时，随机梯度下降法性能卓越。
上图反映了随机梯度下降法找寻最优解的过程，相较于批量梯度下降法，随机梯度下降法的曲线就显得不是那么平滑，而是很曲折了，其也倾向于找到局部最优解而不是全局最优解。因此，我们通常需要绘制调试曲线来监控随机梯度的工作过程是否正确。例如，假定误差定义为 $cost(\theta,(x^{(i)},y^{(i)}))=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$，则每完成 1000 次迭代，即遍历了 1000 个样本，我们求取平均误差并进行绘制，得到误差随迭代次数的变化曲线："><link rel=stylesheet href=/css/style-white.min.css><link rel=manifest href=/manifest.json><link rel=stylesheet href=/prism.css><link href=/images/apple-touch-icon-60x60.png rel=apple-touch-icon sizes=60x60><link href=/images/apple-touch-icon-76x76.png rel=apple-touch-icon sizes=76x76><link href=/images/apple-touch-icon-120x120.png rel=apple-touch-icon sizes=120x120><link href=/images/apple-touch-icon-152x152.png rel=apple-touch-icon sizes=152x152><link href=/images/apple-touch-icon-180x180.png rel=apple-touch-icon sizes=180x180><link href=/images/apple-touch-icon-512x512.png rel=apple-touch-icon sizes=512x512><link href=/images/apple-touch-icon-1024x1024.png rel=apple-touch-icon sizes=1024x1024><script async>if('serviceWorker'in navigator){navigator.serviceWorker.register("\/serviceworker-v1.min.a64912b78d282eab1ad3715a0943da21616e5f326f8afea27034784ad445043b.js").then(function(){if(navigator.serviceWorker.controller){console.log('Assets cached by the controlling service worker.');}else{console.log('Please reload this page to allow the service worker to handle network operations.');}}).catch(function(error){console.log('ERROR: '+error);});}else{console.log('Service workers are not supported in the current browser.');}</script><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://new.halfrost.com/images/favicon.ico><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-82753806-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class="single-max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a><a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a><a id=top-icon-tablet href=# onclick="$('html, body').animate({scrollTop:0},'fast');" style=display:none><i class="fas fa-chevron-up fa-lg"></i></a><span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://new.halfrost.com/recommender_systems/><i class="fas fa-chevron-left" aria-hidden=true onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li><li><a class=icon href=https://new.halfrost.com/application_example_photo_ocr/><i class="fas fa-chevron-right" aria-hidden=true onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li><li><a class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up" aria-hidden=true onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li><li><a class=icon href=#><i class="fas fa-share-alt" aria-hidden=true onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f"><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&text=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&title=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&is_video=false&description=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f"><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&title=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&title=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&title=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-stumbleupon" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&title=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-digg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&name=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fLarge_Scale_Machine_Learning.ipynb%0a%20%e4%b8%80.%20Gradient%20Descent%20with%20Large%20Datasets%20%e5%a6%82%e6%9e%9c%e6%88%91%e4%bb%ac%e6%9c%89%e4%b8%80%e4%b8%aa%e4%bd%8e%e6%96%b9%e5%b7%ae%e7%9a%84%e6%a8%a1%e5%9e%8b%ef%bc%8c%e5%a2%9e%e5%8a%a0%e6%95%b0%e6%8d%ae%e9%9b%86%e7%9a%84%e8%a7%84%e6%a8%a1%e5%8f%af%e4%bb%a5%e5%b8%ae%e5%8a%a9%e4%bd%a0%e8%8e%b7%e5%be%97%e6%9b%b4%e5%a5%bd%e7%9a%84%e7%bb%93%e6%9e%9c%e3%80%82%e6%88%91%e4%bb%ac%e5%ba%94%e8%af%a5%e6%80%8e%e6%a0%b7%e5%ba%94%e5%af%b9%e4%b8%80%e4%b8%aa%e6%9c%89100%e4%b8%87%e6%9d%a1%e8%ae%b0%e5%bd%95%e7%9a%84%e8%ae%ad%e7%bb%83%e9%9b%86%ef%bc%9f%0a%e4%bb%a5%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e6%a8%a1%e5%9e%8b%e4%b8%ba%e4%be%8b%ef%bc%8c%e6%af%8f%e4%b8%80%e6%ac%a1%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e8%bf%ad%e4%bb%a3%ef%bc%8c%e6%88%91%e4%bb%ac%e9%83%bd%e9%9c%80%e8%a6%81%e8%ae%a1%e7%ae%97%e8%ae%ad%e7%bb%83%e9%9b%86%e7%9a%84%e8%af%af%e5%b7%ae%e7%9a%84%e5%b9%b3%e6%96%b9%e5%92%8c%ef%bc%8c%e5%a6%82%e6%9e%9c%e6%88%91%e4%bb%ac%e7%9a%84%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e9%9c%80%e8%a6%81%e6%9c%8920%e6%ac%a1%e8%bf%ad%e4%bb%a3%ef%bc%8c%e8%bf%99%e4%be%bf%e5%b7%b2%e7%bb%8f%e6%98%af%e9%9d%9e%e5%b8%b8%e5%a4%a7%e7%9a%84%e8%ae%a1%e7%ae%97%e4%bb%a3%e4%bb%b7%e3%80%82%0a%e9%a6%96%e5%85%88%e5%ba%94%e8%af%a5%e5%81%9a%e7%9a%84%e4%ba%8b%e6%98%af%e5%8e%bb%e6%a3%80%e6%9f%a5%e4%b8%80%e4%b8%aa%e8%bf%99%e4%b9%88%e5%a4%a7%e8%a7%84%e6%a8%a1%e7%9a%84%e8%ae%ad%e7%bb%83%e9%9b%86%e6%98%af%e5%90%a6%e7%9c%9f%e7%9a%84%e5%bf%85%e8%a6%81%ef%bc%8c%e4%b9%9f%e8%ae%b8%e6%88%91%e4%bb%ac%e5%8f%aa%e7%94%a81000%e4%b8%aa%e8%ae%ad%e7%bb%83%e9%9b%86%e4%b9%9f%e8%83%bd%e8%8e%b7%e5%be%97%e8%be%83%e5%a5%bd%e7%9a%84%e6%95%88%e6%9e%9c%ef%bc%8c%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e7%bb%98%e5%88%b6%e5%ad%a6%e4%b9%a0%e6%9b%b2%e7%ba%bf%e6%9d%a5%e5%b8%ae%e5%8a%a9%e5%88%a4%e6%96%ad%e3%80%82%0a%20%e4%ba%8c.%20Advanced%20Topics%201.%20%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%88Batch%20gradient%20descent%ef%bc%89%20%e6%8b%a5%e6%9c%89%e4%ba%86%e5%a4%a7%e6%95%b0%e6%8d%ae%ef%bc%8c%e5%b0%b1%e6%84%8f%e5%91%b3%e7%9d%80%ef%bc%8c%e6%88%91%e4%bb%ac%e7%9a%84%e7%ae%97%e6%b3%95%e6%a8%a1%e5%9e%8b%e4%b8%ad%e5%be%97%e9%9d%a2%e4%b8%b4%e4%b8%80%e4%b8%aa%e5%be%88%e5%a4%a7%e7%9a%84%20m%20%e5%80%bc%e3%80%82%e5%9b%9e%e9%a1%be%e5%88%b0%e6%88%91%e4%bb%ac%e7%9a%84%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%9a%0a%e9%87%8d%e5%a4%8d%e7%9b%b4%e5%88%b0%e6%94%b6%e6%95%9b%ef%bc%9a%0a%24%24%5ctheta_j%3d%5ctheta_j-%5calpha%20%5cfrac%7b1%7d%7bm%7d%20%5csum_%7bi%3d1%7d%5e%7bm%7d%28h_%7b%5ctheta%7d%28x%5e%7b%28i%29%7d%29-y%5e%7b%28i%29%7d%29x%5e%7b%28i%29%7d_j%2c%5c%3b%5c%3b%5c%3b%5c%3bfor%5c%3b%5c%3bj%3d0%2c%5ccdots%2cn%24%24%0a%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%88%b0%ef%bc%8c%e6%af%8f%e6%9b%b4%e6%96%b0%e4%b8%80%e4%b8%aa%e5%8f%82%e6%95%b0%20%24%5ctheta_j%24%20%ef%bc%8c%e6%88%91%e4%bb%ac%e9%83%bd%e4%b8%8d%e5%be%97%e4%b8%8d%e9%81%8d%e5%8e%86%e4%b8%80%e9%81%8d%e6%a0%b7%e6%9c%ac%e9%9b%86%ef%bc%8c%e5%9c%a8%20m%20%e5%be%88%e5%a4%a7%e6%97%b6%ef%bc%8c%e8%af%a5%e7%ae%97%e6%b3%95%e5%b0%b1%e6%98%be%e5%be%97%e6%af%94%e8%be%83%e4%bd%8e%e6%95%88%e3%80%82%e4%bd%86%e6%98%af%ef%bc%8c%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e8%83%bd%e6%89%be%e5%88%b0%e5%85%a8%e5%b1%80%e6%9c%80%e4%bc%98%e8%a7%a3%ef%bc%9a%0a2.%20%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%88Stochastic%20gradient%20descent%ef%bc%89%20%e9%92%88%e5%af%b9%e5%a4%a7%e6%95%b0%e6%8d%ae%e9%9b%86%ef%bc%8c%e5%8f%88%e5%bc%95%e5%85%a5%e4%ba%86%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%8c%e8%af%a5%e7%ae%97%e6%b3%95%e7%9a%84%e6%89%a7%e8%a1%8c%e8%bf%87%e7%a8%8b%e4%b8%ba%ef%bc%9a%0a%e9%87%8d%e5%a4%8d%e7%9b%b4%e5%88%b0%e6%94%b6%e6%95%9b%ef%bc%9a%0a%24%24%20%5cbegin%7balign%2a%7d%20for%5c%3b%5c%3b%5c%3bi%26amp%3b%3d1%2c%5ccdots%2cm%3a%5c%0a%5ctheta_j%26amp%3b%3d%5ctheta_j-%5calpha%28h_%5ctheta%28x%5e%7b%28i%29%7d%29-y%5e%7b%28i%29%7d%29x%5e%7b%28i%29%7d_j%2c%5c%3b%5c%3b%5c%3b%5c%3bfor%5c%3b%5c%3bj%3d0%2c%5ccdots%2cn%5c%0a%5cend%7balign%2a%7d%20%24%24%0a%e7%9b%b8%e8%be%83%e4%ba%8e%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%8c%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e6%af%8f%e6%ac%a1%e6%9b%b4%e6%96%b0%20%24%5ctheta_j%24%20%e5%8f%aa%e4%bc%9a%e7%94%a8%e5%bd%93%e5%89%8d%e9%81%8d%e5%8e%86%e7%9a%84%e6%a0%b7%e6%9c%ac%e3%80%82%e8%99%bd%e7%84%b6%e5%a4%96%e5%b1%82%e5%be%aa%e7%8e%af%e4%bb%8d%e9%9c%80%e8%a6%81%e9%81%8d%e5%8e%86%e6%89%80%e6%9c%89%e6%a0%b7%e6%9c%ac%ef%bc%8c%e4%bd%86%e6%98%af%ef%bc%8c%e5%be%80%e5%be%80%e6%88%91%e4%bb%ac%e8%83%bd%e5%9c%a8%e6%a0%b7%e6%9c%ac%e5%b0%9a%e6%9c%aa%e9%81%8d%e5%8e%86%e5%ae%8c%e6%97%b6%e5%b0%b1%e5%b7%b2%e7%bb%8f%e6%94%b6%e6%95%9b%ef%bc%8c%e5%9b%a0%e6%ad%a4%ef%bc%8c%e9%9d%a2%e4%b8%b4%e5%a4%a7%e6%95%b0%e6%8d%ae%e9%9b%86%e6%97%b6%ef%bc%8c%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e6%80%a7%e8%83%bd%e5%8d%93%e8%b6%8a%e3%80%82%0a%e4%b8%8a%e5%9b%be%e5%8f%8d%e6%98%a0%e4%ba%86%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e6%89%be%e5%af%bb%e6%9c%80%e4%bc%98%e8%a7%a3%e7%9a%84%e8%bf%87%e7%a8%8b%ef%bc%8c%e7%9b%b8%e8%be%83%e4%ba%8e%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%8c%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e7%9a%84%e6%9b%b2%e7%ba%bf%e5%b0%b1%e6%98%be%e5%be%97%e4%b8%8d%e6%98%af%e9%82%a3%e4%b9%88%e5%b9%b3%e6%bb%91%ef%bc%8c%e8%80%8c%e6%98%af%e5%be%88%e6%9b%b2%e6%8a%98%e4%ba%86%ef%bc%8c%e5%85%b6%e4%b9%9f%e5%80%be%e5%90%91%e4%ba%8e%e6%89%be%e5%88%b0%e5%b1%80%e9%83%a8%e6%9c%80%e4%bc%98%e8%a7%a3%e8%80%8c%e4%b8%8d%e6%98%af%e5%85%a8%e5%b1%80%e6%9c%80%e4%bc%98%e8%a7%a3%e3%80%82%e5%9b%a0%e6%ad%a4%ef%bc%8c%e6%88%91%e4%bb%ac%e9%80%9a%e5%b8%b8%e9%9c%80%e8%a6%81%e7%bb%98%e5%88%b6%e8%b0%83%e8%af%95%e6%9b%b2%e7%ba%bf%e6%9d%a5%e7%9b%91%e6%8e%a7%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e7%9a%84%e5%b7%a5%e4%bd%9c%e8%bf%87%e7%a8%8b%e6%98%af%e5%90%a6%e6%ad%a3%e7%a1%ae%e3%80%82%e4%be%8b%e5%a6%82%ef%bc%8c%e5%81%87%e5%ae%9a%e8%af%af%e5%b7%ae%e5%ae%9a%e4%b9%89%e4%b8%ba%20%24cost%28%5ctheta%2c%28x%5e%7b%28i%29%7d%2cy%5e%7b%28i%29%7d%29%29%3d%5cfrac%7b1%7d%7b2%7d%28h_%5ctheta%28x%5e%7b%28i%29%7d%29-y%5e%7b%28i%29%7d%29%5e2%24%ef%bc%8c%e5%88%99%e6%af%8f%e5%ae%8c%e6%88%90%201000%20%e6%ac%a1%e8%bf%ad%e4%bb%a3%ef%bc%8c%e5%8d%b3%e9%81%8d%e5%8e%86%e4%ba%86%201000%20%e4%b8%aa%e6%a0%b7%e6%9c%ac%ef%bc%8c%e6%88%91%e4%bb%ac%e6%b1%82%e5%8f%96%e5%b9%b3%e5%9d%87%e8%af%af%e5%b7%ae%e5%b9%b6%e8%bf%9b%e8%a1%8c%e7%bb%98%e5%88%b6%ef%bc%8c%e5%be%97%e5%88%b0%e8%af%af%e5%b7%ae%e9%9a%8f%e8%bf%ad%e4%bb%a3%e6%ac%a1%e6%95%b0%e7%9a%84%e5%8f%98%e5%8c%96%e6%9b%b2%e7%ba%bf%ef%bc%9a"><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&t=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><nav id=TableOfContents><ul><li><a href=#一-gradient-descent-with-large-datasets>一. Gradient Descent with Large Datasets</a></li><li><a href=#二-advanced-topics>二. Advanced Topics</a><ul><li><a href=#1-批量梯度下降法batch-gradient-descent>1. 批量梯度下降法（Batch gradient descent）</a></li><li><a href=#2-随机梯度下降法stochastic-gradient-descent>2. 随机梯度下降法（Stochastic gradient descent）</a></li><li><a href=#3-mini-批量梯度下降法mini-batch-gradient-descent>3. Mini 批量梯度下降法（Mini-batch gradient descent）</a></li><li><a href=#4-在线学习>4. 在线学习</a></li><li><a href=#5-mapreduce>5. MapReduce</a></li></ul></li><li><a href=#三-large-scale-machine-learning-测试>三. Large Scale Machine Learning 测试</a><ul><li><a href=#1-question-1>1. Question 1</a></li><li><a href=#2-question-2>2. Question 2</a></li><li><a href=#3-question-3>3. Question 3</a></li><li><a href=#4-question-4>4. Question 4</a></li><li><a href=#5-question-5>5. Question 5</a></li></ul></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">大规模机器学习中如何优化算法？</h1><div class=meta><div class=postdate><time datetime="2018-04-03 18:28:00 +0000 UTC" itemprop=datePublished>Apr 03</time></div><div class=article-category><i class="fas fa-archive"></i><a class=category-link href=/categories/machine-learning>Machine Learning</a>
,
<a class=category-link href=/categories/ai>AI</a></div><div class=article-tag><i class="fas fa-tag"></i><a class=tag-link href=/tags/machine-learning rel=tag>Machine Learning</a>
,
<a class=tag-link href=/tags/ai rel=tag>AI</a></div></div></header><div class=content itemprop=articleBody><blockquote><p>由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/contents.md>Github</a> 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。</p><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a><br>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a><br>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Large_Scale_Machine_Learning.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Large_Scale_Machine_Learning.ipynb</a></p></blockquote><h2 id=一-gradient-descent-with-large-datasets>一. Gradient Descent with Large Datasets</h2><p>如果我们有一个低方差的模型，增加数据集的规模可以帮助你获得更好的结果。我们应该怎样应对一个有100万条记录的训练集？</p><p>以线性回归模型为例，每一次梯度下降迭代，我们都需要计算训练集的误差的平方和，如果我们的学习算法需要有20次迭代，这便已经是非常大的计算代价。</p><p>首先应该做的事是去检查一个这么大规模的训练集是否真的必要，也许我们只用1000个训练集也能获得较好的效果，我们可以绘制学习曲线来帮助判断。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/81_1.png alt></p><hr><h2 id=二-advanced-topics>二. Advanced Topics</h2><h3 id=1-批量梯度下降法batch-gradient-descent>1. 批量梯度下降法（Batch gradient descent）</h3><p>拥有了大数据，就意味着，我们的算法模型中得面临一个很大的 m 值。回顾到我们的批量梯度下降法：</p><p>重复直到收敛：</p><p>$$\theta_j=\theta_j-\alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j,\;\;\;\;for\;\;j=0,\cdots,n$$</p><p>可以看到，每更新一个参数 $\theta_j$ ，我们都不得不遍历一遍样本集，在 m 很大时，该算法就显得比较低效。但是，批量梯度下降法能找到全局最优解：</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/81_2.png alt></p><h3 id=2-随机梯度下降法stochastic-gradient-descent>2. 随机梯度下降法（Stochastic gradient descent）</h3><p>针对大数据集，又引入了随机梯度下降法，该算法的执行过程为：</p><p>重复直到收敛：</p><p>$$
\begin{align*}
for\;\;\;i&=1,\cdots,m:\<br>\theta_j&=\theta_j-\alpha(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j,\;\;\;\;for\;\;j=0,\cdots,n\<br>\end{align*}
$$</p><p>相较于批量梯度下降法，随机梯度下降法每次更新 $\theta_j$ 只会用当前遍历的样本。虽然外层循环仍需要遍历所有样本，但是，往往我们能在样本尚未遍历完时就已经收敛，因此，面临大数据集时，随机梯度下降法性能卓越。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/81_3.png alt></p><p>上图反映了随机梯度下降法找寻最优解的过程，相较于批量梯度下降法，随机梯度下降法的曲线就显得不是那么平滑，而是很曲折了，其也倾向于找到局部最优解而不是全局最优解。因此，我们通常需要绘制调试曲线来监控随机梯度的工作过程是否正确。例如，假定误差定义为 $cost(\theta,(x^{(i)},y^{(i)}))=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$，则每完成 1000 次迭代，即遍历了 1000 个样本，我们求取平均误差并进行绘制，得到误差随迭代次数的变化曲线：</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/81_4.png alt></p><p>另外，遇到下面的曲线也不用担心，其并不意味着我们的学习率出了问题，有可能是我们的平均间隔取的太小：</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/81_5.png alt></p><p>如果，我们每进行 5000 次迭代才进行绘制，那么曲线将更加平滑：</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/81_6.png alt></p><p>如果我们面临明显上升态势的曲线，就要考虑降低学习率 $\alpha$ 了：</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/81_7.png alt></p><p>学习率 $\alpha$ 还可以随着迭代次数进行优化</p><p>$$\alpha=\frac{constant1}{iterationNumber+constant2}$$</p><p>随着迭代次数的增多，我们的下降步调就会放缓，避免出现抖动：</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/81_8.png alt></p><blockquote><p>随机梯度下降法工作前，需要先随机化乱序数据集，是的遍历样本的过程更加分散。</p></blockquote><h3 id=3-mini-批量梯度下降法mini-batch-gradient-descent>3. Mini 批量梯度下降法（Mini-batch gradient descent）</h3><p>Mini 批量梯度下降法是批量梯度下降法和随机梯度下降法的折中，通过参数 b 指明了每次迭代时，用于更新 $\theta$ 的样本数。假定 b=10,m=1000 ，Mini 批量梯度下降法的工作过程如下：</p><p>重复直到收敛：</p><p>$$
\begin{align*}
for;;;i&=1,11,21,\cdots,991:\<br>\theta_j&=\theta_j-\alpha \frac{1}{10}\sum_{k=i}^{i+9}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j,;;;;for;;j=0,\cdots,n\<br>\end{align*}
$$</p><hr><h3 id=4-在线学习>4. 在线学习</h3><p>用户登录了某提供货运服务的网站，输入了货运的发件地址和收件地址，该网站给出了货运报价，用户决定是购买该服务（ y=1 ）或者是放弃购买该服务（ y=0 ）。</p><p>特征向量 x 包括了收发地址，报价信息，我们想要学习 $p(y=1|x;\theta)$ 来最优化报价：</p><p>重复直到收敛：</p><p>获得关于该用户的样本 (x,y)，使用该样本更新 $\theta$：</p><p>$$\theta_j=\theta_j-\alpha(h_\theta(x)-y)x_j,;;;for;;j=0,\cdots,n$$</p><p>这就是<strong>在线学习（Online learning）</strong>，与前面章节提到的机器学习过程不同，在线学习并不需要一个固定的样本集进行学习，而是不断接收样本，不断通过接收到的样本进行学习。因此，在线学习的前提是：我们面临着流动的数据。</p><h3 id=5-mapreduce>5. MapReduce</h3><p>前面，我们提到了 Mini 批量梯度下降法，假定 b=400,m=400,000,000 ，我们对 $\theta$ 的优化就为：</p><p>$$\theta_j=\theta_j-\alpha \frac{1}{400} \sum i=1^{400}(h_\theta(x^i)-y^{(i)})x^{(i)}_j$$</p><p>假定我们有 4 个机器（Machine），我们首先通过 Map (映射) 过程来并行计算式中的求和项，每个机器被分配到 100 个样本进行计算：</p><p>$$
\begin{align*}
temp^{(1)}_j&=\sum_{i=1}^{100}(h_\theta(x^{(i)}-y^{(i)})x^{(i)}_j\<br>temp^{(2)}_j&=\sum_{i=101}^{200}(h_\theta(x^{(i)}-y^{(i)})x^{(i)}_j\<br>temp^{(3)}_j&=\sum_{i=201}^{300}(h_\theta(x^{(i)}-y^{(i)})x^{(i)}_j\<br>temp^{(4)}_j&=\sum_{i=301}^{400}(h_\theta(x^{(i)}-y^{(i)})x^{(i)}_j\<br>\end{align*}
$$
最后，通过 Reduce（规约）操作进行求和：</p><p>$$\theta_j=\theta_j-\alpha \frac{1}{400}(temp_j^{(1)}+temp_j^{(2)}+temp_j^{(3)}+temp_j^{(4)})$$</p><p>我们可以使用多台机器进行 MapReduce，此时，Map 任务被分配到多个机器完成：</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/81_9.png alt></p><p>也可以使用单机多核心进行 MapReduce，此时，Map 任务被分配到多个 CPU 核心完成：</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/81_10.png alt></p><hr><h2 id=三-large-scale-machine-learning-测试>三. Large Scale Machine Learning 测试</h2><h3 id=1-question-1>1. Question 1</h3><p>Suppose you are training a logistic regression classifier using stochastic gradient descent. You find that the cost (say, $cost(\theta,(x^{(i)},y^{(i)})$), averaged over the last 500 examples), plotted as a function of the number of iterations, is slowly increasing over time. Which of the following changes are likely to help?</p><p>A. Use fewer examples from your training set.</p><p>B. Try averaging the cost over a smaller number of examples (say 250 examples instead of 500) in the plot.</p><p>C. This is not possible with stochastic gradient descent, as it is guaranteed to converge to the optimal parameters $\theta$.</p><p>D. Try halving (decreasing) the learning rate $\alpha$, and see if that causes the cost to now consistently go down; and if not, keep halving it until it does.</p><p>解答：D</p><h3 id=2-question-2>2. Question 2</h3><p>Which of the following statements about stochastic gradient descent are true? Check all that apply.</p><p>A. Stochastic gradient descent is particularly well suited to problems with small training set sizes; in these problems, stochastic gradient descent is often preferred to batch gradient descent.</p><p>B. In each iteration of stochastic gradient descent, the algorithm needs to examine/use only one training example.</p><p>C. Suppose you are using stochastic gradient descent to train a linear regression classifier. The cost function $J(\theta)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$ is guaranteed to decrease after every iteration of the stochastic gradient descent algorithm.</p><p>D. One of the advantages of stochastic gradient descent is that it can start progress in improving the parameters $\theta$ after looking at just a single training example; in contrast, batch gradient descent needs to take a pass over the entire training set before it starts to make progress in improving the parameters&rsquo; values.</p><p>E. n order to make sure stochastic gradient descent is converging, we typically compute $J_{train}(\theta)$ after each iteration (and plot it) in order to make sure that the cost function is generally decreasing.</p><p>F. If you have a huge training set, then stochastic gradient descent may be much faster than batch gradient descent.</p><p>G. In order to make sure stochastic gradient descent is converging, we typically compute $J_{train}(\theta)$ after each iteration (and plot it) in order to make sure that the cost function is generally decreasing.</p><p>H. Before running stochastic gradient descent, you should randomly shuffle (reorder) the training set.</p><p>解答： F、H</p><p>C 错误<br>G 并不需要代价函数总是减少，可能会降低故错误</p><h3 id=3-question-3>3. Question 3</h3><p>Which of the following statements about online learning are true? Check all that apply.</p><p>A. Online learning algorithms are most appropriate when we have a fixed training set of size m that we want to train on.</p><p>B. Online learning algorithms are usually best suited to problems were we have a continuous/non-stop stream of data that we want to learn from.</p><p>C. When using online learning, you must save every new training example you get, as you will need to reuse past examples to re-train the model even after you get new training examples in the future.</p><p>D. One of the advantages of online learning is that if the function we&rsquo;re modeling changes over time (such as if we are modeling the probability of users clicking on different URLs, and user tastes/preferences are changing over time), the online learning algorithm will automatically adapt to these changes.</p><p>解答： B、D</p><h3 id=4-question-4>4. Question 4</h3><p>Assuming that you have a very large training set, which of the following algorithms do you think can be parallelized using map-reduce and splitting the training set across different machines? Check all that apply.</p><p>A. Logistic regression trained using batch gradient descent.</p><p>B. Linear regression trained using stochastic gradient descent.</p><p>C. Logistic regression trained using stochastic gradient descent.</p><p>D. Computing the average of all the features in your training set $\mu=\frac{1}{m}\sum^m_{i=1}x^{(i)}$ (say in order to perform mean normalization).</p><p>解答： A、D</p><p>可以用映射约减算法的有用批量梯度下降的逻辑回归，凡是要计算大量值的算法，用随机梯度下降不用计算大量的值故选 A、D</p><p>B. Linear regression trained using batch gradient descent. 所以 B 错误</p><p>C. 错误</p><h3 id=5-question-5>5. Question 5</h3><p>Which of the following statements about map-reduce are true? Check all that apply.</p><p>A. When using map-reduce with gradient descent, we usually use a single machine that accumulates the gradients from each of the map-reduce machines, in order to compute the parameter update for that iteration.</p><p>B. Linear regression and logistic regression can be parallelized using map-reduce, but not neural network training.</p><p>C. Because of network latency and other overhead associated with map-reduce, if we run map-reduce using N computers, we might get less than an N-fold speedup compared to using 1 computer.</p><p>D. If you have only 1 computer with 1 computing core, then map-reduce is unlikely to help.</p><p>E. If you have just 1 computer, but your computer has multiple CPUs or multiple cores, then map-reduce might be a viable way to parallelize your learning algorithm.</p><p>F. In order to parallelize a learning algorithm using map-reduce, the first step is to figure out how to express the main work done by the algorithm as computing sums of functions of training examples.</p><p>G. Running map-reduce over N computers requires that we split the training set into $N^2$ pieces.</p><p>解答：A、C、D</p><p>用N台电脑比用一台，要快不到 N 倍。用一台计算机统计其他计算出来的数据。神经网络也要计算代价函数也要有大量计算，所以神经网络也可以并行计算。
B 错误<br>C 正确<br>E 错误<br>F 错误<br>G 可能正确？</p><hr><blockquote><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a></p><p>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a></p><p>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Large_Scale_Machine_Learning.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Large_Scale_Machine_Learning.ipynb</a></p></blockquote><img src=https://img.halfrost.com/wechat-qr-code.png></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#一-gradient-descent-with-large-datasets>一. Gradient Descent with Large Datasets</a></li><li><a href=#二-advanced-topics>二. Advanced Topics</a><ul><li><a href=#1-批量梯度下降法batch-gradient-descent>1. 批量梯度下降法（Batch gradient descent）</a></li><li><a href=#2-随机梯度下降法stochastic-gradient-descent>2. 随机梯度下降法（Stochastic gradient descent）</a></li><li><a href=#3-mini-批量梯度下降法mini-batch-gradient-descent>3. Mini 批量梯度下降法（Mini-batch gradient descent）</a></li><li><a href=#4-在线学习>4. 在线学习</a></li><li><a href=#5-mapreduce>5. MapReduce</a></li></ul></li><li><a href=#三-large-scale-machine-learning-测试>三. Large Scale Machine Learning 测试</a><ul><li><a href=#1-question-1>1. Question 1</a></li><li><a href=#2-question-2>2. Question 2</a></li><li><a href=#3-question-3>3. Question 3</a></li><li><a href=#4-question-4>4. Question 4</a></li><li><a href=#5-question-5>5. Question 5</a></li></ul></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f"><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&text=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&title=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&is_video=false&description=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f"><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&title=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&title=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&title=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-stumbleupon fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&title=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-digg fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&name=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fLarge_Scale_Machine_Learning.ipynb%0a%20%e4%b8%80.%20Gradient%20Descent%20with%20Large%20Datasets%20%e5%a6%82%e6%9e%9c%e6%88%91%e4%bb%ac%e6%9c%89%e4%b8%80%e4%b8%aa%e4%bd%8e%e6%96%b9%e5%b7%ae%e7%9a%84%e6%a8%a1%e5%9e%8b%ef%bc%8c%e5%a2%9e%e5%8a%a0%e6%95%b0%e6%8d%ae%e9%9b%86%e7%9a%84%e8%a7%84%e6%a8%a1%e5%8f%af%e4%bb%a5%e5%b8%ae%e5%8a%a9%e4%bd%a0%e8%8e%b7%e5%be%97%e6%9b%b4%e5%a5%bd%e7%9a%84%e7%bb%93%e6%9e%9c%e3%80%82%e6%88%91%e4%bb%ac%e5%ba%94%e8%af%a5%e6%80%8e%e6%a0%b7%e5%ba%94%e5%af%b9%e4%b8%80%e4%b8%aa%e6%9c%89100%e4%b8%87%e6%9d%a1%e8%ae%b0%e5%bd%95%e7%9a%84%e8%ae%ad%e7%bb%83%e9%9b%86%ef%bc%9f%0a%e4%bb%a5%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e6%a8%a1%e5%9e%8b%e4%b8%ba%e4%be%8b%ef%bc%8c%e6%af%8f%e4%b8%80%e6%ac%a1%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e8%bf%ad%e4%bb%a3%ef%bc%8c%e6%88%91%e4%bb%ac%e9%83%bd%e9%9c%80%e8%a6%81%e8%ae%a1%e7%ae%97%e8%ae%ad%e7%bb%83%e9%9b%86%e7%9a%84%e8%af%af%e5%b7%ae%e7%9a%84%e5%b9%b3%e6%96%b9%e5%92%8c%ef%bc%8c%e5%a6%82%e6%9e%9c%e6%88%91%e4%bb%ac%e7%9a%84%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e9%9c%80%e8%a6%81%e6%9c%8920%e6%ac%a1%e8%bf%ad%e4%bb%a3%ef%bc%8c%e8%bf%99%e4%be%bf%e5%b7%b2%e7%bb%8f%e6%98%af%e9%9d%9e%e5%b8%b8%e5%a4%a7%e7%9a%84%e8%ae%a1%e7%ae%97%e4%bb%a3%e4%bb%b7%e3%80%82%0a%e9%a6%96%e5%85%88%e5%ba%94%e8%af%a5%e5%81%9a%e7%9a%84%e4%ba%8b%e6%98%af%e5%8e%bb%e6%a3%80%e6%9f%a5%e4%b8%80%e4%b8%aa%e8%bf%99%e4%b9%88%e5%a4%a7%e8%a7%84%e6%a8%a1%e7%9a%84%e8%ae%ad%e7%bb%83%e9%9b%86%e6%98%af%e5%90%a6%e7%9c%9f%e7%9a%84%e5%bf%85%e8%a6%81%ef%bc%8c%e4%b9%9f%e8%ae%b8%e6%88%91%e4%bb%ac%e5%8f%aa%e7%94%a81000%e4%b8%aa%e8%ae%ad%e7%bb%83%e9%9b%86%e4%b9%9f%e8%83%bd%e8%8e%b7%e5%be%97%e8%be%83%e5%a5%bd%e7%9a%84%e6%95%88%e6%9e%9c%ef%bc%8c%e6%88%91%e4%bb%ac%e5%8f%af%e4%bb%a5%e7%bb%98%e5%88%b6%e5%ad%a6%e4%b9%a0%e6%9b%b2%e7%ba%bf%e6%9d%a5%e5%b8%ae%e5%8a%a9%e5%88%a4%e6%96%ad%e3%80%82%0a%20%e4%ba%8c.%20Advanced%20Topics%201.%20%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%88Batch%20gradient%20descent%ef%bc%89%20%e6%8b%a5%e6%9c%89%e4%ba%86%e5%a4%a7%e6%95%b0%e6%8d%ae%ef%bc%8c%e5%b0%b1%e6%84%8f%e5%91%b3%e7%9d%80%ef%bc%8c%e6%88%91%e4%bb%ac%e7%9a%84%e7%ae%97%e6%b3%95%e6%a8%a1%e5%9e%8b%e4%b8%ad%e5%be%97%e9%9d%a2%e4%b8%b4%e4%b8%80%e4%b8%aa%e5%be%88%e5%a4%a7%e7%9a%84%20m%20%e5%80%bc%e3%80%82%e5%9b%9e%e9%a1%be%e5%88%b0%e6%88%91%e4%bb%ac%e7%9a%84%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%9a%0a%e9%87%8d%e5%a4%8d%e7%9b%b4%e5%88%b0%e6%94%b6%e6%95%9b%ef%bc%9a%0a%24%24%5ctheta_j%3d%5ctheta_j-%5calpha%20%5cfrac%7b1%7d%7bm%7d%20%5csum_%7bi%3d1%7d%5e%7bm%7d%28h_%7b%5ctheta%7d%28x%5e%7b%28i%29%7d%29-y%5e%7b%28i%29%7d%29x%5e%7b%28i%29%7d_j%2c%5c%3b%5c%3b%5c%3b%5c%3bfor%5c%3b%5c%3bj%3d0%2c%5ccdots%2cn%24%24%0a%e5%8f%af%e4%bb%a5%e7%9c%8b%e5%88%b0%ef%bc%8c%e6%af%8f%e6%9b%b4%e6%96%b0%e4%b8%80%e4%b8%aa%e5%8f%82%e6%95%b0%20%24%5ctheta_j%24%20%ef%bc%8c%e6%88%91%e4%bb%ac%e9%83%bd%e4%b8%8d%e5%be%97%e4%b8%8d%e9%81%8d%e5%8e%86%e4%b8%80%e9%81%8d%e6%a0%b7%e6%9c%ac%e9%9b%86%ef%bc%8c%e5%9c%a8%20m%20%e5%be%88%e5%a4%a7%e6%97%b6%ef%bc%8c%e8%af%a5%e7%ae%97%e6%b3%95%e5%b0%b1%e6%98%be%e5%be%97%e6%af%94%e8%be%83%e4%bd%8e%e6%95%88%e3%80%82%e4%bd%86%e6%98%af%ef%bc%8c%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e8%83%bd%e6%89%be%e5%88%b0%e5%85%a8%e5%b1%80%e6%9c%80%e4%bc%98%e8%a7%a3%ef%bc%9a%0a2.%20%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%88Stochastic%20gradient%20descent%ef%bc%89%20%e9%92%88%e5%af%b9%e5%a4%a7%e6%95%b0%e6%8d%ae%e9%9b%86%ef%bc%8c%e5%8f%88%e5%bc%95%e5%85%a5%e4%ba%86%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%8c%e8%af%a5%e7%ae%97%e6%b3%95%e7%9a%84%e6%89%a7%e8%a1%8c%e8%bf%87%e7%a8%8b%e4%b8%ba%ef%bc%9a%0a%e9%87%8d%e5%a4%8d%e7%9b%b4%e5%88%b0%e6%94%b6%e6%95%9b%ef%bc%9a%0a%24%24%20%5cbegin%7balign%2a%7d%20for%5c%3b%5c%3b%5c%3bi%26amp%3b%3d1%2c%5ccdots%2cm%3a%5c%0a%5ctheta_j%26amp%3b%3d%5ctheta_j-%5calpha%28h_%5ctheta%28x%5e%7b%28i%29%7d%29-y%5e%7b%28i%29%7d%29x%5e%7b%28i%29%7d_j%2c%5c%3b%5c%3b%5c%3b%5c%3bfor%5c%3b%5c%3bj%3d0%2c%5ccdots%2cn%5c%0a%5cend%7balign%2a%7d%20%24%24%0a%e7%9b%b8%e8%be%83%e4%ba%8e%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%8c%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e6%af%8f%e6%ac%a1%e6%9b%b4%e6%96%b0%20%24%5ctheta_j%24%20%e5%8f%aa%e4%bc%9a%e7%94%a8%e5%bd%93%e5%89%8d%e9%81%8d%e5%8e%86%e7%9a%84%e6%a0%b7%e6%9c%ac%e3%80%82%e8%99%bd%e7%84%b6%e5%a4%96%e5%b1%82%e5%be%aa%e7%8e%af%e4%bb%8d%e9%9c%80%e8%a6%81%e9%81%8d%e5%8e%86%e6%89%80%e6%9c%89%e6%a0%b7%e6%9c%ac%ef%bc%8c%e4%bd%86%e6%98%af%ef%bc%8c%e5%be%80%e5%be%80%e6%88%91%e4%bb%ac%e8%83%bd%e5%9c%a8%e6%a0%b7%e6%9c%ac%e5%b0%9a%e6%9c%aa%e9%81%8d%e5%8e%86%e5%ae%8c%e6%97%b6%e5%b0%b1%e5%b7%b2%e7%bb%8f%e6%94%b6%e6%95%9b%ef%bc%8c%e5%9b%a0%e6%ad%a4%ef%bc%8c%e9%9d%a2%e4%b8%b4%e5%a4%a7%e6%95%b0%e6%8d%ae%e9%9b%86%e6%97%b6%ef%bc%8c%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e6%80%a7%e8%83%bd%e5%8d%93%e8%b6%8a%e3%80%82%0a%e4%b8%8a%e5%9b%be%e5%8f%8d%e6%98%a0%e4%ba%86%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e6%89%be%e5%af%bb%e6%9c%80%e4%bc%98%e8%a7%a3%e7%9a%84%e8%bf%87%e7%a8%8b%ef%bc%8c%e7%9b%b8%e8%be%83%e4%ba%8e%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%8c%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e7%9a%84%e6%9b%b2%e7%ba%bf%e5%b0%b1%e6%98%be%e5%be%97%e4%b8%8d%e6%98%af%e9%82%a3%e4%b9%88%e5%b9%b3%e6%bb%91%ef%bc%8c%e8%80%8c%e6%98%af%e5%be%88%e6%9b%b2%e6%8a%98%e4%ba%86%ef%bc%8c%e5%85%b6%e4%b9%9f%e5%80%be%e5%90%91%e4%ba%8e%e6%89%be%e5%88%b0%e5%b1%80%e9%83%a8%e6%9c%80%e4%bc%98%e8%a7%a3%e8%80%8c%e4%b8%8d%e6%98%af%e5%85%a8%e5%b1%80%e6%9c%80%e4%bc%98%e8%a7%a3%e3%80%82%e5%9b%a0%e6%ad%a4%ef%bc%8c%e6%88%91%e4%bb%ac%e9%80%9a%e5%b8%b8%e9%9c%80%e8%a6%81%e7%bb%98%e5%88%b6%e8%b0%83%e8%af%95%e6%9b%b2%e7%ba%bf%e6%9d%a5%e7%9b%91%e6%8e%a7%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e7%9a%84%e5%b7%a5%e4%bd%9c%e8%bf%87%e7%a8%8b%e6%98%af%e5%90%a6%e6%ad%a3%e7%a1%ae%e3%80%82%e4%be%8b%e5%a6%82%ef%bc%8c%e5%81%87%e5%ae%9a%e8%af%af%e5%b7%ae%e5%ae%9a%e4%b9%89%e4%b8%ba%20%24cost%28%5ctheta%2c%28x%5e%7b%28i%29%7d%2cy%5e%7b%28i%29%7d%29%29%3d%5cfrac%7b1%7d%7b2%7d%28h_%5ctheta%28x%5e%7b%28i%29%7d%29-y%5e%7b%28i%29%7d%29%5e2%24%ef%bc%8c%e5%88%99%e6%af%8f%e5%ae%8c%e6%88%90%201000%20%e6%ac%a1%e8%bf%ad%e4%bb%a3%ef%bc%8c%e5%8d%b3%e9%81%8d%e5%8e%86%e4%ba%86%201000%20%e4%b8%aa%e6%a0%b7%e6%9c%ac%ef%bc%8c%e6%88%91%e4%bb%ac%e6%b1%82%e5%8f%96%e5%b9%b3%e5%9d%87%e8%af%af%e5%b7%ae%e5%b9%b6%e8%bf%9b%e8%a1%8c%e7%bb%98%e5%88%b6%ef%bc%8c%e5%be%97%e5%88%b0%e8%af%af%e5%b7%ae%e9%9a%8f%e8%bf%ad%e4%bb%a3%e6%ac%a1%e6%95%b0%e7%9a%84%e5%8f%98%e5%8c%96%e6%9b%b2%e7%ba%bf%ef%bc%9a"><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2flarge_scale_machine_learning%2f&t=%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%a6%82%e4%bd%95%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%9f"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu class=icon href=# onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden=true></i>Menu</a>
<a id=toc class=icon href=# onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden=true></i>TOC</a>
<a id=share class=icon href=# onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden=true></i>share</a>
<a id=top style=display:none class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i>Top</a></div></div></div><footer id=footer><div class=footer-left><p class=copyright style=float:left;margin-bottom:0><a href=https://github.com/halfrost/Halfrost-Field class=github-repo style=height:18px><span class=gadget-github></span>Star</a>
Copyright &copy;halfrost 2016 - 2021
<a href=http://www.miit.gov.cn/>鄂ICP备16014744号</a></p><br><p class="copyright statistics" style=margin-bottom:20px><span id=busuanzi_container_site_pv>Cumulative Page Views <span id=busuanzi_value_site_pv></span>| Unique Visitors <span id=busuanzi_value_site_uv></span></span></p></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script><script src=/main.min.f870a4d110314b9e50e65f8ac982dc1c9c376c8f1a5083d39c62cfc49073f011.js></script><script async src=/prism.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}};</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>