<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=theme-color content="#FFFFFF"><meta http-equiv=x-ua-compatible content="IE=edge"><title>逻辑回归 | prometheus</title><meta name=description content="Explore in every moment of the hard thinking"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="逻辑回归"><meta property="og:description" content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Logistic_Regression.ipynb
 一. Classification and Representation 要尝试分类，一种方法是使用线性回归，并将所有大于0.5的预测值映射为1，将小于0.5的所有预测值映射为0.但是，此方法效果不佳，因为分类实际上不是线性函数。 分类问题就像回归问题一样，除了我们现在想要预测的值只有少数离散值。
线性回归用来解决分类问题，通常不是一个好主意。
我们解决分类问题，忽略y是离散值，并使用我们的旧线性回归算法来尝试预测给定的x。但是，构建这种方法性能很差的示例很容易。直观地说，当知道$y\in \begin{Bmatrix} 0,1 \end{Bmatrix}$时，$h_{\theta}(x)$ 取大于1或小于0的值也是没有意义的。为了解决这个问题，让我们改变我们的假设 $h_{\theta}(x)$ 的形式以满足 $0\leqslant h_{\theta}(x)\leqslant 1$。这是通过将 $\theta^{T}x$ 插入 Logistic 函数来完成的：
$$g(x) = \frac{1}{1+e^{-x}}$$
上式称为 Sigmoid Function 或者 Logistic Function
令 $h_{\theta}(x) = g(\theta^{T}x)$,$z = \theta^{T}x$,则:
$$g(x) = \frac{1}{1+e^{-\theta^{T}x}}$$
这里显示的函数$g(x)$将任何实数映射到（0,1）区间，使得它可用于将任意值函数转换为更适合分类的函数。
决策边界不是训练集的属性，而是假设本身及其参数的属性。
 二. Logistic Regression Model 1. Cost Function 之前定义的代价函数："><meta property="og:type" content="article"><meta property="og:url" content="https://new.halfrost.com/logistic_regression/"><meta property="article:published_time" content="2018-03-22T08:00:00+00:00"><meta property="article:modified_time" content="2018-03-22T08:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="逻辑回归"><meta name=twitter:description content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Logistic_Regression.ipynb
 一. Classification and Representation 要尝试分类，一种方法是使用线性回归，并将所有大于0.5的预测值映射为1，将小于0.5的所有预测值映射为0.但是，此方法效果不佳，因为分类实际上不是线性函数。 分类问题就像回归问题一样，除了我们现在想要预测的值只有少数离散值。
线性回归用来解决分类问题，通常不是一个好主意。
我们解决分类问题，忽略y是离散值，并使用我们的旧线性回归算法来尝试预测给定的x。但是，构建这种方法性能很差的示例很容易。直观地说，当知道$y\in \begin{Bmatrix} 0,1 \end{Bmatrix}$时，$h_{\theta}(x)$ 取大于1或小于0的值也是没有意义的。为了解决这个问题，让我们改变我们的假设 $h_{\theta}(x)$ 的形式以满足 $0\leqslant h_{\theta}(x)\leqslant 1$。这是通过将 $\theta^{T}x$ 插入 Logistic 函数来完成的：
$$g(x) = \frac{1}{1+e^{-x}}$$
上式称为 Sigmoid Function 或者 Logistic Function
令 $h_{\theta}(x) = g(\theta^{T}x)$,$z = \theta^{T}x$,则:
$$g(x) = \frac{1}{1+e^{-\theta^{T}x}}$$
这里显示的函数$g(x)$将任何实数映射到（0,1）区间，使得它可用于将任意值函数转换为更适合分类的函数。
决策边界不是训练集的属性，而是假设本身及其参数的属性。
 二. Logistic Regression Model 1. Cost Function 之前定义的代价函数："><link rel=stylesheet href=/css/style-white.min.css><link rel=manifest href=/manifest.json><link rel=stylesheet href=/prism.css><link href=/images/apple-touch-icon-60x60.png rel=apple-touch-icon sizes=60x60><link href=/images/apple-touch-icon-76x76.png rel=apple-touch-icon sizes=76x76><link href=/images/apple-touch-icon-120x120.png rel=apple-touch-icon sizes=120x120><link href=/images/apple-touch-icon-152x152.png rel=apple-touch-icon sizes=152x152><link href=/images/apple-touch-icon-180x180.png rel=apple-touch-icon sizes=180x180><link href=/images/apple-touch-icon-512x512.png rel=apple-touch-icon sizes=512x512><link href=/images/apple-touch-icon-1024x1024.png rel=apple-touch-icon sizes=1024x1024><script async>if('serviceWorker'in navigator){navigator.serviceWorker.register("\/serviceworker-v1.min.a64912b78d282eab1ad3715a0943da21616e5f326f8afea27034784ad445043b.js").then(function(){if(navigator.serviceWorker.controller){console.log('Assets cached by the controlling service worker.');}else{console.log('Please reload this page to allow the service worker to handle network operations.');}}).catch(function(error){console.log('ERROR: '+error);});}else{console.log('Service workers are not supported in the current browser.');}</script><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://new.halfrost.com/images/favicon.ico><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-82753806-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class="single-max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a><a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a><a id=top-icon-tablet href=# onclick="$('html, body').animate({scrollTop:0},'fast');" style=display:none><i class="fas fa-chevron-up fa-lg"></i></a><span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://new.halfrost.com/octave_matlab_tutorial/><i class="fas fa-chevron-left" aria-hidden=true onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li><li><a class=icon href=https://new.halfrost.com/regularization/><i class="fas fa-chevron-right" aria-hidden=true onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li><li><a class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up" aria-hidden=true onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li><li><a class=icon href=#><i class="fas fa-share-alt" aria-hidden=true onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f"><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&text=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&title=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&is_video=false&description=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f"><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&title=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&title=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&title=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-stumbleupon" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&title=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-digg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&name=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fLogistic_Regression.ipynb%0a%20%e4%b8%80.%20Classification%20and%20Representation%20%e8%a6%81%e5%b0%9d%e8%af%95%e5%88%86%e7%b1%bb%ef%bc%8c%e4%b8%80%e7%a7%8d%e6%96%b9%e6%b3%95%e6%98%af%e4%bd%bf%e7%94%a8%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%ef%bc%8c%e5%b9%b6%e5%b0%86%e6%89%80%e6%9c%89%e5%a4%a7%e4%ba%8e0.5%e7%9a%84%e9%a2%84%e6%b5%8b%e5%80%bc%e6%98%a0%e5%b0%84%e4%b8%ba1%ef%bc%8c%e5%b0%86%e5%b0%8f%e4%ba%8e0.5%e7%9a%84%e6%89%80%e6%9c%89%e9%a2%84%e6%b5%8b%e5%80%bc%e6%98%a0%e5%b0%84%e4%b8%ba0.%e4%bd%86%e6%98%af%ef%bc%8c%e6%ad%a4%e6%96%b9%e6%b3%95%e6%95%88%e6%9e%9c%e4%b8%8d%e4%bd%b3%ef%bc%8c%e5%9b%a0%e4%b8%ba%e5%88%86%e7%b1%bb%e5%ae%9e%e9%99%85%e4%b8%8a%e4%b8%8d%e6%98%af%e7%ba%bf%e6%80%a7%e5%87%bd%e6%95%b0%e3%80%82%20%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98%e5%b0%b1%e5%83%8f%e5%9b%9e%e5%bd%92%e9%97%ae%e9%a2%98%e4%b8%80%e6%a0%b7%ef%bc%8c%e9%99%a4%e4%ba%86%e6%88%91%e4%bb%ac%e7%8e%b0%e5%9c%a8%e6%83%b3%e8%a6%81%e9%a2%84%e6%b5%8b%e7%9a%84%e5%80%bc%e5%8f%aa%e6%9c%89%e5%b0%91%e6%95%b0%e7%a6%bb%e6%95%a3%e5%80%bc%e3%80%82%0a%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e7%94%a8%e6%9d%a5%e8%a7%a3%e5%86%b3%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98%ef%bc%8c%e9%80%9a%e5%b8%b8%e4%b8%8d%e6%98%af%e4%b8%80%e4%b8%aa%e5%a5%bd%e4%b8%bb%e6%84%8f%e3%80%82%0a%e6%88%91%e4%bb%ac%e8%a7%a3%e5%86%b3%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98%ef%bc%8c%e5%bf%bd%e7%95%a5y%e6%98%af%e7%a6%bb%e6%95%a3%e5%80%bc%ef%bc%8c%e5%b9%b6%e4%bd%bf%e7%94%a8%e6%88%91%e4%bb%ac%e7%9a%84%e6%97%a7%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e7%ae%97%e6%b3%95%e6%9d%a5%e5%b0%9d%e8%af%95%e9%a2%84%e6%b5%8b%e7%bb%99%e5%ae%9a%e7%9a%84x%e3%80%82%e4%bd%86%e6%98%af%ef%bc%8c%e6%9e%84%e5%bb%ba%e8%bf%99%e7%a7%8d%e6%96%b9%e6%b3%95%e6%80%a7%e8%83%bd%e5%be%88%e5%b7%ae%e7%9a%84%e7%a4%ba%e4%be%8b%e5%be%88%e5%ae%b9%e6%98%93%e3%80%82%e7%9b%b4%e8%a7%82%e5%9c%b0%e8%af%b4%ef%bc%8c%e5%bd%93%e7%9f%a5%e9%81%93%24y%5cin%20%5cbegin%7bBmatrix%7d%200%2c1%20%5cend%7bBmatrix%7d%24%e6%97%b6%ef%bc%8c%24h_%7b%5ctheta%7d%28x%29%24%20%e5%8f%96%e5%a4%a7%e4%ba%8e1%e6%88%96%e5%b0%8f%e4%ba%8e0%e7%9a%84%e5%80%bc%e4%b9%9f%e6%98%af%e6%b2%a1%e6%9c%89%e6%84%8f%e4%b9%89%e7%9a%84%e3%80%82%e4%b8%ba%e4%ba%86%e8%a7%a3%e5%86%b3%e8%bf%99%e4%b8%aa%e9%97%ae%e9%a2%98%ef%bc%8c%e8%ae%a9%e6%88%91%e4%bb%ac%e6%94%b9%e5%8f%98%e6%88%91%e4%bb%ac%e7%9a%84%e5%81%87%e8%ae%be%20%24h_%7b%5ctheta%7d%28x%29%24%20%e7%9a%84%e5%bd%a2%e5%bc%8f%e4%bb%a5%e6%bb%a1%e8%b6%b3%20%240%5cleqslant%20h_%7b%5ctheta%7d%28x%29%5cleqslant%201%24%e3%80%82%e8%bf%99%e6%98%af%e9%80%9a%e8%bf%87%e5%b0%86%20%24%5ctheta%5e%7bT%7dx%24%20%e6%8f%92%e5%85%a5%20Logistic%20%e5%87%bd%e6%95%b0%e6%9d%a5%e5%ae%8c%e6%88%90%e7%9a%84%ef%bc%9a%0a%24%24g%28x%29%20%3d%20%5cfrac%7b1%7d%7b1%2be%5e%7b-x%7d%7d%24%24%0a%e4%b8%8a%e5%bc%8f%e7%a7%b0%e4%b8%ba%20Sigmoid%20Function%20%e6%88%96%e8%80%85%20Logistic%20Function%0a%e4%bb%a4%20%24h_%7b%5ctheta%7d%28x%29%20%3d%20g%28%5ctheta%5e%7bT%7dx%29%24%2c%24z%20%3d%20%5ctheta%5e%7bT%7dx%24%2c%e5%88%99%3a%0a%24%24g%28x%29%20%3d%20%5cfrac%7b1%7d%7b1%2be%5e%7b-%5ctheta%5e%7bT%7dx%7d%7d%24%24%0a%e8%bf%99%e9%87%8c%e6%98%be%e7%a4%ba%e7%9a%84%e5%87%bd%e6%95%b0%24g%28x%29%24%e5%b0%86%e4%bb%bb%e4%bd%95%e5%ae%9e%e6%95%b0%e6%98%a0%e5%b0%84%e5%88%b0%ef%bc%880%2c1%ef%bc%89%e5%8c%ba%e9%97%b4%ef%bc%8c%e4%bd%bf%e5%be%97%e5%ae%83%e5%8f%af%e7%94%a8%e4%ba%8e%e5%b0%86%e4%bb%bb%e6%84%8f%e5%80%bc%e5%87%bd%e6%95%b0%e8%bd%ac%e6%8d%a2%e4%b8%ba%e6%9b%b4%e9%80%82%e5%90%88%e5%88%86%e7%b1%bb%e7%9a%84%e5%87%bd%e6%95%b0%e3%80%82%0a%e5%86%b3%e7%ad%96%e8%be%b9%e7%95%8c%e4%b8%8d%e6%98%af%e8%ae%ad%e7%bb%83%e9%9b%86%e7%9a%84%e5%b1%9e%e6%80%a7%ef%bc%8c%e8%80%8c%e6%98%af%e5%81%87%e8%ae%be%e6%9c%ac%e8%ba%ab%e5%8f%8a%e5%85%b6%e5%8f%82%e6%95%b0%e7%9a%84%e5%b1%9e%e6%80%a7%e3%80%82%0a%20%e4%ba%8c.%20Logistic%20Regression%20Model%201.%20Cost%20Function%20%e4%b9%8b%e5%89%8d%e5%ae%9a%e4%b9%89%e7%9a%84%e4%bb%a3%e4%bb%b7%e5%87%bd%e6%95%b0%ef%bc%9a"><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&t=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><nav id=TableOfContents><ul><li><a href=#一-classification-and-representation>一. Classification and Representation</a></li><li><a href=#二-logistic-regression-model>二. Logistic Regression Model</a><ul><li><a href=#1-cost-function>1. Cost Function</a></li><li><a href=#2-simplified-cost-function-and-gradient-descent>2. Simplified Cost Function and Gradient Descent</a></li><li><a href=#3-求导过程>3. 求导过程</a></li><li><a href=#4-advanced-optimization>4. Advanced Optimization</a></li></ul></li><li><a href=#三-multiclass-classification>三. Multiclass Classification</a></li><li><a href=#四-logistic-regression-测试>四. Logistic Regression 测试</a><ul><li><a href=#1-question-1>1. Question 1</a></li><li><a href=#2-question-2>2. Question 2</a></li><li><a href=#3-question-3>3. Question 3</a></li><li><a href=#4-question-4>4. Question 4</a></li><li><a href=#5-question-5>5. Question 5</a></li></ul></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">逻辑回归</h1><div class=meta><div class=postdate><time datetime="2018-03-22 08:00:00 +0000 UTC" itemprop=datePublished>Mar 22</time></div><div class=article-category><i class="fas fa-archive"></i><a class=category-link href=/categories/machine-learning>Machine Learning</a>
,
<a class=category-link href=/categories/ai>AI</a></div><div class=article-tag><i class="fas fa-tag"></i><a class=tag-link href=/tags/machine-learning rel=tag>Machine Learning</a>
,
<a class=tag-link href=/tags/ai rel=tag>AI</a></div></div></header><div class=content itemprop=articleBody><blockquote><p>由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/contents.md>Github</a> 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。</p><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a><br>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a><br>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Logistic_Regression.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Logistic_Regression.ipynb</a></p></blockquote><h2 id=一-classification-and-representation>一. Classification and Representation</h2><p>要尝试分类，一种方法是使用线性回归，并将所有大于0.5的预测值映射为1，将小于0.5的所有预测值映射为0.但是，此方法效果不佳，因为分类实际上不是线性函数。 分类问题就像回归问题一样，除了我们现在想要预测的值只有少数离散值。</p><p><strong>线性回归用来解决分类问题，通常不是一个好主意</strong>。</p><p>我们解决分类问题，忽略y是离散值，并使用我们的旧线性回归算法来尝试预测给定的x。但是，构建这种方法性能很差的示例很容易。直观地说，当知道$y\in \begin{Bmatrix}
0,1
\end{Bmatrix}$时，$h_{\theta}(x)$ 取大于1或小于0的值也是没有意义的。为了解决这个问题，让我们改变我们的假设 $h_{\theta}(x)$ 的形式以满足 $0\leqslant h_{\theta}(x)\leqslant 1$。这是通过将 $\theta^{T}x$ 插入 Logistic 函数来完成的：</p><p>$$g(x) = \frac{1}{1+e^{-x}}$$</p><p>上式称为 Sigmoid Function 或者 Logistic Function</p><p>令 $h_{\theta}(x) = g(\theta^{T}x)$,$z = \theta^{T}x$,则:</p><p>$$g(x) = \frac{1}{1+e^{-\theta^{T}x}}$$</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/69_8.png alt></p><p>这里显示的函数$g(x)$将任何实数映射到（0,1）区间，使得它可用于将任意值函数转换为更适合分类的函数。</p><p><strong>决策边界不是训练集的属性，而是假设本身及其参数的属性</strong>。</p><hr><h2 id=二-logistic-regression-model>二. Logistic Regression Model</h2><h3 id=1-cost-function>1. Cost Function</h3><p>之前定义的代价函数：</p><p>$$ \rm{CostFunction} = \rm{F}({\theta}) = \frac{1}{m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2 $$</p><p>如果将 $$h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}} $$ 代入到上面的式子中，$\rm{CostFunction}$ 的函数图像会是一个非凸函数，会有很多个局部极值点。</p><p>于是我们重新寻找一个新的代价函数：</p><p>$$\rm{CostFunction} = \rm{F}({\theta}) = \frac{1}{m}\sum_{i = 1}^{m} \rm{Cost}(h_{\theta}(x^{(i)}),y^{(i)})$$</p><p>$$\rm{Cost}(h_{\theta}(x^{(i)}),y^{(i)}) = \left{\begin{matrix}
-log(h_{\theta}(x)) &if ; y = 1 \
-log(1-h_{\theta}(x)) & if; y = 0
\end{matrix}\right.$$</p><p>需要说明的一点是，在我们的训练集中，甚至不在训练集中的样本，y 的值总是等于 0 或者 1 。</p><h3 id=2-simplified-cost-function-and-gradient-descent>2. Simplified Cost Function and Gradient Descent</h3><p>于是进一步我们把代价函数写成一个式子：</p><p>$$\rm{Cost}(h_{\theta}(x),y) = - ylog(h_{\theta}(x)) - (1-y)log(1-h_{\theta}(x))$$</p><p>所以代价函数最终表示为：</p><p>$$
\begin{align*}
\rm{CostFunction} = \rm{F}({\theta}) &= \frac{1}{m}\sum_{i = 1}^{m} \rm{Cost}(h_{\theta}(x^{(i)}),y^{(i)})\<br>&= -\frac{1}{m}\left [ \sum_{i=1}^{m} y^{(i)}logh_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)})) \right ] \<br>\left( h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}} \right )
\end{align*}
$$</p><p>向量化形式：</p><p>$$
\begin{align*}
h &= g(X\theta)\
\rm{CostFunction} = \rm{F}({\theta}) &= \frac{1}{m} \left ( -\overrightarrow{y}^{T}log(h) - (1-\overrightarrow{y})^{T}log(1-h) \right ) \
\end{align*}
$$</p><p>为了把式子写成上面这样子是来自于统计学的极大似然估计法得来的，它是统计学里为不同的模型快速寻找参数的方法。它的性质之一是它是凸函数。</p><p>利用梯度下降的方法，得到代价函数的最小值：</p><p>$$ \theta_{j} := \theta_{j} - \alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_{j}$$</p><p>矢量化，即：</p><p>$$ \theta := \theta - \alpha \frac{1}{m} X^{T}(g(X\Theta)-\vec{y})$$</p><p><strong>这里需要注意的是</strong>，</p><p>**线性回归中，$h_{\theta}(x) = \theta^{T}x $**,</p><p>**而 Logistic 回归中，$h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}}$** 。</p><p>最后，特征缩放的方法同样适用于 Logistic 回归，让其梯度下降收敛更快。</p><hr><h3 id=3-求导过程>3. 求导过程</h3><p>逻辑函数</p><p>我们先来看看如何对逻辑函数（Sigmoid函数）求导：</p><p>$$
\begin{align*}
\sigma(x)'&=\left(\frac{1}{1+e^{-x}}\right)&lsquo;=\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\frac{-1&rsquo;-(e^{-x})'}{(1+e^{-x})^2}=\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\frac{e^{-x}}{(1+e^{-x})^2} \newline &=\left(\frac{1}{1+e^{-x}}\right)\left(\frac{e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{+1-1 + e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{1 + e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}}\right)\<br>&=\sigma(x)(1 - \sigma(x))\<br>\end{align*}
$$</p><p>代价函数</p><p>利用上面的结果，借助复合函数求导公式等，可得：</p><p>$$
\begin{align*}
\frac{\partial}{\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j} \frac{-1}{m}\sum_{i=1}^m \left [ y^{(i)} log (h_\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\theta(x^{(i)})) \right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [ y^{(i)} \frac{\partial}{\partial \theta_j} log (h_\theta(x^{(i)})) + (1-y^{(i)}) \frac{\partial}{\partial \theta_j} log (1 - h_\theta(x^{(i)}))\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [ \frac{y^{(i)} \frac{\partial}{\partial \theta_j} h_\theta(x^{(i)})}{h_\theta(x^{(i)})} + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - h_\theta(x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [ \frac{y^{(i)} \frac{\partial}{\partial \theta_j} \sigma(\theta^T x^{(i)})}{h_\theta(x^{(i)})} + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - \sigma(\theta^T x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [ \frac{y^{(i)} \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})} + \frac{- (1-y^{(i)}) \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [ \frac{y^{(i)} h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})} - \frac{(1-y^{(i)}) h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [ y^{(i)} (1 - h_\theta(x^{(i)})) x^{(i)}_j - (1-y^{(i)}) h_\theta(x^{(i)}) x^{(i)}_j\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [ y^{(i)} (1 - h_\theta(x^{(i)})) - (1-y^{(i)}) h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&= - \frac{1}{m}\sum_{i=1}^m \left [ y^{(i)} - y^{(i)} h_\theta(x^{(i)}) - h_\theta(x^{(i)}) + y^{(i)} h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&= - \frac{1}{m}\sum_{i=1}^m \left [ y^{(i)} - h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&= \frac{1}{m}\sum_{i=1}^m \left [ h_\theta(x^{(i)}) - y^{(i)} \right ] x^{(i)}_j
\end{align*}
$$</p><p>向量化形式：</p><p>$$\nabla J(\theta) = \frac{1}{m} \cdot X^T \cdot \left(g\left(X\cdot\theta\right) - \vec{y}\right)$$</p><hr><h3 id=4-advanced-optimization>4. Advanced Optimization</h3><p>除去梯度下降法，还有其他的优化方法，</p><p>conjugate gradient 共轭梯度法，<br>BFGS，<br>L_BFGS，</p><p>上述3种算法在高等数值计算中。它们相比梯度下降，有以下一些优点：</p><ol><li>不需要手动选择学习率 $\alpha$ 。可以理解为它们有一个智能的内循环(线搜索算法)，它会自动尝试不同的学习速率 $\alpha$，并自动选择一个最好的学习速率 $\alpha$ 。甚至还可以为每次迭代选择不同的学习速率，那么就不需要自己选择了。</li><li>收敛速度远远快于梯度下降。</li></ol><p>缺点就是相比梯度下降而言，更加复杂。</p><p>举个例子：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>
function [jVal, gradient] <span style=color:#f92672>=</span> costFunction(theta)

jVal <span style=color:#f92672>=</span> (theta(<span style=color:#ae81ff>1</span>)<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>)<span style=color:#f92672>^</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> (theta(<span style=color:#ae81ff>2</span>)<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>)<span style=color:#f92672>^</span><span style=color:#ae81ff>2</span>;

gradient <span style=color:#f92672>=</span> zeros(<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>1</span>);
gradient(<span style=color:#ae81ff>1</span>) <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>(theta(<span style=color:#ae81ff>1</span>)<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>);
gradient(<span style=color:#ae81ff>2</span>) <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>(theta(<span style=color:#ae81ff>2</span>)<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>);

</code></pre></div><p>调用高级函数 fminunc:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>
options <span style=color:#f92672>=</span> optimset(<span style=color:#960050;background-color:#1e0010>&#39;</span>GrabObj<span style=color:#e6db74>&#39;,&#39;</span>on<span style=color:#e6db74>&#39;,&#39;</span>MaxIter<span style=color:#e6db74>&#39;,&#39;</span><span style=color:#ae81ff>100</span><span style=color:#960050;background-color:#1e0010>&#39;</span>);
initialTheta <span style=color:#f92672>=</span> zeros(<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>1</span>);
[optTheta, functionVal, exitFlag] <span style=color:#f92672>=</span> fminunc(<span style=color:#960050;background-color:#1e0010>@</span>costFunction, initialTheta, options);


</code></pre></div><p>最终结果:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>
optTheta <span style=color:#f92672>=</span> 
    
    <span style=color:#ae81ff>5.0000</span>
    <span style=color:#ae81ff>5.0000</span>
    
functionVal <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.5777e-030</span>
exitFlag <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>

</code></pre></div><p>optTheta 表示的是最终求得的结果，functionVal 表示的是代价函数的最小值，这里是 0，是我们期望的。exitFlag 表示的是最终是否收敛，1表示收敛。</p><p>这里的 fminunc 是试图找到一个多变量函数的最小值，从一个估计的初试值开始，这通常被认为是无约束非线性优化问题。</p><p>另外一些例子：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>
x <span style=color:#f92672>=</span>fminunc(fun,x0)                                   <span style=color:#f92672>%</span><span style=color:#960050;background-color:#1e0010>试图从</span>x0附近开始找到函数的局部最小值<span style=color:#960050;background-color:#1e0010>，</span>x0可以是标量<span style=color:#960050;background-color:#1e0010>，向量或矩阵</span>
x <span style=color:#f92672>=</span>fminunc(fun,x0,options)                           <span style=color:#f92672>%</span><span style=color:#960050;background-color:#1e0010>根据结构体</span>options中的设置来找到最小值<span style=color:#960050;background-color:#1e0010>，可用</span>optimset来设置options
x <span style=color:#f92672>=</span>fminunc(problem)                                  <span style=color:#f92672>%</span><span style=color:#960050;background-color:#1e0010>为</span>problem找到最小值,<span style=color:#960050;background-color:#1e0010>而</span>problem是在Input Arguments中定义的结构体

[x,fval]<span style=color:#f92672>=</span> fminunc(...)                               <span style=color:#f92672>%</span><span style=color:#960050;background-color:#1e0010>返回目标函数</span>fun在解x处的函数值
[x,fval,exitflag]<span style=color:#f92672>=</span> fminunc(...)                      <span style=color:#f92672>%</span><span style=color:#960050;background-color:#1e0010>返回一个描述退出条件的值</span>exitflag
[x,fval,exitflag,output]<span style=color:#f92672>=</span> fminunc(...)               <span style=color:#f92672>%</span><span style=color:#960050;background-color:#1e0010>返回一个叫</span>output的结构体<span style=color:#960050;background-color:#1e0010>，它包含着优化的信息</span>
[x,fval,exitflag,output,grad]<span style=color:#f92672>=</span> fminunc(...)          <span style=color:#f92672>%</span><span style=color:#960050;background-color:#1e0010>返回函数在解</span>x处的梯度的值<span style=color:#960050;background-color:#1e0010>，存储在</span>grad中
[x,fval,exitflag,output,grad,hessian]<span style=color:#f92672>=</span> fminunc(...)  <span style=color:#f92672>%</span><span style=color:#960050;background-color:#1e0010>返回函数在解</span>x处的Hessian矩阵的值<span style=color:#960050;background-color:#1e0010>，存储在</span>hessian中


</code></pre></div><hr><h2 id=三-multiclass-classification>三. Multiclass Classification</h2><p>这一章节我们来讨论一下如何利用逻辑回归来解决多类别分类问题。介绍一个一对多的分类算法。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/69_7.png alt></p><p>现在，当我们有两个以上的类别时，我们将处理数据的分类。我们将扩展我们的定义，使得y = {0,1 &mldr; n}，而不是y = {0,1}。 由于y = {0,1 &mldr; n}，我们将问题分成n + 1（+1，因为索引从0开始）二元分类问题;在每一个中，我们都预测&rsquo;y&rsquo;是我们其中一个类的成员的概率。</p><p>最终在 n + 1 个分类器中分别输入 x ，然后取这 n + 1 个分类器概率的最大值,即是对应 $y=i$ 的概率值。</p><hr><h2 id=四-logistic-regression-测试>四. Logistic Regression 测试</h2><h3 id=1-question-1>1. Question 1</h3><p>Suppose that you have trained a logistic regression classifier, and it outputs on a new example x a prediction hθ(x) = 0.7. This means (check all that apply):</p><p>A. Our estimate for P(y=1|x;θ) is 0.7.<br>B. Our estimate for P(y=0|x;θ) is 0.3.<br>C. Our estimate for P(y=1|x;θ) is 0.3.<br>D. Our estimate for P(y=0|x;θ) is 0.7.</p><p>解答： A、B</p><h3 id=2-question-2>2. Question 2</h3><p>Suppose you have the following training set, and fit a logistic regression classifier hθ(x)=g(θ0+θ1x1+θ2x2).</p><p>Which of the following are true? Check all that apply.</p><p>A. Adding polynomial features (e.g., instead using hθ(x)=g(θ0+θ1x1+θ2x2+θ3x21+θ4x1x2+θ5x22) ) could increase how well we can fit the training data.</p><p>B. At the optimal value of θ (e.g., found by fminunc), we will have J(θ)≥0.</p><p>C. Adding polynomial features (e.g., instead using hθ(x)=g(θ0+θ1x1+θ2x2+θ3x21+θ4x1x2+θ5x22) ) would increase J(θ) because we are now summing over more terms.</p><p>D. If we train gradient descent for enough iterations, for some examples x(i) in the training set it is possible to obtain hθ(x(i))>1.</p><p>解答： A、B</p><h3 id=3-question-3>3. Question 3</h3><p>For logistic regression, the gradient is given by ∂∂θjJ(θ)=1m∑mi=1(hθ(x(i))−y(i))x(i)j. Which of these is a correct gradient descent update for logistic regression with a learning rate of α? Check all that apply.</p><p>A. θj:=θj−α1m∑mi=1(hθ(x(i))−y(i))x(i) (simultaneously update for all j).</p><p>B. θj:=θj−α1m∑mi=1(hθ(x(i))−y(i))x(i)j (simultaneously update for all j).</p><p>C. θj:=θj−α1m∑mi=1(11+e−θTx(i)−y(i))x(i)j (simultaneously update for all j).</p><p>D. θ:=θ−α1m∑mi=1(θTx−y(i))x(i).</p><p>解答： A、D</p><p>线性回归与逻辑回归的区别</p><h3 id=4-question-4>4. Question 4</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. The cost function J(θ) for logistic regression trained with m≥1 examples is always greater than or equal to zero.</p><p>B. Linear regression always works well for classification if you classify by using a threshold on the prediction made by linear regression.</p><p>C. The one-vs-all technique allows you to use logistic regression for problems in which each y(i) comes from a fixed, discrete set of values.</p><p>D. For logistic regression, sometimes gradient descent will converge to a local minimum (and fail to find the global minimum). This is the reason we prefer more advanced optimization algorithms such as fminunc (conjugate gradient/BFGS/L-BFGS/etc).</p><p>解答： A、C</p><p>D由于使用代价函数为线性回归代价函数，会有很多局部最优值</p><h3 id=5-question-5>5. Question 5</h3><p>Suppose you train a logistic classifier hθ(x)=g(θ0+θ1x1+θ2x2). Suppose θ0=6,θ1=0,θ2=−1. Which of the following figures represents the decision boundary found by your classifier?</p><p>解答： C</p><p>6-x2>=0 即X2&lt;6时为1</p><hr><blockquote><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a></p><p>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a></p><p>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Logistic_Regression.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Logistic_Regression.ipynb</a></p></blockquote><img src=https://img.halfrost.com/wechat-qr-code.png></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#一-classification-and-representation>一. Classification and Representation</a></li><li><a href=#二-logistic-regression-model>二. Logistic Regression Model</a><ul><li><a href=#1-cost-function>1. Cost Function</a></li><li><a href=#2-simplified-cost-function-and-gradient-descent>2. Simplified Cost Function and Gradient Descent</a></li><li><a href=#3-求导过程>3. 求导过程</a></li><li><a href=#4-advanced-optimization>4. Advanced Optimization</a></li></ul></li><li><a href=#三-multiclass-classification>三. Multiclass Classification</a></li><li><a href=#四-logistic-regression-测试>四. Logistic Regression 测试</a><ul><li><a href=#1-question-1>1. Question 1</a></li><li><a href=#2-question-2>2. Question 2</a></li><li><a href=#3-question-3>3. Question 3</a></li><li><a href=#4-question-4>4. Question 4</a></li><li><a href=#5-question-5>5. Question 5</a></li></ul></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f"><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&text=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&title=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&is_video=false&description=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f"><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&title=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&title=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&title=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-stumbleupon fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&title=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-digg fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&name=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fLogistic_Regression.ipynb%0a%20%e4%b8%80.%20Classification%20and%20Representation%20%e8%a6%81%e5%b0%9d%e8%af%95%e5%88%86%e7%b1%bb%ef%bc%8c%e4%b8%80%e7%a7%8d%e6%96%b9%e6%b3%95%e6%98%af%e4%bd%bf%e7%94%a8%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%ef%bc%8c%e5%b9%b6%e5%b0%86%e6%89%80%e6%9c%89%e5%a4%a7%e4%ba%8e0.5%e7%9a%84%e9%a2%84%e6%b5%8b%e5%80%bc%e6%98%a0%e5%b0%84%e4%b8%ba1%ef%bc%8c%e5%b0%86%e5%b0%8f%e4%ba%8e0.5%e7%9a%84%e6%89%80%e6%9c%89%e9%a2%84%e6%b5%8b%e5%80%bc%e6%98%a0%e5%b0%84%e4%b8%ba0.%e4%bd%86%e6%98%af%ef%bc%8c%e6%ad%a4%e6%96%b9%e6%b3%95%e6%95%88%e6%9e%9c%e4%b8%8d%e4%bd%b3%ef%bc%8c%e5%9b%a0%e4%b8%ba%e5%88%86%e7%b1%bb%e5%ae%9e%e9%99%85%e4%b8%8a%e4%b8%8d%e6%98%af%e7%ba%bf%e6%80%a7%e5%87%bd%e6%95%b0%e3%80%82%20%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98%e5%b0%b1%e5%83%8f%e5%9b%9e%e5%bd%92%e9%97%ae%e9%a2%98%e4%b8%80%e6%a0%b7%ef%bc%8c%e9%99%a4%e4%ba%86%e6%88%91%e4%bb%ac%e7%8e%b0%e5%9c%a8%e6%83%b3%e8%a6%81%e9%a2%84%e6%b5%8b%e7%9a%84%e5%80%bc%e5%8f%aa%e6%9c%89%e5%b0%91%e6%95%b0%e7%a6%bb%e6%95%a3%e5%80%bc%e3%80%82%0a%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e7%94%a8%e6%9d%a5%e8%a7%a3%e5%86%b3%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98%ef%bc%8c%e9%80%9a%e5%b8%b8%e4%b8%8d%e6%98%af%e4%b8%80%e4%b8%aa%e5%a5%bd%e4%b8%bb%e6%84%8f%e3%80%82%0a%e6%88%91%e4%bb%ac%e8%a7%a3%e5%86%b3%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98%ef%bc%8c%e5%bf%bd%e7%95%a5y%e6%98%af%e7%a6%bb%e6%95%a3%e5%80%bc%ef%bc%8c%e5%b9%b6%e4%bd%bf%e7%94%a8%e6%88%91%e4%bb%ac%e7%9a%84%e6%97%a7%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e7%ae%97%e6%b3%95%e6%9d%a5%e5%b0%9d%e8%af%95%e9%a2%84%e6%b5%8b%e7%bb%99%e5%ae%9a%e7%9a%84x%e3%80%82%e4%bd%86%e6%98%af%ef%bc%8c%e6%9e%84%e5%bb%ba%e8%bf%99%e7%a7%8d%e6%96%b9%e6%b3%95%e6%80%a7%e8%83%bd%e5%be%88%e5%b7%ae%e7%9a%84%e7%a4%ba%e4%be%8b%e5%be%88%e5%ae%b9%e6%98%93%e3%80%82%e7%9b%b4%e8%a7%82%e5%9c%b0%e8%af%b4%ef%bc%8c%e5%bd%93%e7%9f%a5%e9%81%93%24y%5cin%20%5cbegin%7bBmatrix%7d%200%2c1%20%5cend%7bBmatrix%7d%24%e6%97%b6%ef%bc%8c%24h_%7b%5ctheta%7d%28x%29%24%20%e5%8f%96%e5%a4%a7%e4%ba%8e1%e6%88%96%e5%b0%8f%e4%ba%8e0%e7%9a%84%e5%80%bc%e4%b9%9f%e6%98%af%e6%b2%a1%e6%9c%89%e6%84%8f%e4%b9%89%e7%9a%84%e3%80%82%e4%b8%ba%e4%ba%86%e8%a7%a3%e5%86%b3%e8%bf%99%e4%b8%aa%e9%97%ae%e9%a2%98%ef%bc%8c%e8%ae%a9%e6%88%91%e4%bb%ac%e6%94%b9%e5%8f%98%e6%88%91%e4%bb%ac%e7%9a%84%e5%81%87%e8%ae%be%20%24h_%7b%5ctheta%7d%28x%29%24%20%e7%9a%84%e5%bd%a2%e5%bc%8f%e4%bb%a5%e6%bb%a1%e8%b6%b3%20%240%5cleqslant%20h_%7b%5ctheta%7d%28x%29%5cleqslant%201%24%e3%80%82%e8%bf%99%e6%98%af%e9%80%9a%e8%bf%87%e5%b0%86%20%24%5ctheta%5e%7bT%7dx%24%20%e6%8f%92%e5%85%a5%20Logistic%20%e5%87%bd%e6%95%b0%e6%9d%a5%e5%ae%8c%e6%88%90%e7%9a%84%ef%bc%9a%0a%24%24g%28x%29%20%3d%20%5cfrac%7b1%7d%7b1%2be%5e%7b-x%7d%7d%24%24%0a%e4%b8%8a%e5%bc%8f%e7%a7%b0%e4%b8%ba%20Sigmoid%20Function%20%e6%88%96%e8%80%85%20Logistic%20Function%0a%e4%bb%a4%20%24h_%7b%5ctheta%7d%28x%29%20%3d%20g%28%5ctheta%5e%7bT%7dx%29%24%2c%24z%20%3d%20%5ctheta%5e%7bT%7dx%24%2c%e5%88%99%3a%0a%24%24g%28x%29%20%3d%20%5cfrac%7b1%7d%7b1%2be%5e%7b-%5ctheta%5e%7bT%7dx%7d%7d%24%24%0a%e8%bf%99%e9%87%8c%e6%98%be%e7%a4%ba%e7%9a%84%e5%87%bd%e6%95%b0%24g%28x%29%24%e5%b0%86%e4%bb%bb%e4%bd%95%e5%ae%9e%e6%95%b0%e6%98%a0%e5%b0%84%e5%88%b0%ef%bc%880%2c1%ef%bc%89%e5%8c%ba%e9%97%b4%ef%bc%8c%e4%bd%bf%e5%be%97%e5%ae%83%e5%8f%af%e7%94%a8%e4%ba%8e%e5%b0%86%e4%bb%bb%e6%84%8f%e5%80%bc%e5%87%bd%e6%95%b0%e8%bd%ac%e6%8d%a2%e4%b8%ba%e6%9b%b4%e9%80%82%e5%90%88%e5%88%86%e7%b1%bb%e7%9a%84%e5%87%bd%e6%95%b0%e3%80%82%0a%e5%86%b3%e7%ad%96%e8%be%b9%e7%95%8c%e4%b8%8d%e6%98%af%e8%ae%ad%e7%bb%83%e9%9b%86%e7%9a%84%e5%b1%9e%e6%80%a7%ef%bc%8c%e8%80%8c%e6%98%af%e5%81%87%e8%ae%be%e6%9c%ac%e8%ba%ab%e5%8f%8a%e5%85%b6%e5%8f%82%e6%95%b0%e7%9a%84%e5%b1%9e%e6%80%a7%e3%80%82%0a%20%e4%ba%8c.%20Logistic%20Regression%20Model%201.%20Cost%20Function%20%e4%b9%8b%e5%89%8d%e5%ae%9a%e4%b9%89%e7%9a%84%e4%bb%a3%e4%bb%b7%e5%87%bd%e6%95%b0%ef%bc%9a"><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2flogistic_regression%2f&t=%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu class=icon href=# onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden=true></i>Menu</a>
<a id=toc class=icon href=# onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden=true></i>TOC</a>
<a id=share class=icon href=# onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden=true></i>share</a>
<a id=top style=display:none class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i>Top</a></div></div></div><footer id=footer><div class=footer-left><p class=copyright style=float:left;margin-bottom:0><a href=https://github.com/halfrost/Halfrost-Field class=github-repo style=height:18px><span class=gadget-github></span>Star</a>
Copyright &copy;halfrost 2016 - 2021
<a href=http://www.miit.gov.cn/>鄂ICP备16014744号</a></p><br><p class="copyright statistics" style=margin-bottom:20px><span id=busuanzi_container_site_pv>Cumulative Page Views <span id=busuanzi_value_site_pv></span>| Unique Visitors <span id=busuanzi_value_site_uv></span></span></p></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script><script src=/main.min.f870a4d110314b9e50e65f8ac982dc1c9c376c8f1a5083d39c62cfc49073f011.js></script><script async src=/prism.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}};</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>