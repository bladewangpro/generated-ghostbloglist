<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=theme-color content="#FFFFFF"><meta http-equiv=x-ua-compatible content="IE=edge"><title>机器学习算法评估 | prometheus</title><meta name=description content="Explore in every moment of the hard thinking"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="机器学习算法评估"><meta property="og:description" content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Advice_for_Applying_Machine_Learning.ipynb
 一. Evaluating a Learning Algorithm 想要降低预测误差，即提高预测精度，我们往往会采用这些手段：
  采集更多的样本
错误的认为样本越多越好，其实数据多并不是越好。
  降低特征维度
降维可能去掉了有用的特征。
  采集更多的特征
增加了计算负担，也可能导致过拟合。
  进行高次多项式回归
过高的多项式可能造成过拟合。
  调试正规化参数 $\lambda$,增大或者减少 $\lambda$
增大或者减少都是凭感觉。
  有这么多种解决办法我们怎么知道是哪一种呢？很多人选择这些方法的标准就是凭感觉随便选择一种，然后花很长的时间最后发现是没用的，走上了不归路。所以下面我们介绍一我们需要一种简单有效的办法，我们将其称为机器学习算法诊断（Machine learning diagnostic）。
1. Evaluating a Hypothesis 评价假设函数 首先我们要评估的是我们的假设函数（Hypothesis）。当我们选择特征值或者参数来使训练集误差最小化，但是我们会遇到过拟合的问题，推广到新的训练集就不再使用了。而且当特征量很多的时候，我们就不能将 $J(\theta)$ 可视化看出其是否随着迭代次数而下降了。所以我们采用以下的方法来评估我们的假设函数：
假设有 10 组数据，随机把 70% 做为训练集，剩下的 30% 做为测试集。训练集和测试集尽量保证是随机排列。
接下来："><meta property="og:type" content="article"><meta property="og:url" content="https://new.halfrost.com/advice_for_applying_machine_learning/"><meta property="article:published_time" content="2018-03-27T17:28:00+00:00"><meta property="article:modified_time" content="2018-03-27T17:28:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="机器学习算法评估"><meta name=twitter:description content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Advice_for_Applying_Machine_Learning.ipynb
 一. Evaluating a Learning Algorithm 想要降低预测误差，即提高预测精度，我们往往会采用这些手段：
  采集更多的样本
错误的认为样本越多越好，其实数据多并不是越好。
  降低特征维度
降维可能去掉了有用的特征。
  采集更多的特征
增加了计算负担，也可能导致过拟合。
  进行高次多项式回归
过高的多项式可能造成过拟合。
  调试正规化参数 $\lambda$,增大或者减少 $\lambda$
增大或者减少都是凭感觉。
  有这么多种解决办法我们怎么知道是哪一种呢？很多人选择这些方法的标准就是凭感觉随便选择一种，然后花很长的时间最后发现是没用的，走上了不归路。所以下面我们介绍一我们需要一种简单有效的办法，我们将其称为机器学习算法诊断（Machine learning diagnostic）。
1. Evaluating a Hypothesis 评价假设函数 首先我们要评估的是我们的假设函数（Hypothesis）。当我们选择特征值或者参数来使训练集误差最小化，但是我们会遇到过拟合的问题，推广到新的训练集就不再使用了。而且当特征量很多的时候，我们就不能将 $J(\theta)$ 可视化看出其是否随着迭代次数而下降了。所以我们采用以下的方法来评估我们的假设函数：
假设有 10 组数据，随机把 70% 做为训练集，剩下的 30% 做为测试集。训练集和测试集尽量保证是随机排列。
接下来："><link rel=stylesheet href=/css/style-white.min.css><link rel=manifest href=/manifest.json><link rel=stylesheet href=/prism.css><link href=/images/apple-touch-icon-60x60.png rel=apple-touch-icon sizes=60x60><link href=/images/apple-touch-icon-76x76.png rel=apple-touch-icon sizes=76x76><link href=/images/apple-touch-icon-120x120.png rel=apple-touch-icon sizes=120x120><link href=/images/apple-touch-icon-152x152.png rel=apple-touch-icon sizes=152x152><link href=/images/apple-touch-icon-180x180.png rel=apple-touch-icon sizes=180x180><link href=/images/apple-touch-icon-512x512.png rel=apple-touch-icon sizes=512x512><link href=/images/apple-touch-icon-1024x1024.png rel=apple-touch-icon sizes=1024x1024><script async>if('serviceWorker'in navigator){navigator.serviceWorker.register("\/serviceworker-v1.min.a64912b78d282eab1ad3715a0943da21616e5f326f8afea27034784ad445043b.js").then(function(){if(navigator.serviceWorker.controller){console.log('Assets cached by the controlling service worker.');}else{console.log('Please reload this page to allow the service worker to handle network operations.');}}).catch(function(error){console.log('ERROR: '+error);});}else{console.log('Service workers are not supported in the current browser.');}</script><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://new.halfrost.com/images/favicon.ico><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-82753806-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class="single-max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a><a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a><a id=top-icon-tablet href=# onclick="$('html, body').animate({scrollTop:0},'fast');" style=display:none><i class="fas fa-chevron-up fa-lg"></i></a><span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://new.halfrost.com/backpropagation_in_practice/><i class="fas fa-chevron-left" aria-hidden=true onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li><li><a class=icon href=https://new.halfrost.com/machine_learning_system_design/><i class="fas fa-chevron-right" aria-hidden=true onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li><li><a class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up" aria-hidden=true onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li><li><a class=icon href=#><i class="fas fa-share-alt" aria-hidden=true onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f"><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&text=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&is_video=false&description=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f"><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-stumbleupon" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-digg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&name=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fAdvice_for_Applying_Machine_Learning.ipynb%0a%20%e4%b8%80.%20Evaluating%20a%20Learning%20Algorithm%20%e6%83%b3%e8%a6%81%e9%99%8d%e4%bd%8e%e9%a2%84%e6%b5%8b%e8%af%af%e5%b7%ae%ef%bc%8c%e5%8d%b3%e6%8f%90%e9%ab%98%e9%a2%84%e6%b5%8b%e7%b2%be%e5%ba%a6%ef%bc%8c%e6%88%91%e4%bb%ac%e5%be%80%e5%be%80%e4%bc%9a%e9%87%87%e7%94%a8%e8%bf%99%e4%ba%9b%e6%89%8b%e6%ae%b5%ef%bc%9a%0a%20%20%e9%87%87%e9%9b%86%e6%9b%b4%e5%a4%9a%e7%9a%84%e6%a0%b7%e6%9c%ac%0a%e9%94%99%e8%af%af%e7%9a%84%e8%ae%a4%e4%b8%ba%e6%a0%b7%e6%9c%ac%e8%b6%8a%e5%a4%9a%e8%b6%8a%e5%a5%bd%ef%bc%8c%e5%85%b6%e5%ae%9e%e6%95%b0%e6%8d%ae%e5%a4%9a%e5%b9%b6%e4%b8%8d%e6%98%af%e8%b6%8a%e5%a5%bd%e3%80%82%0a%20%20%e9%99%8d%e4%bd%8e%e7%89%b9%e5%be%81%e7%bb%b4%e5%ba%a6%0a%e9%99%8d%e7%bb%b4%e5%8f%af%e8%83%bd%e5%8e%bb%e6%8e%89%e4%ba%86%e6%9c%89%e7%94%a8%e7%9a%84%e7%89%b9%e5%be%81%e3%80%82%0a%20%20%e9%87%87%e9%9b%86%e6%9b%b4%e5%a4%9a%e7%9a%84%e7%89%b9%e5%be%81%0a%e5%a2%9e%e5%8a%a0%e4%ba%86%e8%ae%a1%e7%ae%97%e8%b4%9f%e6%8b%85%ef%bc%8c%e4%b9%9f%e5%8f%af%e8%83%bd%e5%af%bc%e8%87%b4%e8%bf%87%e6%8b%9f%e5%90%88%e3%80%82%0a%20%20%e8%bf%9b%e8%a1%8c%e9%ab%98%e6%ac%a1%e5%a4%9a%e9%a1%b9%e5%bc%8f%e5%9b%9e%e5%bd%92%0a%e8%bf%87%e9%ab%98%e7%9a%84%e5%a4%9a%e9%a1%b9%e5%bc%8f%e5%8f%af%e8%83%bd%e9%80%a0%e6%88%90%e8%bf%87%e6%8b%9f%e5%90%88%e3%80%82%0a%20%20%e8%b0%83%e8%af%95%e6%ad%a3%e8%a7%84%e5%8c%96%e5%8f%82%e6%95%b0%20%24%5clambda%24%2c%e5%a2%9e%e5%a4%a7%e6%88%96%e8%80%85%e5%87%8f%e5%b0%91%20%24%5clambda%24%0a%e5%a2%9e%e5%a4%a7%e6%88%96%e8%80%85%e5%87%8f%e5%b0%91%e9%83%bd%e6%98%af%e5%87%ad%e6%84%9f%e8%a7%89%e3%80%82%0a%20%20%e6%9c%89%e8%bf%99%e4%b9%88%e5%a4%9a%e7%a7%8d%e8%a7%a3%e5%86%b3%e5%8a%9e%e6%b3%95%e6%88%91%e4%bb%ac%e6%80%8e%e4%b9%88%e7%9f%a5%e9%81%93%e6%98%af%e5%93%aa%e4%b8%80%e7%a7%8d%e5%91%a2%ef%bc%9f%e5%be%88%e5%a4%9a%e4%ba%ba%e9%80%89%e6%8b%a9%e8%bf%99%e4%ba%9b%e6%96%b9%e6%b3%95%e7%9a%84%e6%a0%87%e5%87%86%e5%b0%b1%e6%98%af%e5%87%ad%e6%84%9f%e8%a7%89%e9%9a%8f%e4%be%bf%e9%80%89%e6%8b%a9%e4%b8%80%e7%a7%8d%ef%bc%8c%e7%84%b6%e5%90%8e%e8%8a%b1%e5%be%88%e9%95%bf%e7%9a%84%e6%97%b6%e9%97%b4%e6%9c%80%e5%90%8e%e5%8f%91%e7%8e%b0%e6%98%af%e6%b2%a1%e7%94%a8%e7%9a%84%ef%bc%8c%e8%b5%b0%e4%b8%8a%e4%ba%86%e4%b8%8d%e5%bd%92%e8%b7%af%e3%80%82%e6%89%80%e4%bb%a5%e4%b8%8b%e9%9d%a2%e6%88%91%e4%bb%ac%e4%bb%8b%e7%bb%8d%e4%b8%80%e6%88%91%e4%bb%ac%e9%9c%80%e8%a6%81%e4%b8%80%e7%a7%8d%e7%ae%80%e5%8d%95%e6%9c%89%e6%95%88%e7%9a%84%e5%8a%9e%e6%b3%95%ef%bc%8c%e6%88%91%e4%bb%ac%e5%b0%86%e5%85%b6%e7%a7%b0%e4%b8%ba%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%8a%e6%96%ad%ef%bc%88Machine%20learning%20diagnostic%ef%bc%89%e3%80%82%0a1.%20Evaluating%20a%20Hypothesis%20%e8%af%84%e4%bb%b7%e5%81%87%e8%ae%be%e5%87%bd%e6%95%b0%20%e9%a6%96%e5%85%88%e6%88%91%e4%bb%ac%e8%a6%81%e8%af%84%e4%bc%b0%e7%9a%84%e6%98%af%e6%88%91%e4%bb%ac%e7%9a%84%e5%81%87%e8%ae%be%e5%87%bd%e6%95%b0%ef%bc%88Hypothesis%ef%bc%89%e3%80%82%e5%bd%93%e6%88%91%e4%bb%ac%e9%80%89%e6%8b%a9%e7%89%b9%e5%be%81%e5%80%bc%e6%88%96%e8%80%85%e5%8f%82%e6%95%b0%e6%9d%a5%e4%bd%bf%e8%ae%ad%e7%bb%83%e9%9b%86%e8%af%af%e5%b7%ae%e6%9c%80%e5%b0%8f%e5%8c%96%ef%bc%8c%e4%bd%86%e6%98%af%e6%88%91%e4%bb%ac%e4%bc%9a%e9%81%87%e5%88%b0%e8%bf%87%e6%8b%9f%e5%90%88%e7%9a%84%e9%97%ae%e9%a2%98%ef%bc%8c%e6%8e%a8%e5%b9%bf%e5%88%b0%e6%96%b0%e7%9a%84%e8%ae%ad%e7%bb%83%e9%9b%86%e5%b0%b1%e4%b8%8d%e5%86%8d%e4%bd%bf%e7%94%a8%e4%ba%86%e3%80%82%e8%80%8c%e4%b8%94%e5%bd%93%e7%89%b9%e5%be%81%e9%87%8f%e5%be%88%e5%a4%9a%e7%9a%84%e6%97%b6%e5%80%99%ef%bc%8c%e6%88%91%e4%bb%ac%e5%b0%b1%e4%b8%8d%e8%83%bd%e5%b0%86%20%24J%28%5ctheta%29%24%20%e5%8f%af%e8%a7%86%e5%8c%96%e7%9c%8b%e5%87%ba%e5%85%b6%e6%98%af%e5%90%a6%e9%9a%8f%e7%9d%80%e8%bf%ad%e4%bb%a3%e6%ac%a1%e6%95%b0%e8%80%8c%e4%b8%8b%e9%99%8d%e4%ba%86%e3%80%82%e6%89%80%e4%bb%a5%e6%88%91%e4%bb%ac%e9%87%87%e7%94%a8%e4%bb%a5%e4%b8%8b%e7%9a%84%e6%96%b9%e6%b3%95%e6%9d%a5%e8%af%84%e4%bc%b0%e6%88%91%e4%bb%ac%e7%9a%84%e5%81%87%e8%ae%be%e5%87%bd%e6%95%b0%ef%bc%9a%0a%e5%81%87%e8%ae%be%e6%9c%89%2010%20%e7%bb%84%e6%95%b0%e6%8d%ae%ef%bc%8c%e9%9a%8f%e6%9c%ba%e6%8a%8a%2070%25%20%e5%81%9a%e4%b8%ba%e8%ae%ad%e7%bb%83%e9%9b%86%ef%bc%8c%e5%89%a9%e4%b8%8b%e7%9a%84%2030%25%20%e5%81%9a%e4%b8%ba%e6%b5%8b%e8%af%95%e9%9b%86%e3%80%82%e8%ae%ad%e7%bb%83%e9%9b%86%e5%92%8c%e6%b5%8b%e8%af%95%e9%9b%86%e5%b0%bd%e9%87%8f%e4%bf%9d%e8%af%81%e6%98%af%e9%9a%8f%e6%9c%ba%e6%8e%92%e5%88%97%e3%80%82%0a%e6%8e%a5%e4%b8%8b%e6%9d%a5%ef%bc%9a"><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&t=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><nav id=TableOfContents><ul><li><a href=#一-evaluating-a-learning-algorithm>一. Evaluating a Learning Algorithm</a><ul><li><a href=#1-evaluating-a-hypothesis-评价假设函数>1. Evaluating a Hypothesis 评价假设函数</a></li><li><a href=#2-model-selection-and-trainvalidationtest-sets-模型选择和训练集验证集测试集>2. Model Selection and Train/Validation/Test Sets 模型选择和训练集/验证集/测试集</a></li></ul></li><li><a href=#二-bias-vs-variance>二. Bias vs. Variance</a><ul><li><a href=#1-diagnosing-bias-vs-variance-诊断偏差和方差>1. Diagnosing Bias vs. Variance 诊断偏差和方差</a></li><li><a href=#2-regularization-and-biasvariance-正则化的偏差和方差>2. Regularization and Bias/Variance 正则化的偏差和方差</a></li><li><a href=#3-learning-curves-学习曲线>3. Learning Curves 学习曲线</a></li><li><a href=#4-deciding-what-to-do-next-revisited-决定下一步该做什么>4. Deciding What to Do Next Revisited 决定下一步该做什么</a></li></ul></li><li><a href=#三-advice-for-applying-machine-learning-测试>三. Advice for Applying Machine Learning 测试</a><ul><li><a href=#1-question-1>1. Question 1</a></li><li><a href=#2-question-2>2. Question 2</a></li><li><a href=#3-question-3>3. Question 3</a></li><li><a href=#4-question-4>4. Question 4</a></li><li><a href=#5-question-5>5. Question 5</a></li></ul></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">机器学习算法评估</h1><div class=meta><div class=postdate><time datetime="2018-03-27 17:28:00 +0000 UTC" itemprop=datePublished>Mar 27</time></div><div class=article-category><i class="fas fa-archive"></i><a class=category-link href=/categories/machine-learning>Machine Learning</a>
,
<a class=category-link href=/categories/ai>AI</a></div><div class=article-tag><i class="fas fa-tag"></i><a class=tag-link href=/tags/machine-learning rel=tag>Machine Learning</a>
,
<a class=tag-link href=/tags/ai rel=tag>AI</a></div></div></header><div class=content itemprop=articleBody><blockquote><p>由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/contents.md>Github</a> 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。</p><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a><br>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a><br>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Advice_for_Applying_Machine_Learning.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Advice_for_Applying_Machine_Learning.ipynb</a></p></blockquote><h2 id=一-evaluating-a-learning-algorithm>一. Evaluating a Learning Algorithm</h2><p>想要降低预测误差，即提高预测精度，我们往往会采用这些手段：</p><ul><li><p>采集更多的样本<br>错误的认为样本越多越好，其实数据多并不是越好。</p></li><li><p>降低特征维度<br>降维可能去掉了有用的特征。</p></li><li><p>采集更多的特征<br>增加了计算负担，也可能导致过拟合。</p></li><li><p>进行高次多项式回归<br>过高的多项式可能造成过拟合。</p></li><li><p>调试正规化参数 $\lambda$,增大或者减少 $\lambda$<br>增大或者减少都是凭感觉。</p></li></ul><p>有这么多种解决办法我们怎么知道是哪一种呢？很多人选择这些方法的标准就是凭感觉随便选择一种，然后花很长的时间最后发现是没用的，走上了不归路。所以下面我们介绍一我们需要一种简单有效的办法，我们将其称为机器学习算法诊断（Machine learning diagnostic）。</p><h3 id=1-evaluating-a-hypothesis-评价假设函数>1. Evaluating a Hypothesis 评价假设函数</h3><p>首先我们要评估的是我们的假设函数（Hypothesis）。当我们选择特征值或者参数来使训练集误差最小化，但是我们会遇到过拟合的问题，推广到新的训练集就不再使用了。而且当特征量很多的时候，我们就不能将 $J(\theta)$ 可视化看出其是否随着迭代次数而下降了。所以我们采用以下的方法来评估我们的假设函数：</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/74_1.png alt></p><p>假设有 10 组数据，随机把 70% 做为训练集，剩下的 30% 做为测试集。训练集和测试集尽量保证是随机排列。</p><p>接下来：</p><ol><li>对训练集进行学习得到参数 $\Theta$ ，也就是利用训练集最小化训练误差 $J_{train}(\Theta)$</li><li>计算出测试误差 $J_{test}(\Theta)$，取出之前从训练集中学习得到的参数 $\Theta$ 放在这里，来计算测试误差。</li></ol><p>对于线性回归： $$J_{test}(\theta)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}{(h_\theta(x^{(i)}_{test})-y^{(i)}_{test})^2}$$</p><p>对于逻辑回归： $$J_{test}(\theta)=-\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}{y^{(i)}_{test}logh_\theta(x^{(i)}_{test})+(1-y^{(i)}_{test})logh_\theta(x^{(i)}_{test})}$$</p><p>逻辑回归不同于线性回归，因为它只有0和1两个值， 所以怎么判断误差如下：</p><p>$$
err(h_\theta(x),y)=\left{\begin{matrix}
1 ;;;( if ;;; h_\theta(x) \geqslant 0.5 , y=0 ;;;or;;; if;;; h_\theta(x) &lt; 0.5 ， y=1 )\
0 ;;;( otherwise ) ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
\end{matrix}\right.
$$</p><p>这里的误差也叫误分类率，也叫 $0/1$ 错分率。</p><p>$( if ;;; h_\theta(x) \geqslant 0.5 , y=0 ;;;or;;; if;;; h_\theta(x) &lt; 0.5 ， y=1 )$</p><p>这种情况下，假设结果更趋向于1，但是实际给出的判断却是0，或者假设结果更趋向于0，实际给出的判断却是1 。</p><p>如果以上情况都没有，那么就没有误差，即为0 ，也代表了假设值能够正确的对样本进行分类。</p><p>测试集的平均测试误差为：</p><p>$$Test;Error=\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err(h_{\theta}(x^{(i)}_{test}),y^{(i)}_{test})$$</p><hr><h3 id=2-model-selection-and-trainvalidationtest-sets-模型选择和训练集验证集测试集>2. Model Selection and Train/Validation/Test Sets 模型选择和训练集/验证集/测试集</h3><p><img src=https://img.halfrost.com/Blog/ArticleImage/74_2.png alt></p><p>我们这里用 d 表示多项式的个数。我们可以改变多项式次数的多少来选择合适我们的模型。例如上面的 $h_\theta(x)=\theta_0+\theta_1x$ ，这个多项式 $d=1$ 。</p><p>我们可以测试每一个模型得到他们的 $J_{test}(\theta)$ ，判断哪一个模型比较好。</p><p>当选择出了一个多项式 d 能很完美的拟合测试集，接下来就不能再用测试集了，因为 d 本来就已经完美拟合测试集了，再测试就没有意义了，需要换一个测试集。所以更需要关心的对新样本的拟合效果。</p><p>为了解决上述问题，我们把数据分为 3 类，训练集 60% /交叉验证集 20% /测试集 20%。</p><p>通过三个集合，可以算出训练误差：</p><p>$$J_{train}(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^{2}$$</p><p>交叉验证误差：</p><p>$$J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_{i=1}^{m_{cv}}(h_\theta(x^{(i)}_{cv})-y^{(i)}_{cv})^{2}$$</p><p>测试误差：</p><p>$$J_{test}(\theta) = \frac{1}{2m_{test}}\sum_{i=1}^{m}(h_\theta(x^{(i)}_{test})-y^{(i)}_{test})^{2}$$</p><p>于是我们选择模型不在仅仅通过测试集来选择了，而是：</p><ol><li>利用训练集的数据代入每一个多项式模型。</li><li>用交叉验证集的数据找出最小误差的多项式模型。</li><li>最后在测试集再找出相对较少误差的那个模型。</li></ol><hr><h2 id=二-bias-vs-variance>二. Bias vs. Variance</h2><h3 id=1-diagnosing-bias-vs-variance-诊断偏差和方差>1. Diagnosing Bias vs. Variance 诊断偏差和方差</h3><p>在机器学习中，偏差（bias）反映了模型无法描述数据规律，而方差（variance）反映了模型对训练集过度敏感，而丢失了数据规律，高偏差和高方差都会造成新数据到来时，模型给出错误的预测。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/74_3.png alt></p><p>还是以这个图为例，最左边的图是欠拟合，最右边的图是过拟合。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/74_4.png alt></p><p>上图是 训练集、交叉验证集误差随多项式次数 d 的变化规律。横坐标是我们的d，也就是多项式的个数，纵坐标就是我们的代价函数。</p><p>我们先来看一下红色曲线 $J_{training}(\theta)$ ，随着多项式个数的增加，其假设函数是越来越接近要拟合的数据，所以其代价函数会随着多项式个数的增加下降。</p><p>然后绿色的曲线是 $J_{cross-validation}(\theta)$ ,当多项式个数比较少的时候，那当然会出现欠拟合的现象，所以一开始其代价函数 $J_{cross-validation}(\theta)$ 是很大的，随着多项式个数的增加而下降，但是当其多项式个数再继续增加的话，就会出现过拟合的现象， $J_{cross-validation}(\theta)$ 就又会增加。所以 $J_{cross-validation}(\theta)$ 函数是先递减再递增的，在其最低点就是最合适的多项式次数。</p><p>多项式回归中，如果多项式次数较高，则容易造成过拟合，此时训练误差很低，但是对于新数据的泛化能力较差，导致交叉验证集和测试集的误差都很高，此时模型出现了<strong>高方差(过拟合)</strong>：</p><p>$$
\left{\begin{matrix}
J_{train}(\theta) ;;;is;; low\
J_{cv}(\theta)&#187;J_{test}(\theta)
\end{matrix}\right.
$$</p><p>过拟合的情况下，训练集误差通常比较小，并且远小于交叉验证误差。</p><p>而当次数较低时，又易出现欠拟合的状况，此时无论是训练集，交叉验证集，还是测试集，都会有很高的误差，此时模型出现了<strong>高偏差(欠拟合)</strong>：</p><p>$$
\left{\begin{matrix}
J_{train}(\theta),J_{cv}(\theta);;; is ;; high\
J_{cv}(\theta) \approx J_{test}(\theta)
\end{matrix}\right.
$$</p><p>欠拟合的情况下，训练集误差会很大。</p><p>为什么 $J_{cross-validation}(\theta)$ 会先降后升，而 $J_{training}(\theta)$ 一直下降？</p><p>原因是 $\theta$ 是只针对训练集所训练出来的，当其代入到 $J_{cross-validation}(\theta)$ 后，就会随着多项式的增加数据偏差就会越来越大。</p><hr><h3 id=2-regularization-and-biasvariance-正则化的偏差和方差>2. Regularization and Bias/Variance 正则化的偏差和方差</h3><p><img src=https://img.halfrost.com/Blog/ArticleImage/74_5.png alt></p><p>为了防止过拟合的现象，我们加上一个正则化项，但是正则化参数 $\lambda$ 与过拟合又有什么关系呢？</p><p>当 $\lambda$ 很大的时候，就会使得后面的每一个 $\theta_i$ 都被惩罚了，所以只剩下 $\theta_0$ ，那么其假设函数就会变成一条直线，出现欠拟合的现象。</p><p>当 $\lambda$ 很小的话，一个极端例子就是 $\lambda=0$ ，也就是相当于没有加正则化那项，这就会导致过拟合的现象。</p><p>$\lambda$的取值不能过大也不能过小。</p><p>$\lambda$的取值可以在 $\left[0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24\right]$，在这12个不同的模型中针对每一个 $\lambda$ 的值，都去计算出一个最小代价函数,从而得到 $\Theta^{(i)}$</p><p>得到了12个 $\Theta^{(i)}$ 以后，就再用交叉验证集去评价它们。即计算每个 $\Theta$ 在交叉验证集上的平均误差平方和 $J_{cv}(\Theta^{(i)})$</p><p>选择一个交叉验证集误差最小的 $\lambda$ 最能拟合数据的作为正则化参数。</p><p>最后拿这个正则化参数去测试集里面验证 $J_{test}(\Theta^{(i)})$ 预测效果如何。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/74_6.png alt></p><p>随着 $\lambda$ 参数的增大， $J_{train}(\theta)$ 自然也会随之增大，这是因为当 $\lambda=0$ 的时候， $J_{train}(\theta)$ 是没有正则化项的。</p><p>但是对于 $J_{cv}(\theta)$ 来说，它假设函数里面的 $\theta$ 是根据训练集里面拟合出来的，所以在没有加入正则化前， $J_{cv}(\theta)$ 是很大的。但是随着 $\lambda$ 的逐渐增大，也就是随着正则化的效果逐渐体现出来，在交叉验证集里面与测试数据就会越来越拟合，这时候的 $J_{cv}(\theta)$ 自然会慢慢下降。但是当 $\lambda$ 变得足够大的时候，交叉训练集的 $h_\theta(x)$ 就会趋近一条直线，$J_{cv}(\theta)$ 自然会随之上升。</p><hr><h3 id=3-learning-curves-学习曲线>3. Learning Curves 学习曲线</h3><p><img src=https://img.halfrost.com/Blog/ArticleImage/74_7.png alt></p><p>假设我们用 $h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2$ 去拟合数据，当数据只有几个的时候，拟合效果那肯定的非常好的，但是，当数据越来越多，我们的假设函数因为多项式太少就不能很好地拟合数据了。所以训练集的误差 $J_{train}(\theta)$ 会随着数据的增多而增大。如上图蓝色的曲线。</p><p>但是对于交叉验证集呢？因为一开始只有几个数据，那么在训练集拟合出来的参数就有很大的可能不适合交叉验证集，所以在数据很小的情况下其误差是很大的，但是随着数据的慢慢增多，虽然个别的数据拟合不上，但是整体的拟合效果那肯定比只有几个数据的时候好了，所以其整体误差是逐步下降的。如上图粉色的曲线。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/74_8.png alt></p><p>当数据存在高偏差也就是欠拟合的时候，即使数据再继续增多也无补于事，所以其误差会趋于一个平衡的位置，而且 $J_{train}(\theta)$ 和 $J_{cv}(\theta)$ 的误差都会很大。</p><p>所以，当数据存在欠拟合的问题，我们选用更多的训练样本是没有办法解决问题的。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/74_9.png alt></p><p>当数据存在高方差也就是过拟合的时候，随着数据的增多，因为过拟合所以在训练集基本能完美拟合其数据，所以训练集的误差虽然会上升，但是其幅度是非常缓慢的，在交叉验证集也一样，所以过拟合的时候其图像如上，在 $J_{train}(\theta)$ 和 $J_{cv}(\theta)$ 之间有一大段空隙。</p><p>所以，当数据存在过拟合的现象，选用更多的样本有利于我们解决这个问题。</p><hr><h3 id=4-deciding-what-to-do-next-revisited-决定下一步该做什么>4. Deciding What to Do Next Revisited 决定下一步该做什么</h3><p>总结 ：</p><table><thead><tr><th align=left>手段</th><th align=center>使用场景</th></tr></thead><tbody><tr><td align=left>采集更多的样本</td><td align=center>高方差(过拟合)</td></tr><tr><td align=left>降低特征维度</td><td align=center>高方差(过拟合)</td></tr><tr><td align=left>采集更多的特征</td><td align=center>高偏差(欠拟合)</td></tr><tr><td align=left>进行高次多项式回归</td><td align=center>高偏差(欠拟合)</td></tr><tr><td align=left>降低参数 λ</td><td align=center>高偏差(欠拟合)</td></tr><tr><td align=left>增大参数 λ</td><td align=center>高方差(过拟合)</td></tr></tbody></table><p><img src=https://img.halfrost.com/Blog/ArticleImage/74_10.png alt></p><p>当我们选用一些较小的神经网络，虽然其计算量较少，但是容易出现欠拟合的现象。相反，我们选用一些层数比较多，层的单元比较多的神经网络，容易出现过拟合的现象。我们之前提到越大型的神经网络效果越好，为了防止出现过拟合的现象，我们可以使用正则化的方法来修正。</p><p>使用单个隐藏层是一个很好的默认开始。您可以使用交叉验证集在许多隐藏层上训练您的神经网络。然后您可以选择性能最好的一个。</p><p>模型复杂性影响：</p><ul><li>低阶多项式（低模型复杂度）具有高偏差和低方差。在这种情况下，该模型很难一致</li><li>高阶多项式（高模型复杂度）非常适合训练数据，测试数据极其糟糕。这些对训练数据的偏倚低，但差异很大</li><li>实际上，我们希望选择一个介于两者之间的模型，它可以很好地推广，但也可以很好地适合数据。</li></ul><hr><h2 id=三-advice-for-applying-machine-learning-测试>三. Advice for Applying Machine Learning 测试</h2><h3 id=1-question-1>1. Question 1</h3><p>You train a learning algorithm, and find that it has unacceptably high error on the test set. You plot the learning curve, and obtain the figure below. Is the algorithm suffering from high bias, high variance, or neither?</p><p><img src=http://spark-public.s3.amazonaws.com/ml/images/10.1-c.png alt></p><p>A. High variance</p><p>B. Neither</p><p>C. High bias</p><p>解答：A</p><p>高方差的图像</p><h3 id=2-question-2>2. Question 2</h3><p>Suppose you have implemented regularized logistic regression to classify what object is in an image (i.e., to do object recognition). However, when you test your hypothesis on a new set of images, you find that it makes unacceptably large errors with its predictions on the new images. However, your hypothesis performs well (has low error) on the training set. Which of the following are promising steps to take? Check all that apply.</p><p>A. Try adding polynomial features.</p><p>B. Get more training examples.</p><p>C. Try using a smaller set of features.</p><p>D. Use fewer training examples.</p><p>解答：B、C</p><p>过拟合可以减少特征量和增加训练样本数量，或者增大正则化参数 $\lambda$ 。</p><h3 id=3-question-3>3. Question 3</h3><p>Suppose you have implemented regularized logistic regression to predict what items customers will purchase on a web shopping site. However, when you test your hypothesis on a new set of customers, you find that it makes unacceptably large errors in its predictions. Furthermore, the hypothesis performs poorly on the training set. Which of the following might be promising steps to take? Check all that apply.</p><p>A. Try using a smaller set of features.</p><p>B. Try adding polynomial features.</p><p>C. Try to obtain and use additional features.</p><p>D. Try increasing the regularization parameter $\lambda$.</p><p>解答：B、C</p><p>欠拟合可以增加特征量和假设函数的多项式。</p><h3 id=4-question-4>4. Question 4</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. Suppose you are training a regularized linear regression model. The recommended way to choose what value of regularization parameter $\lambda$ to use is to choose the value of $\lambda$ which gives the lowest test set error.</p><p>B. The performance of a learning algorithm on the training set will typically be better than its performance on the test set.</p><p>C. Suppose you are training a regularized linear regression model.The recommended way to choose what value of regularization parameter $\lambda$ to use is to choose the value of $\lambda$ which gives the lowest training set error.</p><p>D. Suppose you are training a regularized linear regression model. The recommended way to choose what value of regularization parameter $\lambda$ to use is to choose the value of $\lambda$ which gives the lowest cross validation error.</p><p>解答：B、D</p><p>在正则化线性回归中，$\lambda$ 选择一个交叉验证集误差最小的 λλ 最能拟合数据的作为正则化参数。</p><h3 id=5-question-5>5. Question 5</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. If a learning algorithm is suffering from high variance, adding more training examples is likely to improve the test error.</p><p>B. We always prefer models with high variance (over those with high bias) as they will able to better fit the training set.</p><p>C. If a learning algorithm is suffering from high bias, only adding more training examples may not improve the test error significantly.</p><p>D. When debugging learning algorithms, it is useful to plot a learning curve to understand if there is a high bias or high variance problem.</p><p>解答：A、C、D</p><p>A 过拟合高方差，增加样本数量有用。<br>B 高偏差和高方差的模型都不好。<br>C 增加训练样本对于欠拟合是没用的正确。<br>D 绘制学习曲线有利于帮助我们分析问题正确。</p><hr><blockquote><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a></p><p>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a></p><p>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Advice_for_Applying_Machine_Learning.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Advice_for_Applying_Machine_Learning.ipynb</a></p></blockquote><img src=https://img.halfrost.com/wechat-qr-code.png></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#一-evaluating-a-learning-algorithm>一. Evaluating a Learning Algorithm</a><ul><li><a href=#1-evaluating-a-hypothesis-评价假设函数>1. Evaluating a Hypothesis 评价假设函数</a></li><li><a href=#2-model-selection-and-trainvalidationtest-sets-模型选择和训练集验证集测试集>2. Model Selection and Train/Validation/Test Sets 模型选择和训练集/验证集/测试集</a></li></ul></li><li><a href=#二-bias-vs-variance>二. Bias vs. Variance</a><ul><li><a href=#1-diagnosing-bias-vs-variance-诊断偏差和方差>1. Diagnosing Bias vs. Variance 诊断偏差和方差</a></li><li><a href=#2-regularization-and-biasvariance-正则化的偏差和方差>2. Regularization and Bias/Variance 正则化的偏差和方差</a></li><li><a href=#3-learning-curves-学习曲线>3. Learning Curves 学习曲线</a></li><li><a href=#4-deciding-what-to-do-next-revisited-决定下一步该做什么>4. Deciding What to Do Next Revisited 决定下一步该做什么</a></li></ul></li><li><a href=#三-advice-for-applying-machine-learning-测试>三. Advice for Applying Machine Learning 测试</a><ul><li><a href=#1-question-1>1. Question 1</a></li><li><a href=#2-question-2>2. Question 2</a></li><li><a href=#3-question-3>3. Question 3</a></li><li><a href=#4-question-4>4. Question 4</a></li><li><a href=#5-question-5>5. Question 5</a></li></ul></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f"><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&text=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&is_video=false&description=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f"><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-stumbleupon fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-digg fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&name=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fAdvice_for_Applying_Machine_Learning.ipynb%0a%20%e4%b8%80.%20Evaluating%20a%20Learning%20Algorithm%20%e6%83%b3%e8%a6%81%e9%99%8d%e4%bd%8e%e9%a2%84%e6%b5%8b%e8%af%af%e5%b7%ae%ef%bc%8c%e5%8d%b3%e6%8f%90%e9%ab%98%e9%a2%84%e6%b5%8b%e7%b2%be%e5%ba%a6%ef%bc%8c%e6%88%91%e4%bb%ac%e5%be%80%e5%be%80%e4%bc%9a%e9%87%87%e7%94%a8%e8%bf%99%e4%ba%9b%e6%89%8b%e6%ae%b5%ef%bc%9a%0a%20%20%e9%87%87%e9%9b%86%e6%9b%b4%e5%a4%9a%e7%9a%84%e6%a0%b7%e6%9c%ac%0a%e9%94%99%e8%af%af%e7%9a%84%e8%ae%a4%e4%b8%ba%e6%a0%b7%e6%9c%ac%e8%b6%8a%e5%a4%9a%e8%b6%8a%e5%a5%bd%ef%bc%8c%e5%85%b6%e5%ae%9e%e6%95%b0%e6%8d%ae%e5%a4%9a%e5%b9%b6%e4%b8%8d%e6%98%af%e8%b6%8a%e5%a5%bd%e3%80%82%0a%20%20%e9%99%8d%e4%bd%8e%e7%89%b9%e5%be%81%e7%bb%b4%e5%ba%a6%0a%e9%99%8d%e7%bb%b4%e5%8f%af%e8%83%bd%e5%8e%bb%e6%8e%89%e4%ba%86%e6%9c%89%e7%94%a8%e7%9a%84%e7%89%b9%e5%be%81%e3%80%82%0a%20%20%e9%87%87%e9%9b%86%e6%9b%b4%e5%a4%9a%e7%9a%84%e7%89%b9%e5%be%81%0a%e5%a2%9e%e5%8a%a0%e4%ba%86%e8%ae%a1%e7%ae%97%e8%b4%9f%e6%8b%85%ef%bc%8c%e4%b9%9f%e5%8f%af%e8%83%bd%e5%af%bc%e8%87%b4%e8%bf%87%e6%8b%9f%e5%90%88%e3%80%82%0a%20%20%e8%bf%9b%e8%a1%8c%e9%ab%98%e6%ac%a1%e5%a4%9a%e9%a1%b9%e5%bc%8f%e5%9b%9e%e5%bd%92%0a%e8%bf%87%e9%ab%98%e7%9a%84%e5%a4%9a%e9%a1%b9%e5%bc%8f%e5%8f%af%e8%83%bd%e9%80%a0%e6%88%90%e8%bf%87%e6%8b%9f%e5%90%88%e3%80%82%0a%20%20%e8%b0%83%e8%af%95%e6%ad%a3%e8%a7%84%e5%8c%96%e5%8f%82%e6%95%b0%20%24%5clambda%24%2c%e5%a2%9e%e5%a4%a7%e6%88%96%e8%80%85%e5%87%8f%e5%b0%91%20%24%5clambda%24%0a%e5%a2%9e%e5%a4%a7%e6%88%96%e8%80%85%e5%87%8f%e5%b0%91%e9%83%bd%e6%98%af%e5%87%ad%e6%84%9f%e8%a7%89%e3%80%82%0a%20%20%e6%9c%89%e8%bf%99%e4%b9%88%e5%a4%9a%e7%a7%8d%e8%a7%a3%e5%86%b3%e5%8a%9e%e6%b3%95%e6%88%91%e4%bb%ac%e6%80%8e%e4%b9%88%e7%9f%a5%e9%81%93%e6%98%af%e5%93%aa%e4%b8%80%e7%a7%8d%e5%91%a2%ef%bc%9f%e5%be%88%e5%a4%9a%e4%ba%ba%e9%80%89%e6%8b%a9%e8%bf%99%e4%ba%9b%e6%96%b9%e6%b3%95%e7%9a%84%e6%a0%87%e5%87%86%e5%b0%b1%e6%98%af%e5%87%ad%e6%84%9f%e8%a7%89%e9%9a%8f%e4%be%bf%e9%80%89%e6%8b%a9%e4%b8%80%e7%a7%8d%ef%bc%8c%e7%84%b6%e5%90%8e%e8%8a%b1%e5%be%88%e9%95%bf%e7%9a%84%e6%97%b6%e9%97%b4%e6%9c%80%e5%90%8e%e5%8f%91%e7%8e%b0%e6%98%af%e6%b2%a1%e7%94%a8%e7%9a%84%ef%bc%8c%e8%b5%b0%e4%b8%8a%e4%ba%86%e4%b8%8d%e5%bd%92%e8%b7%af%e3%80%82%e6%89%80%e4%bb%a5%e4%b8%8b%e9%9d%a2%e6%88%91%e4%bb%ac%e4%bb%8b%e7%bb%8d%e4%b8%80%e6%88%91%e4%bb%ac%e9%9c%80%e8%a6%81%e4%b8%80%e7%a7%8d%e7%ae%80%e5%8d%95%e6%9c%89%e6%95%88%e7%9a%84%e5%8a%9e%e6%b3%95%ef%bc%8c%e6%88%91%e4%bb%ac%e5%b0%86%e5%85%b6%e7%a7%b0%e4%b8%ba%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%8a%e6%96%ad%ef%bc%88Machine%20learning%20diagnostic%ef%bc%89%e3%80%82%0a1.%20Evaluating%20a%20Hypothesis%20%e8%af%84%e4%bb%b7%e5%81%87%e8%ae%be%e5%87%bd%e6%95%b0%20%e9%a6%96%e5%85%88%e6%88%91%e4%bb%ac%e8%a6%81%e8%af%84%e4%bc%b0%e7%9a%84%e6%98%af%e6%88%91%e4%bb%ac%e7%9a%84%e5%81%87%e8%ae%be%e5%87%bd%e6%95%b0%ef%bc%88Hypothesis%ef%bc%89%e3%80%82%e5%bd%93%e6%88%91%e4%bb%ac%e9%80%89%e6%8b%a9%e7%89%b9%e5%be%81%e5%80%bc%e6%88%96%e8%80%85%e5%8f%82%e6%95%b0%e6%9d%a5%e4%bd%bf%e8%ae%ad%e7%bb%83%e9%9b%86%e8%af%af%e5%b7%ae%e6%9c%80%e5%b0%8f%e5%8c%96%ef%bc%8c%e4%bd%86%e6%98%af%e6%88%91%e4%bb%ac%e4%bc%9a%e9%81%87%e5%88%b0%e8%bf%87%e6%8b%9f%e5%90%88%e7%9a%84%e9%97%ae%e9%a2%98%ef%bc%8c%e6%8e%a8%e5%b9%bf%e5%88%b0%e6%96%b0%e7%9a%84%e8%ae%ad%e7%bb%83%e9%9b%86%e5%b0%b1%e4%b8%8d%e5%86%8d%e4%bd%bf%e7%94%a8%e4%ba%86%e3%80%82%e8%80%8c%e4%b8%94%e5%bd%93%e7%89%b9%e5%be%81%e9%87%8f%e5%be%88%e5%a4%9a%e7%9a%84%e6%97%b6%e5%80%99%ef%bc%8c%e6%88%91%e4%bb%ac%e5%b0%b1%e4%b8%8d%e8%83%bd%e5%b0%86%20%24J%28%5ctheta%29%24%20%e5%8f%af%e8%a7%86%e5%8c%96%e7%9c%8b%e5%87%ba%e5%85%b6%e6%98%af%e5%90%a6%e9%9a%8f%e7%9d%80%e8%bf%ad%e4%bb%a3%e6%ac%a1%e6%95%b0%e8%80%8c%e4%b8%8b%e9%99%8d%e4%ba%86%e3%80%82%e6%89%80%e4%bb%a5%e6%88%91%e4%bb%ac%e9%87%87%e7%94%a8%e4%bb%a5%e4%b8%8b%e7%9a%84%e6%96%b9%e6%b3%95%e6%9d%a5%e8%af%84%e4%bc%b0%e6%88%91%e4%bb%ac%e7%9a%84%e5%81%87%e8%ae%be%e5%87%bd%e6%95%b0%ef%bc%9a%0a%e5%81%87%e8%ae%be%e6%9c%89%2010%20%e7%bb%84%e6%95%b0%e6%8d%ae%ef%bc%8c%e9%9a%8f%e6%9c%ba%e6%8a%8a%2070%25%20%e5%81%9a%e4%b8%ba%e8%ae%ad%e7%bb%83%e9%9b%86%ef%bc%8c%e5%89%a9%e4%b8%8b%e7%9a%84%2030%25%20%e5%81%9a%e4%b8%ba%e6%b5%8b%e8%af%95%e9%9b%86%e3%80%82%e8%ae%ad%e7%bb%83%e9%9b%86%e5%92%8c%e6%b5%8b%e8%af%95%e9%9b%86%e5%b0%bd%e9%87%8f%e4%bf%9d%e8%af%81%e6%98%af%e9%9a%8f%e6%9c%ba%e6%8e%92%e5%88%97%e3%80%82%0a%e6%8e%a5%e4%b8%8b%e6%9d%a5%ef%bc%9a"><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2fadvice_for_applying_machine_learning%2f&t=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e8%af%84%e4%bc%b0"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu class=icon href=# onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden=true></i>Menu</a>
<a id=toc class=icon href=# onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden=true></i>TOC</a>
<a id=share class=icon href=# onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden=true></i>share</a>
<a id=top style=display:none class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i>Top</a></div></div></div><footer id=footer><div class=footer-left><p class=copyright style=float:left;margin-bottom:0><a href=https://github.com/halfrost/Halfrost-Field class=github-repo style=height:18px><span class=gadget-github></span>Star</a>
Copyright &copy;halfrost 2016 - 2021
<a href=http://www.miit.gov.cn/>鄂ICP备16014744号</a></p><br><p class="copyright statistics" style=margin-bottom:20px><span id=busuanzi_container_site_pv>Cumulative Page Views <span id=busuanzi_value_site_pv></span>| Unique Visitors <span id=busuanzi_value_site_uv></span></span></p></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script><script src=/main.min.f870a4d110314b9e50e65f8ac982dc1c9c376c8f1a5083d39c62cfc49073f011.js></script><script async src=/prism.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}};</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>