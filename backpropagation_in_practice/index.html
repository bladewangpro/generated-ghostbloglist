<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=theme-color content="#FFFFFF"><meta http-equiv=x-ua-compatible content="IE=edge"><title>神经网络反向传播实践 | prometheus</title><meta name=description content="Explore in every moment of the hard thinking"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="神经网络反向传播实践"><meta property="og:description" content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Backpropagation_in_Practice.ipynb
 一. Backpropagation in Practice 为了利用梯度下降的优化算法，需要用到 fminunc 函数。其输入的参数是 $\theta$ ，函数的返回值是代价函数 jVal 和导数值 gradient。然后将返回值传递给高级优化算法 fminunc，然后输出为输入值 @costFunction，以及 $\theta$ 值的初始值。
其中参数 $\Theta_1,\Theta_2,\Theta_3,\cdots$ 和 $D^{(1)},D^{(2)},D^{(3)},\cdots$ 都为矩阵，那么为了能调用 fminunc 函数，我们要将其变成向量，
假如我们 $\Theta_1,\Theta_2,\Theta_3$ 参数和 $D^{(1)},D^{(2)},D^{(3)}$ 参数，Theta1 是 $10 * 11$，Theta2 是 $10 * 11$，Theta3 是 $1 * 11$。
% 打包成一个向量 thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ] deltaVector = [ D1(:); D2(:); D3(:) ] % 解包还原 Theta1 = reshape(thetaVector(1:110),10,11) Theta2 = reshape(thetaVector(111:220),10,11) Theta3 = reshape(thetaVector(221:231),1,11) 所以套路是："><meta property="og:type" content="article"><meta property="og:url" content="https://new.halfrost.com/backpropagation_in_practice/"><meta property="article:published_time" content="2018-03-26T08:35:00+00:00"><meta property="article:modified_time" content="2018-03-26T08:35:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="神经网络反向传播实践"><meta name=twitter:description content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Backpropagation_in_Practice.ipynb
 一. Backpropagation in Practice 为了利用梯度下降的优化算法，需要用到 fminunc 函数。其输入的参数是 $\theta$ ，函数的返回值是代价函数 jVal 和导数值 gradient。然后将返回值传递给高级优化算法 fminunc，然后输出为输入值 @costFunction，以及 $\theta$ 值的初始值。
其中参数 $\Theta_1,\Theta_2,\Theta_3,\cdots$ 和 $D^{(1)},D^{(2)},D^{(3)},\cdots$ 都为矩阵，那么为了能调用 fminunc 函数，我们要将其变成向量，
假如我们 $\Theta_1,\Theta_2,\Theta_3$ 参数和 $D^{(1)},D^{(2)},D^{(3)}$ 参数，Theta1 是 $10 * 11$，Theta2 是 $10 * 11$，Theta3 是 $1 * 11$。
% 打包成一个向量 thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ] deltaVector = [ D1(:); D2(:); D3(:) ] % 解包还原 Theta1 = reshape(thetaVector(1:110),10,11) Theta2 = reshape(thetaVector(111:220),10,11) Theta3 = reshape(thetaVector(221:231),1,11) 所以套路是："><link rel=stylesheet href=/css/style-white.min.css><link rel=manifest href=/manifest.json><link rel=stylesheet href=/prism.css><link href=/images/apple-touch-icon-60x60.png rel=apple-touch-icon sizes=60x60><link href=/images/apple-touch-icon-76x76.png rel=apple-touch-icon sizes=76x76><link href=/images/apple-touch-icon-120x120.png rel=apple-touch-icon sizes=120x120><link href=/images/apple-touch-icon-152x152.png rel=apple-touch-icon sizes=152x152><link href=/images/apple-touch-icon-180x180.png rel=apple-touch-icon sizes=180x180><link href=/images/apple-touch-icon-512x512.png rel=apple-touch-icon sizes=512x512><link href=/images/apple-touch-icon-1024x1024.png rel=apple-touch-icon sizes=1024x1024><script async>if('serviceWorker'in navigator){navigator.serviceWorker.register("\/serviceworker-v1.min.a64912b78d282eab1ad3715a0943da21616e5f326f8afea27034784ad445043b.js").then(function(){if(navigator.serviceWorker.controller){console.log('Assets cached by the controlling service worker.');}else{console.log('Please reload this page to allow the service worker to handle network operations.');}}).catch(function(error){console.log('ERROR: '+error);});}else{console.log('Service workers are not supported in the current browser.');}</script><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://new.halfrost.com/images/favicon.ico><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-82753806-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class="single-max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a><a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a><a id=top-icon-tablet href=# onclick="$('html, body').animate({scrollTop:0},'fast');" style=display:none><i class="fas fa-chevron-up fa-lg"></i></a><span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://new.halfrost.com/neural_networks_learning/><i class="fas fa-chevron-left" aria-hidden=true onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li><li><a class=icon href=https://new.halfrost.com/advice_for_applying_machine_learning/><i class="fas fa-chevron-right" aria-hidden=true onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li><li><a class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up" aria-hidden=true onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li><li><a class=icon href=#><i class="fas fa-share-alt" aria-hidden=true onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f"><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&text=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&is_video=false&description=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f"><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-stumbleupon" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-digg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&name=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fBackpropagation_in_Practice.ipynb%0a%20%e4%b8%80.%20Backpropagation%20in%20Practice%20%e4%b8%ba%e4%ba%86%e5%88%a9%e7%94%a8%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e7%9a%84%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%8c%e9%9c%80%e8%a6%81%e7%94%a8%e5%88%b0%20fminunc%20%e5%87%bd%e6%95%b0%e3%80%82%e5%85%b6%e8%be%93%e5%85%a5%e7%9a%84%e5%8f%82%e6%95%b0%e6%98%af%20%24%5ctheta%24%20%ef%bc%8c%e5%87%bd%e6%95%b0%e7%9a%84%e8%bf%94%e5%9b%9e%e5%80%bc%e6%98%af%e4%bb%a3%e4%bb%b7%e5%87%bd%e6%95%b0%20jVal%20%e5%92%8c%e5%af%bc%e6%95%b0%e5%80%bc%20gradient%e3%80%82%e7%84%b6%e5%90%8e%e5%b0%86%e8%bf%94%e5%9b%9e%e5%80%bc%e4%bc%a0%e9%80%92%e7%bb%99%e9%ab%98%e7%ba%a7%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%20fminunc%ef%bc%8c%e7%84%b6%e5%90%8e%e8%be%93%e5%87%ba%e4%b8%ba%e8%be%93%e5%85%a5%e5%80%bc%20%40costFunction%ef%bc%8c%e4%bb%a5%e5%8f%8a%20%24%5ctheta%24%20%e5%80%bc%e7%9a%84%e5%88%9d%e5%a7%8b%e5%80%bc%e3%80%82%0a%e5%85%b6%e4%b8%ad%e5%8f%82%e6%95%b0%20%24%5cTheta_1%2c%5cTheta_2%2c%5cTheta_3%2c%5ccdots%24%20%e5%92%8c%20%24D%5e%7b%281%29%7d%2cD%5e%7b%282%29%7d%2cD%5e%7b%283%29%7d%2c%5ccdots%24%20%e9%83%bd%e4%b8%ba%e7%9f%a9%e9%98%b5%ef%bc%8c%e9%82%a3%e4%b9%88%e4%b8%ba%e4%ba%86%e8%83%bd%e8%b0%83%e7%94%a8%20fminunc%20%e5%87%bd%e6%95%b0%ef%bc%8c%e6%88%91%e4%bb%ac%e8%a6%81%e5%b0%86%e5%85%b6%e5%8f%98%e6%88%90%e5%90%91%e9%87%8f%ef%bc%8c%0a%e5%81%87%e5%a6%82%e6%88%91%e4%bb%ac%20%24%5cTheta_1%2c%5cTheta_2%2c%5cTheta_3%24%20%e5%8f%82%e6%95%b0%e5%92%8c%20%24D%5e%7b%281%29%7d%2cD%5e%7b%282%29%7d%2cD%5e%7b%283%29%7d%24%20%e5%8f%82%e6%95%b0%ef%bc%8cTheta1%20%e6%98%af%20%2410%20%2a%2011%24%ef%bc%8cTheta2%20%e6%98%af%20%2410%20%2a%2011%24%ef%bc%8cTheta3%20%e6%98%af%20%241%20%2a%2011%24%e3%80%82%0a%25%20%e6%89%93%e5%8c%85%e6%88%90%e4%b8%80%e4%b8%aa%e5%90%91%e9%87%8f%20thetaVector%20%3d%20%5b%20Theta1%28%3a%29%3b%20Theta2%28%3a%29%3b%20Theta3%28%3a%29%3b%20%5d%20deltaVector%20%3d%20%5b%20D1%28%3a%29%3b%20D2%28%3a%29%3b%20D3%28%3a%29%20%5d%20%25%20%e8%a7%a3%e5%8c%85%e8%bf%98%e5%8e%9f%20Theta1%20%3d%20reshape%28thetaVector%281%3a110%29%2c10%2c11%29%20Theta2%20%3d%20reshape%28thetaVector%28111%3a220%29%2c10%2c11%29%20Theta3%20%3d%20reshape%28thetaVector%28221%3a231%29%2c1%2c11%29%20%e6%89%80%e4%bb%a5%e5%a5%97%e8%b7%af%e6%98%af%ef%bc%9a"><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&t=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><nav id=TableOfContents><ul><li><a href=#一-backpropagation-in-practice>一. Backpropagation in Practice</a></li><li><a href=#二-gradient-checking>二. Gradient Checking</a></li><li><a href=#三-random-initialization>三. Random Initialization</a></li><li><a href=#四-总结>四. 总结</a><ul><li><a href=#1-准备>1. 准备</a></li><li><a href=#2-训练>2. 训练</a></li></ul></li><li><a href=#五-neural-networks-learning-测试>五. Neural Networks: Learning 测试</a><ul><li><a href=#1-question-1>1. Question 1</a></li><li><a href=#2-question-2>2. Question 2</a></li><li><a href=#3-question-3>3. Question 3</a></li><li><a href=#4-question-4>4. Question 4</a></li><li><a href=#5-question-5>5. Question 5</a></li></ul></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">神经网络反向传播实践</h1><div class=meta><div class=postdate><time datetime="2018-03-26 08:35:00 +0000 UTC" itemprop=datePublished>Mar 26</time></div><div class=article-category><i class="fas fa-archive"></i><a class=category-link href=/categories/machine-learning>Machine Learning</a>
,
<a class=category-link href=/categories/ai>AI</a></div><div class=article-tag><i class="fas fa-tag"></i><a class=tag-link href=/tags/machine-learning rel=tag>Machine Learning</a>
,
<a class=tag-link href=/tags/ai rel=tag>AI</a></div></div></header><div class=content itemprop=articleBody><blockquote><p>由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/contents.md>Github</a> 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。</p><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a><br>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a><br>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Backpropagation_in_Practice.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Backpropagation_in_Practice.ipynb</a></p></blockquote><h2 id=一-backpropagation-in-practice>一. Backpropagation in Practice</h2><p>为了利用梯度下降的优化算法，需要用到 fminunc 函数。其输入的参数是 $\theta$ ，函数的返回值是代价函数 jVal 和导数值 gradient。然后将返回值传递给高级优化算法 fminunc，然后输出为输入值 @costFunction，以及 $\theta$ 值的初始值。</p><p>其中参数 $\Theta_1,\Theta_2,\Theta_3,\cdots$ 和 $D^{(1)},D^{(2)},D^{(3)},\cdots$ 都为矩阵，那么为了能调用 fminunc 函数，我们要将其变成向量，</p><p>假如我们 $\Theta_1,\Theta_2,\Theta_3$ 参数和 $D^{(1)},D^{(2)},D^{(3)}$ 参数，Theta1 是 $10 * 11$，Theta2 是 $10 * 11$，Theta3 是 $1 * 11$。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>
<span style=color:#f92672>%</span> <span style=color:#960050;background-color:#1e0010>打包成一个向量</span>
thetaVector <span style=color:#f92672>=</span> [ Theta1(<span style=color:#f92672>:</span>); Theta2(<span style=color:#f92672>:</span>); Theta3(<span style=color:#f92672>:</span>); ]
deltaVector <span style=color:#f92672>=</span> [ D1(<span style=color:#f92672>:</span>); D2(<span style=color:#f92672>:</span>); D3(<span style=color:#f92672>:</span>) ]

<span style=color:#f92672>%</span> <span style=color:#960050;background-color:#1e0010>解包还原</span>
Theta1 <span style=color:#f92672>=</span> reshape(thetaVector(<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>110</span>),<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>11</span>)
Theta2 <span style=color:#f92672>=</span> reshape(thetaVector(<span style=color:#ae81ff>111</span><span style=color:#f92672>:</span><span style=color:#ae81ff>220</span>),<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>11</span>)
Theta3 <span style=color:#f92672>=</span> reshape(thetaVector(<span style=color:#ae81ff>221</span><span style=color:#f92672>:</span><span style=color:#ae81ff>231</span>),<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>11</span>)


</code></pre></div><p>所以<strong>套路</strong>是：</p><ol><li><p>先将 $\Theta_1,\Theta_2,\Theta_3$ ,这些矩阵展开为一个长向量赋值给 initialTheta，然后作为theta参数的初始设置传入优化函数 fminunc。</p></li><li><p>再实现代价函数 costFunction。costFunction 函数将传入参数 thetaVec（就是刚才包含所有 $\Theta$ 参数的向量），然后通过 reshape 函数得到初始的矩阵，这样可以更方便地通过前向传播和反向传播以求得导数 $D^{(1)},D^{(2)},D^{(3)}$ 和代价函数 $F(\Theta)$ 。</p></li><li><p>最后按顺序展开得到 gradientVec，让它们保持和之前展开的 $\theta$ 值同样的顺序。以一个向量的形式返回这些导数值。</p></li></ol><hr><h2 id=二-gradient-checking>二. Gradient Checking</h2><p>在计算导数的时候，习惯将其等于在该点的导数，在我们使用梯度下降计算导数的时候，虽然可能 $F(\Theta)$ 每次迭代都在下降，但是因为反向传播的复杂性，可能导致我们的代码存在 BUG。有一个办法叫做梯度检验（Gradient Checking），它能减少这种错误的概率（出现这个问题的原因都和反向传播的错误实现有关）。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/73_1.png alt></p><p>在我们求该点的斜率的时候，我们不直接使用其导数，而是用 $$\frac{d}{d\Theta}F(\Theta)\approx\frac{F(\Theta+\epsilon)-F(\Theta-\epsilon)}{2\epsilon}$$ 代替。通常 $\epsilon$ 取较小的一个数。（其实就是使用导数的定义）</p><p>上面这种算法是双侧差分算法，与之相对的是单侧差分算法</p><p>$$\frac{d}{d\Theta}F(\Theta)\approx\frac{F(\Theta+\epsilon)-F(\Theta)}{\epsilon}$$</p><p>单侧差分和双侧差分相比，双侧差分可以得到更加准确的结果。</p><p>推广一下双侧差分：</p><p>$$\frac{d}{d\Theta_j}J(\Theta)\approx\frac{J(\Theta_1,…,+\Theta_j+\epsilon,…,\Theta_n)-J(\Theta_1,…,+\Theta_j-\epsilon,…,\Theta_n)}{2\epsilon}$$</p><p>对应代码实现如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>
epsilon <span style=color:#f92672>=</span> <span style=color:#ae81ff>1e-4</span>;
<span style=color:#66d9ef>for</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>n,
  thetaPlus <span style=color:#f92672>=</span> theta;
  thetaPlus(i) <span style=color:#f92672>+=</span> epsilon;
  thetaMinus <span style=color:#f92672>=</span> theta;
  thetaMinus(i) <span style=color:#f92672>-=</span> epsilon;
  gradApprox(i) <span style=color:#f92672>=</span> (J(thetaPlus) <span style=color:#f92672>-</span> J(thetaMinus))<span style=color:#f92672>/</span>(<span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>epsilon)
end;


</code></pre></div><p>检查反向传播计算出来的导数 DVec 和 上面程序计算出来的 gradApprox 相比较，如果 $gradApprox \approx DVec$ 代表反向传播的实现是正确的。</p><p>最后在使用算法学习的时候关闭梯度检验。因为梯度检验主要是为了让我们知道我们写的程序算法是否存在错误，而不是用来计算导数的，因为这种方法计算导数相比于之前的会非常慢。</p><p>总结一下：</p><ol><li>通过反向传播来计算 DVec，DVec 是每个矩阵打包展开的形式。</li><li>实现数值上的梯度检测，计算出 gradApprox。</li><li>比较 $gradApprox \approx DVec$ 是否相等或者约等于。</li><li>使用算法学习的时候记得要关闭这个梯度检验，梯度检验只在代码测试阶段进行。</li></ol><hr><h2 id=三-random-initialization>三. Random Initialization</h2><p>使用梯度下降算法的时候，需要设置 $\Theta$ 初始值。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>
optTheta <span style=color:#f92672>=</span> fminunc(<span style=color:#960050;background-color:#1e0010>@</span>costFunction, initialTheta, options)

</code></pre></div><p>调用 fminunc 函数的时候，initialTheta 如果全部初始化为0，</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>
initialTheta <span style=color:#f92672>=</span> zeros(n,<span style=color:#ae81ff>1</span>)

</code></pre></div><p>在之前的线性回归和逻辑回归中，使用梯度函数，初始值设置为0是没有问题的，但是到了神经网络里面，如果还这么设置，会出现高度冗余现象。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/73_2.png alt></p><p>假设我们有这样一个网络，其初始参数都设为0。那么我们会发现其激励 $a_1^{(2)}=a_2^{(2)}$ ,且误差 $\delta_1^{(2)}=\delta_2^{(2)}$ ,且导数 $\frac{d}{d\Theta^{(1)}<em>{01}}J(\Theta)=\frac{d}{d\Theta^{(1)}</em>{02}}J(\Theta)$ 。这就导致了在参数更新的情况下，两个参数是一样的。无论怎么重复计算其两边的激励还是一样的。</p><p>上述问题被称为，对称权重问题，也就是所有权重都是一样的。所以随机初始化是解决这个问题的方法。</p><p>我们将初始化权值 $\Theta_{ij}^{(l)}$ 的范围限定在 $[-\Phi ,\Phi ]$ 。</p><p>其代码表示如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c>
<span style=color:#f92672>%</span>If the dimensions of Theta1 is <span style=color:#ae81ff>10</span>x11, Theta2 is <span style=color:#ae81ff>10</span>x11 and Theta3 is <span style=color:#ae81ff>1</span>x11.

Theta1 <span style=color:#f92672>=</span> rand(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>11</span>) <span style=color:#f92672>*</span> (<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> INIT_EPSILON) <span style=color:#f92672>-</span> INIT_EPSILON;
Theta2 <span style=color:#f92672>=</span> rand(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>11</span>) <span style=color:#f92672>*</span> (<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> INIT_EPSILON) <span style=color:#f92672>-</span> INIT_EPSILON;
Theta3 <span style=color:#f92672>=</span> rand(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>11</span>) <span style=color:#f92672>*</span> (<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> INIT_EPSILON) <span style=color:#f92672>-</span> INIT_EPSILON;

</code></pre></div><p>rand(x，y)是随机函数，它将初始化一个0到1之间的随机实数矩阵。</p><hr><h2 id=四-总结>四. 总结</h2><p><img src=https://img.halfrost.com/Blog/ArticleImage/73_3.png alt></p><h3 id=1-准备>1. 准备</h3><p>首先，我们需要确定神经网络有多少输入单元，有多少隐藏层，每一层隐藏层又有多少个单元，还有多少输出单元。那我们怎么去选择呢？</p><ul><li>输入单元是特征向量 $x^{(i)}$ 的维度</li><li>输出单元是分类的个数</li><li>每个隐藏层的单元数通常是越多越好（必须与计算成本平衡，因为随着更多隐藏单元的增加而增加）</li><li>默认值：1个隐藏层。如果有多个隐藏层，那么建议您在每个隐藏层中都有相同数量的单元。</li></ul><p>输出单元如果是多元分类问题，输出单元需要写成矩阵的形式：</p><p>例如有3个分类， 输出单元应该写成</p><p>$$
\begin{align*}
y = \begin{bmatrix} 1\ 0\ 0 \ \end{bmatrix}
or
\begin{bmatrix} 0\ 1\ 0 \ \end{bmatrix}
or
\begin{bmatrix} 0\ 0\ 1\ \end{bmatrix}
\end{align*}
$$</p><h3 id=2-训练>2. 训练</h3><p>第一步：随机初始化权重。初始化的值是随机的，值很小，接近于零。</p><p>第二步：执行前向传播算法，对于每一个 $x^{(i)}$ 计算出假设函数 $h_\Theta(x^{(i)})$ 。</p><p>第三步：计算出代价函数 $F(\Theta)$ 。</p><p>第四步：执行反向传播算法，计算出偏导数 $\frac{\partial}{\partial\Theta_{jk}^{(l)}}F(\Theta)$ 。</p><p><img src=https://img.halfrost.com/Blog/ArticleImage/73_4.png alt></p><p>具体操作就是使用一个for循环，先将 $(x^{(1)},y^{(1)})$ 进行一次前向传播和后向传播的操作，然后再对 $(x^{(2)},y^{(2)})$ 进行相同的操作一直到 $(x^{(n)},y^{(n)})$ ，这样就能得到神经网络中每一层中每个单元对应的激励值，和每一层激励的误差 $\delta^{(l)}$ 。</p><p>第五步：利用梯度检查，对比反向传播算法计算得到的偏导数项是否与梯度检验算法计算出的导数项基本相等。<strong>检查完记得删除掉这段检查的代码</strong>。</p><p>第六步：最后我们利用梯度下降算法或者更高级的算法例如 LBFGS、共轭梯度法等，结合之前算出的偏导数项，最小化代价函数 $F(\Theta)$ 算出权值的大小 $\Theta$ 。</p><p>理想情况下，只要满足了 $h_{\Theta}(x^{(i)})\approx y^{(i)}$，就能使我们的代价函数最小。但是，代价函数 $F(\Theta)$ 不是凸的，因此我们最终可以用局部最小值代替全局最小值。</p><hr><h2 id=五-neural-networks-learning-测试>五. Neural Networks: Learning 测试</h2><h3 id=1-question-1>1. Question 1</h3><p>You are training a three layer neural network and would like to use backpropagation to compute the gradient of the cost function. In the backpropagation algorithm, one of the steps is to update</p><p>$\Delta^{(2)}<em>{ij}:=\Delta^{(2)}</em>{ij}+\delta^{(3)}<em>{i}*(a^{(2)})</em>{j}$</p><p>for every i,j. Which of the following is a correct vectorization of this step?</p><p>A. $\Delta^{(2)}:=\Delta^{(2)}+(a^{(3)})^T * \delta^{(2)} $<br>B. $\Delta^{(2)}:=\Delta^{(2)}+(a^{(2)})^T * \delta^{(3)} $<br>C. $\Delta^{(2)}:=\Delta^{(2)}+\delta^{(3)}<em>(a^{(3)})^T $<br>D. $\Delta^{(2)}:=\Delta^{(2)}+\delta^{(3)}</em>(a^{(2)})^T $</p><p>解答： D</p><h3 id=2-question-2>2. Question 2</h3><p>Suppose Theta1 is a 5x3 matrix, and Theta2 is a 4x6 matrix. You set thetaVec=[Theta1(:);Theta2(:)]. Which of the following correctly recovers Theta2?</p><p>A. reshape(thetaVec(16:39),4,6)<br>B. reshape(thetaVec(15:38),4,6)<br>C. reshape(thetaVec(16:24),4,6)<br>D. reshape(thetaVec(15:39),4,6)<br>E. reshape(thetaVec(16:39),6,4)</p><p>解答：A</p><h3 id=3-question-3>3. Question 3</h3><p>Let $J(\theta)=2\theta^3+2$ . Let $\theta=1$ , and $\epsilon=0.01$ . Use the formula $\frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}$ to numerically compute an approximation to the derivative at $\theta=1$ . What value do you get? (When $\theta=1$ , the true/exact derivati ve is $\frac{dJ(\theta)}{d\theta}=6$ .)</p><p>A.6<br>B.8<br>C.5.9998<br>D.6.0002</p><p>解答： D</p><h3 id=4-question-4>4. Question 4</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. Gradient checking is useful if we are using gradient descent as our optimization algorithm. However, it serves little purpose if we are using one of the advanced optimization methods (such as in fminunc).</p><p>B. If our neural network overfits the training set, one reasonable step to take is to increase the regularization parameter λ .</p><p>C. Using gradient checking can help verify if one&rsquo;s implementation of backpropagation is bug-free.</p><p>D. Using a large value of λ cannot hurt the performance of your neural network; the only reason we do not set λ to be too large is to avoid numerical problems.</p><p>E. For computational efficiency, after we have performed gradient checking to verify that our backpropagation code is correct, we usually disable gradient checking before using backpropagation to train the network.</p><p>F. Computing the gradient of the cost function in a neural network has the same efficiency when we use backpropagation or when we numerically compute it using the method of gradient checking.</p><p>解答：B、C、E</p><p>A.梯度检验只是用来检验我们算偏导数的算法是否正确，而不是用来计算的。<br>B.过拟合增大正则化参数 λ 正确。<br>C.梯度检验能检验反向传播算法是否正确。<br>D.正则化参数 λ 太大会导致欠拟合。<br>E.还是在说梯度检验能验证反向传播算法的正确性。<br>F.还是在说梯度检验可以用来在算法里算偏导数。</p><h3 id=5-question-5>5. Question 5</h3><p>Which of the following statements are true? Check all that apply.</p><p>A. Suppose you have a three layer network with parameters $\Theta^{(1)}$ (controlling the function mapping from the inputs to the hidden units) and $\Theta^{(2)}$ (controlling the mapping from the hidden units to the outputs). If we set all the elements of $\Theta^{(1)}$ to be 0, and all the elements of $\Theta^{(2)}$ to be 1, then this suffices for symmetry breaking, since the neurons are no longer all computing the same function of the input.</p><p>B. If we are training a neural network using gradient descent, one reasonable &ldquo;debugging&rdquo; step to make sure it is working is to plot $J(\Theta)$ as a function of the number of iterations, and make sure it is decreasing (or at least non-increasing) after each iteration.</p><p>C. Suppose you are training a neural network using gradient descent. Depending on your random initialization, your algorithm may converge to different local optima (i.e., if you run the algorithm twice with different random initializations, gradient descent may converge to two different solutions).</p><p>D. If we initialize all the parameters of a neural network to ones instead of zeros, this will suffice for the purpose of &ldquo;symmetry breaking&rdquo; because the parameters are no longer symmetrically equal to zero.</p><p>E. If we are training a neural network using gradient descent, one reasonable &ldquo;debugging&rdquo; step to make sure it is working is to plot $J(\Theta)$ as a function of the number of iterations, and make sure it is decreasing (or at least non-increasing) after each iteration.</p><p>F. Suppose we have a correct implementation of backpropagation, and are training a neural network using gradient descent. Suppose we plot $J(\Theta)$ as a function of the number of iterations, and find that it is increasing rather than decreasing. One possible cause of this is that the learning rate $\alpha$ is too large.</p><p>G. Suppose that the parameter $\Theta^{(1)}$ is a square matrix (meaning the number of rows equals the number of columns). If we replace $\Theta^{(1)}$ with its transpose $(\Theta^{(1)})^T$ , then we have not changed the function that the network is computing.</p><p>H. Suppose we are using gradient descent with learning rate $\alpha$ . For logistic regression and linear regression, $J(\Theta)$ was a convex optimization problem and thus we did not want to choose a learning rate $\alpha$ that is too large. For a neural network however, $J(\Theta)$ may not be convex, and thus choosing a very large value of $\alpha$ can only speed up convergence.</p><p>解答：B、C、F</p><p>A.一层的权重都是一样的数字不能打破对称。<br>B.迭代次数的越多，代价函数 $J(\Theta)$ 下降正确。<br>C.学习速率 $\alpha$ 太大会导致代价函数随着迭代次数的增加也增加正确。<br>D.权重全部为1也不能打破对称的。<br>E.保证 $J(\Theta)$ 随着迭代次数的增加而下降用以验证算法的正确。<br>F.同B。<br>G.矩阵的倒置一般不相等。<br>H.选择大的学习速率 $\alpha$ 会导致 $J(\Theta)$ 不收敛的。</p><hr><blockquote><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a></p><p>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a></p><p>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Backpropagation_in_Practice.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Backpropagation_in_Practice.ipynb</a></p></blockquote><img src=https://img.halfrost.com/wechat-qr-code.png></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#一-backpropagation-in-practice>一. Backpropagation in Practice</a></li><li><a href=#二-gradient-checking>二. Gradient Checking</a></li><li><a href=#三-random-initialization>三. Random Initialization</a></li><li><a href=#四-总结>四. 总结</a><ul><li><a href=#1-准备>1. 准备</a></li><li><a href=#2-训练>2. 训练</a></li></ul></li><li><a href=#五-neural-networks-learning-测试>五. Neural Networks: Learning 测试</a><ul><li><a href=#1-question-1>1. Question 1</a></li><li><a href=#2-question-2>2. Question 2</a></li><li><a href=#3-question-3>3. Question 3</a></li><li><a href=#4-question-4>4. Question 4</a></li><li><a href=#5-question-5>5. Question 5</a></li></ul></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f"><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&text=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&is_video=false&description=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f"><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-stumbleupon fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-digg fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&name=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fBackpropagation_in_Practice.ipynb%0a%20%e4%b8%80.%20Backpropagation%20in%20Practice%20%e4%b8%ba%e4%ba%86%e5%88%a9%e7%94%a8%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e7%9a%84%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%ef%bc%8c%e9%9c%80%e8%a6%81%e7%94%a8%e5%88%b0%20fminunc%20%e5%87%bd%e6%95%b0%e3%80%82%e5%85%b6%e8%be%93%e5%85%a5%e7%9a%84%e5%8f%82%e6%95%b0%e6%98%af%20%24%5ctheta%24%20%ef%bc%8c%e5%87%bd%e6%95%b0%e7%9a%84%e8%bf%94%e5%9b%9e%e5%80%bc%e6%98%af%e4%bb%a3%e4%bb%b7%e5%87%bd%e6%95%b0%20jVal%20%e5%92%8c%e5%af%bc%e6%95%b0%e5%80%bc%20gradient%e3%80%82%e7%84%b6%e5%90%8e%e5%b0%86%e8%bf%94%e5%9b%9e%e5%80%bc%e4%bc%a0%e9%80%92%e7%bb%99%e9%ab%98%e7%ba%a7%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%20fminunc%ef%bc%8c%e7%84%b6%e5%90%8e%e8%be%93%e5%87%ba%e4%b8%ba%e8%be%93%e5%85%a5%e5%80%bc%20%40costFunction%ef%bc%8c%e4%bb%a5%e5%8f%8a%20%24%5ctheta%24%20%e5%80%bc%e7%9a%84%e5%88%9d%e5%a7%8b%e5%80%bc%e3%80%82%0a%e5%85%b6%e4%b8%ad%e5%8f%82%e6%95%b0%20%24%5cTheta_1%2c%5cTheta_2%2c%5cTheta_3%2c%5ccdots%24%20%e5%92%8c%20%24D%5e%7b%281%29%7d%2cD%5e%7b%282%29%7d%2cD%5e%7b%283%29%7d%2c%5ccdots%24%20%e9%83%bd%e4%b8%ba%e7%9f%a9%e9%98%b5%ef%bc%8c%e9%82%a3%e4%b9%88%e4%b8%ba%e4%ba%86%e8%83%bd%e8%b0%83%e7%94%a8%20fminunc%20%e5%87%bd%e6%95%b0%ef%bc%8c%e6%88%91%e4%bb%ac%e8%a6%81%e5%b0%86%e5%85%b6%e5%8f%98%e6%88%90%e5%90%91%e9%87%8f%ef%bc%8c%0a%e5%81%87%e5%a6%82%e6%88%91%e4%bb%ac%20%24%5cTheta_1%2c%5cTheta_2%2c%5cTheta_3%24%20%e5%8f%82%e6%95%b0%e5%92%8c%20%24D%5e%7b%281%29%7d%2cD%5e%7b%282%29%7d%2cD%5e%7b%283%29%7d%24%20%e5%8f%82%e6%95%b0%ef%bc%8cTheta1%20%e6%98%af%20%2410%20%2a%2011%24%ef%bc%8cTheta2%20%e6%98%af%20%2410%20%2a%2011%24%ef%bc%8cTheta3%20%e6%98%af%20%241%20%2a%2011%24%e3%80%82%0a%25%20%e6%89%93%e5%8c%85%e6%88%90%e4%b8%80%e4%b8%aa%e5%90%91%e9%87%8f%20thetaVector%20%3d%20%5b%20Theta1%28%3a%29%3b%20Theta2%28%3a%29%3b%20Theta3%28%3a%29%3b%20%5d%20deltaVector%20%3d%20%5b%20D1%28%3a%29%3b%20D2%28%3a%29%3b%20D3%28%3a%29%20%5d%20%25%20%e8%a7%a3%e5%8c%85%e8%bf%98%e5%8e%9f%20Theta1%20%3d%20reshape%28thetaVector%281%3a110%29%2c10%2c11%29%20Theta2%20%3d%20reshape%28thetaVector%28111%3a220%29%2c10%2c11%29%20Theta3%20%3d%20reshape%28thetaVector%28221%3a231%29%2c1%2c11%29%20%e6%89%80%e4%bb%a5%e5%a5%97%e8%b7%af%e6%98%af%ef%bc%9a"><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2fbackpropagation_in_practice%2f&t=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ae%9e%e8%b7%b5"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu class=icon href=# onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden=true></i>Menu</a>
<a id=toc class=icon href=# onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden=true></i>TOC</a>
<a id=share class=icon href=# onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden=true></i>share</a>
<a id=top style=display:none class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i>Top</a></div></div></div><footer id=footer><div class=footer-left><p class=copyright style=float:left;margin-bottom:0><a href=https://github.com/halfrost/Halfrost-Field class=github-repo style=height:18px><span class=gadget-github></span>Star</a>
Copyright &copy;halfrost 2016 - 2021
<a href=http://www.miit.gov.cn/>鄂ICP备16014744号</a></p><br><p class="copyright statistics" style=margin-bottom:20px><span id=busuanzi_container_site_pv>Cumulative Page Views <span id=busuanzi_value_site_pv></span>| Unique Visitors <span id=busuanzi_value_site_uv></span></span></p></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script><script src=/main.min.f870a4d110314b9e50e65f8ac982dc1c9c376c8f1a5083d39c62cfc49073f011.js></script><script async src=/prism.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}};</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>