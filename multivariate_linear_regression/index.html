<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=theme-color content="#FFFFFF"><meta http-equiv=x-ua-compatible content="IE=edge"><title>多元线性回归 | prometheus</title><meta name=description content="Explore in every moment of the hard thinking"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="多元线性回归"><meta property="og:description" content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Multivariate_Linear_Regression.ipynb
 一. Multiple Features 具有多个变量的线性回归也被称为“多元线性回归”。
$x_{j}^{(i)}$: 训练集第 i 个向量中的第 j 个元素(第 i 行第 j 列)
$x^{(i)}$: 训练集第 i 个向量(第 i 行)
$ m $: 总共 m 行
$ n $: 总共 n 列
适应这些多特征的假设函数的多变量形式如下：
$$ h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} + \cdots + \theta_{n}x_{n} $$
使用矩阵乘法的定义，我们的多变量假设函数可以简洁地表示为："><meta property="og:type" content="article"><meta property="og:url" content="https://new.halfrost.com/multivariate_linear_regression/"><meta property="article:published_time" content="2018-03-20T07:47:00+00:00"><meta property="article:modified_time" content="2018-03-20T07:47:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="多元线性回归"><meta name=twitter:description content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Multivariate_Linear_Regression.ipynb
 一. Multiple Features 具有多个变量的线性回归也被称为“多元线性回归”。
$x_{j}^{(i)}$: 训练集第 i 个向量中的第 j 个元素(第 i 行第 j 列)
$x^{(i)}$: 训练集第 i 个向量(第 i 行)
$ m $: 总共 m 行
$ n $: 总共 n 列
适应这些多特征的假设函数的多变量形式如下：
$$ h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} + \cdots + \theta_{n}x_{n} $$
使用矩阵乘法的定义，我们的多变量假设函数可以简洁地表示为："><link rel=stylesheet href=/css/style-white.min.css><link rel=manifest href=/manifest.json><link rel=stylesheet href=/prism.css><link href=/images/apple-touch-icon-60x60.png rel=apple-touch-icon sizes=60x60><link href=/images/apple-touch-icon-76x76.png rel=apple-touch-icon sizes=76x76><link href=/images/apple-touch-icon-120x120.png rel=apple-touch-icon sizes=120x120><link href=/images/apple-touch-icon-152x152.png rel=apple-touch-icon sizes=152x152><link href=/images/apple-touch-icon-180x180.png rel=apple-touch-icon sizes=180x180><link href=/images/apple-touch-icon-512x512.png rel=apple-touch-icon sizes=512x512><link href=/images/apple-touch-icon-1024x1024.png rel=apple-touch-icon sizes=1024x1024><script async>if('serviceWorker'in navigator){navigator.serviceWorker.register("\/serviceworker-v1.min.a64912b78d282eab1ad3715a0943da21616e5f326f8afea27034784ad445043b.js").then(function(){if(navigator.serviceWorker.controller){console.log('Assets cached by the controlling service worker.');}else{console.log('Please reload this page to allow the service worker to handle network operations.');}}).catch(function(error){console.log('ERROR: '+error);});}else{console.log('Service workers are not supported in the current browser.');}</script><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://new.halfrost.com/images/favicon.ico><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-82753806-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class="single-max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a><a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a><a id=top-icon-tablet href=# onclick="$('html, body').animate({scrollTop:0},'fast');" style=display:none><i class="fas fa-chevron-up fa-lg"></i></a><span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://new.halfrost.com/gradient_descent/><i class="fas fa-chevron-left" aria-hidden=true onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li><li><a class=icon href=https://new.halfrost.com/computing_parameters_analytically/><i class="fas fa-chevron-right" aria-hidden=true onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li><li><a class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up" aria-hidden=true onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li><li><a class=icon href=#><i class="fas fa-share-alt" aria-hidden=true onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f"><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&text=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&title=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&is_video=false&description=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f"><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&title=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&title=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&title=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-stumbleupon" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&title=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-digg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&name=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fMultivariate_Linear_Regression.ipynb%0a%20%e4%b8%80.%20Multiple%20Features%20%e5%85%b7%e6%9c%89%e5%a4%9a%e4%b8%aa%e5%8f%98%e9%87%8f%e7%9a%84%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e4%b9%9f%e8%a2%ab%e7%a7%b0%e4%b8%ba%e2%80%9c%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e2%80%9d%e3%80%82%0a%24x_%7bj%7d%5e%7b%28i%29%7d%24%3a%20%e8%ae%ad%e7%bb%83%e9%9b%86%e7%ac%ac%20i%20%e4%b8%aa%e5%90%91%e9%87%8f%e4%b8%ad%e7%9a%84%e7%ac%ac%20j%20%e4%b8%aa%e5%85%83%e7%b4%a0%28%e7%ac%ac%20i%20%e8%a1%8c%e7%ac%ac%20j%20%e5%88%97%29%0a%24x%5e%7b%28i%29%7d%24%3a%20%e8%ae%ad%e7%bb%83%e9%9b%86%e7%ac%ac%20i%20%e4%b8%aa%e5%90%91%e9%87%8f%28%e7%ac%ac%20i%20%e8%a1%8c%29%0a%24%20m%20%24%3a%20%e6%80%bb%e5%85%b1%20m%20%e8%a1%8c%0a%24%20n%20%24%3a%20%e6%80%bb%e5%85%b1%20n%20%e5%88%97%0a%e9%80%82%e5%ba%94%e8%bf%99%e4%ba%9b%e5%a4%9a%e7%89%b9%e5%be%81%e7%9a%84%e5%81%87%e8%ae%be%e5%87%bd%e6%95%b0%e7%9a%84%e5%a4%9a%e5%8f%98%e9%87%8f%e5%bd%a2%e5%bc%8f%e5%a6%82%e4%b8%8b%ef%bc%9a%0a%24%24%20h_%7b%5ctheta%7d%28x%29%20%3d%20%5ctheta_%7b0%7d%20%2b%20%5ctheta_%7b1%7dx_%7b1%7d%20%2b%20%5ctheta_%7b2%7dx_%7b2%7d%20%2b%20%5ctheta_%7b3%7dx_%7b3%7d%20%2b%20%5ccdots%20%2b%20%5ctheta_%7bn%7dx_%7bn%7d%20%24%24%0a%e4%bd%bf%e7%94%a8%e7%9f%a9%e9%98%b5%e4%b9%98%e6%b3%95%e7%9a%84%e5%ae%9a%e4%b9%89%ef%bc%8c%e6%88%91%e4%bb%ac%e7%9a%84%e5%a4%9a%e5%8f%98%e9%87%8f%e5%81%87%e8%ae%be%e5%87%bd%e6%95%b0%e5%8f%af%e4%bb%a5%e7%ae%80%e6%b4%81%e5%9c%b0%e8%a1%a8%e7%a4%ba%e4%b8%ba%ef%bc%9a"><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&t=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><nav id=TableOfContents><ul><li><a href=#一-multiple-features>一. Multiple Features</a></li><li><a href=#二-gradient-descent-for-multiple-variables>二. Gradient Descent for Multiple Variables</a></li><li><a href=#三-gradient-descent-in-practice-i---feature-scaling>三. Gradient Descent in Practice I - Feature Scaling</a><ul><li><a href=#1-feature-scaling>1. Feature Scaling</a></li><li><a href=#2-mean-normalization>2. Mean normalization</a></li></ul></li><li><a href=#四-gradient-descent-in-practice-ii---learning-rate>四. Gradient Descent in Practice II - Learning Rate</a></li><li><a href=#五-features-and-polynomial-regression>五. Features and Polynomial Regression</a></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">多元线性回归</h1><div class=meta><div class=postdate><time datetime="2018-03-20 07:47:00 +0000 UTC" itemprop=datePublished>Mar 20</time></div><div class=article-category><i class="fas fa-archive"></i><a class=category-link href=/categories/machine-learning>Machine Learning</a>
,
<a class=category-link href=/categories/ai>AI</a></div><div class=article-tag><i class="fas fa-tag"></i><a class=tag-link href=/tags/machine-learning rel=tag>Machine Learning</a>
,
<a class=tag-link href=/tags/ai rel=tag>AI</a></div></div></header><div class=content itemprop=articleBody><blockquote><p>由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/contents.md>Github</a> 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。</p><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a><br>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a><br>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Multivariate_Linear_Regression.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Multivariate_Linear_Regression.ipynb</a></p></blockquote><h2 id=一-multiple-features>一. Multiple Features</h2><p>具有多个变量的线性回归也被称为“多元线性回归”。</p><p>$x_{j}^{(i)}$: 训练集第 i 个向量中的第 j 个元素(第 i 行第 j 列)<br>$x^{(i)}$: 训练集第 i 个向量(第 i 行)<br>$ m $: 总共 m 行<br>$ n $: 总共 n 列</p><p>适应这些多特征的假设函数的多变量形式如下：</p><p>$$ h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} + \cdots + \theta_{n}x_{n} $$</p><p>使用矩阵乘法的定义，我们的多变量假设函数可以简洁地表示为：</p><p>$$ h_{\theta}(x) = \begin{bmatrix}
\theta_{0} & \theta_{1} & \cdots & \theta_{n}
\end{bmatrix} \begin{bmatrix}
x_{0}\
x_{1}\
\vdots \
x_{n}
\end{bmatrix} = \theta^{T}x$$</p><p>其中 $ x_{0}^{(i)} = 1 (i\in 1,\cdots,m)$</p><hr><h2 id=二-gradient-descent-for-multiple-variables>二. Gradient Descent for Multiple Variables</h2><p>多个变量的梯度下降，同时更新 n 个变量。</p><p>$$ \theta_{j} := \theta_{j} - \alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_{j}$$</p><p>其中 $ j \in [0,n]$</p><hr><h2 id=三-gradient-descent-in-practice-i---feature-scaling>三. Gradient Descent in Practice I - Feature Scaling</h2><p>特征缩放包括将输入值除以输入变量的范围（即最大值减去最小值），导致新的范围仅为1。</p><p>均值归一化包括从输入变量的值中减去输入变量的平均值，从而导致输入变量的新平均值为零。</p><h3 id=1-feature-scaling>1. Feature Scaling</h3><p>特征缩放让特征值取值范围都比较一致，这样在执行梯度下降的时候，“下山的路线”会更加简单，更快的收敛。通常进行特征缩放都会把特征值缩尽量缩放到 [-1,1] 之间<strong>或者这个区间附近</strong>。</p><p>即 $ x_{i} = \frac{x_{i}}{s_{i}}$</p><h3 id=2-mean-normalization>2. Mean normalization</h3><p>$ x_{i} = \frac{x_{i} - \mu_{i}}{s_{i}}$</p><p>其中，$\mu_{i}$ 是特征值的所有值的平均值，$s_{i}$ 是值的范围（最大 - 最小），或者 $s_{i}$ 是标准偏差</p><p>当然 $x_{0} = 1$ 就不需要经过上述的处理了，因为它永远等于1，不能有均值等于0的情况。</p><hr><h2 id=四-gradient-descent-in-practice-ii---learning-rate>四. Gradient Descent in Practice II - Learning Rate</h2><p>如果学习率 $\alpha $ 太小的话，就会导致收敛速度过慢的问题。
如果学习率 $\alpha $ 太大的话，代价函数可能不会在每次迭代中都下降，甚至可能不收敛，在某种情况下，学习率 $\alpha $ 过大，也有可能出现收敛缓慢。</p><p>可以通过绘制代价函数随迭代步数变化的曲线去调试这个问题。</p><p>$\alpha $ 的取值可以从 0.001，0.003，0.01，0.03，0.1，0.3，1 这几个值去尝试，选一个最优的。</p><hr><h2 id=五-features-and-polynomial-regression>五. Features and Polynomial Regression</h2><p>可以通过改造特征值，例如合并2个特征，用 $ x_{3}$ 来表示 $ x_{1} * x_{2} $</p><p>在多项式回归中，针对 $ h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{1}^{2} + \theta_{3}x_{1}^{3} $ ，我们可以令 $ x_{2} = x_{1}^{2} , x_{3} = x_{1}^{3} $ 降低次数。</p><p>还可以考虑用根号的式子，例如选用 $ h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}\sqrt{x} $</p><p>通过上述转换以后，需要记得用<strong>特征值缩放，均值归一化，调整学习速率的方式调整一下</strong>。</p><hr><blockquote><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a></p><p>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a></p><p>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Multivariate_Linear_Regression.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Multivariate_Linear_Regression.ipynb</a></p></blockquote><img src=https://img.halfrost.com/wechat-qr-code.png></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#一-multiple-features>一. Multiple Features</a></li><li><a href=#二-gradient-descent-for-multiple-variables>二. Gradient Descent for Multiple Variables</a></li><li><a href=#三-gradient-descent-in-practice-i---feature-scaling>三. Gradient Descent in Practice I - Feature Scaling</a><ul><li><a href=#1-feature-scaling>1. Feature Scaling</a></li><li><a href=#2-mean-normalization>2. Mean normalization</a></li></ul></li><li><a href=#四-gradient-descent-in-practice-ii---learning-rate>四. Gradient Descent in Practice II - Learning Rate</a></li><li><a href=#五-features-and-polynomial-regression>五. Features and Polynomial Regression</a></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f"><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&text=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&title=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&is_video=false&description=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f"><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&title=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&title=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&title=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-stumbleupon fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&title=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-digg fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&name=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fMultivariate_Linear_Regression.ipynb%0a%20%e4%b8%80.%20Multiple%20Features%20%e5%85%b7%e6%9c%89%e5%a4%9a%e4%b8%aa%e5%8f%98%e9%87%8f%e7%9a%84%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e4%b9%9f%e8%a2%ab%e7%a7%b0%e4%b8%ba%e2%80%9c%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e2%80%9d%e3%80%82%0a%24x_%7bj%7d%5e%7b%28i%29%7d%24%3a%20%e8%ae%ad%e7%bb%83%e9%9b%86%e7%ac%ac%20i%20%e4%b8%aa%e5%90%91%e9%87%8f%e4%b8%ad%e7%9a%84%e7%ac%ac%20j%20%e4%b8%aa%e5%85%83%e7%b4%a0%28%e7%ac%ac%20i%20%e8%a1%8c%e7%ac%ac%20j%20%e5%88%97%29%0a%24x%5e%7b%28i%29%7d%24%3a%20%e8%ae%ad%e7%bb%83%e9%9b%86%e7%ac%ac%20i%20%e4%b8%aa%e5%90%91%e9%87%8f%28%e7%ac%ac%20i%20%e8%a1%8c%29%0a%24%20m%20%24%3a%20%e6%80%bb%e5%85%b1%20m%20%e8%a1%8c%0a%24%20n%20%24%3a%20%e6%80%bb%e5%85%b1%20n%20%e5%88%97%0a%e9%80%82%e5%ba%94%e8%bf%99%e4%ba%9b%e5%a4%9a%e7%89%b9%e5%be%81%e7%9a%84%e5%81%87%e8%ae%be%e5%87%bd%e6%95%b0%e7%9a%84%e5%a4%9a%e5%8f%98%e9%87%8f%e5%bd%a2%e5%bc%8f%e5%a6%82%e4%b8%8b%ef%bc%9a%0a%24%24%20h_%7b%5ctheta%7d%28x%29%20%3d%20%5ctheta_%7b0%7d%20%2b%20%5ctheta_%7b1%7dx_%7b1%7d%20%2b%20%5ctheta_%7b2%7dx_%7b2%7d%20%2b%20%5ctheta_%7b3%7dx_%7b3%7d%20%2b%20%5ccdots%20%2b%20%5ctheta_%7bn%7dx_%7bn%7d%20%24%24%0a%e4%bd%bf%e7%94%a8%e7%9f%a9%e9%98%b5%e4%b9%98%e6%b3%95%e7%9a%84%e5%ae%9a%e4%b9%89%ef%bc%8c%e6%88%91%e4%bb%ac%e7%9a%84%e5%a4%9a%e5%8f%98%e9%87%8f%e5%81%87%e8%ae%be%e5%87%bd%e6%95%b0%e5%8f%af%e4%bb%a5%e7%ae%80%e6%b4%81%e5%9c%b0%e8%a1%a8%e7%a4%ba%e4%b8%ba%ef%bc%9a"><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2fmultivariate_linear_regression%2f&t=%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu class=icon href=# onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden=true></i>Menu</a>
<a id=toc class=icon href=# onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden=true></i>TOC</a>
<a id=share class=icon href=# onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden=true></i>share</a>
<a id=top style=display:none class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i>Top</a></div></div></div><footer id=footer><div class=footer-left><p class=copyright style=float:left;margin-bottom:0><a href=https://github.com/halfrost/Halfrost-Field class=github-repo style=height:18px><span class=gadget-github></span>Star</a>
Copyright &copy;halfrost 2016 - 2021
<a href=http://www.miit.gov.cn/>鄂ICP备16014744号</a></p><br><p class="copyright statistics" style=margin-bottom:20px><span id=busuanzi_container_site_pv>Cumulative Page Views <span id=busuanzi_value_site_pv></span>| Unique Visitors <span id=busuanzi_value_site_uv></span></span></p></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script><script src=/main.min.f870a4d110314b9e50e65f8ac982dc1c9c376c8f1a5083d39c62cfc49073f011.js></script><script async src=/prism.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}};</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>