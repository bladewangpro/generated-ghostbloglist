<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=theme-color content="#FFFFFF"><meta http-equiv=x-ua-compatible content="IE=edge"><title>计算参数分析 —— 正规方程法 | prometheus</title><meta name=description content="Explore in every moment of the hard thinking"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="计算参数分析 —— 正规方程法"><meta property="og:description" content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Computing_Parameters_Analytically.ipynb
 一. Normal Equation 1. 正规方程 正规方程法相对梯度下降法，它可以一步找到最小值。而且它也不需要进行特征值的缩放。
样本集是 $ m * n $ 的矩阵，每行样本表示为 $ \vec{x^{(i)}} $ ,第 i 行第 n 列分别表示为 $ x^{(i)}{0} , x^{(i)}{1} , x^{(i)}{2} , x^{(i)}{3} \cdots x^{(i)}_{n} $, m 行向量分别表示为 $ \vec{x^{(1)}} , \vec{x^{(2)}} , \vec{x^{(3)}} , \cdots \vec{x^{(m)}} $
令
$$ \vec{x^{(i)}} = \begin{bmatrix} x^{(i)}{0}\ x^{(i)}{1}\ \vdots \ x^{(i)}_{n}\ \end{bmatrix} $$"><meta property="og:type" content="article"><meta property="og:url" content="https://new.halfrost.com/computing_parameters_analytically/"><meta property="article:published_time" content="2018-03-21T07:50:00+00:00"><meta property="article:modified_time" content="2018-03-21T07:50:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="计算参数分析 —— 正规方程法"><meta name=twitter:description content="由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 Github 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。
GitHub Repo：Halfrost-Field
Follow: halfrost · GitHub
Source: https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Computing_Parameters_Analytically.ipynb
 一. Normal Equation 1. 正规方程 正规方程法相对梯度下降法，它可以一步找到最小值。而且它也不需要进行特征值的缩放。
样本集是 $ m * n $ 的矩阵，每行样本表示为 $ \vec{x^{(i)}} $ ,第 i 行第 n 列分别表示为 $ x^{(i)}{0} , x^{(i)}{1} , x^{(i)}{2} , x^{(i)}{3} \cdots x^{(i)}_{n} $, m 行向量分别表示为 $ \vec{x^{(1)}} , \vec{x^{(2)}} , \vec{x^{(3)}} , \cdots \vec{x^{(m)}} $
令
$$ \vec{x^{(i)}} = \begin{bmatrix} x^{(i)}{0}\ x^{(i)}{1}\ \vdots \ x^{(i)}_{n}\ \end{bmatrix} $$"><link rel=stylesheet href=/css/style-white.min.css><link rel=manifest href=/manifest.json><link rel=stylesheet href=/prism.css><link href=/images/apple-touch-icon-60x60.png rel=apple-touch-icon sizes=60x60><link href=/images/apple-touch-icon-76x76.png rel=apple-touch-icon sizes=76x76><link href=/images/apple-touch-icon-120x120.png rel=apple-touch-icon sizes=120x120><link href=/images/apple-touch-icon-152x152.png rel=apple-touch-icon sizes=152x152><link href=/images/apple-touch-icon-180x180.png rel=apple-touch-icon sizes=180x180><link href=/images/apple-touch-icon-512x512.png rel=apple-touch-icon sizes=512x512><link href=/images/apple-touch-icon-1024x1024.png rel=apple-touch-icon sizes=1024x1024><script async>if('serviceWorker'in navigator){navigator.serviceWorker.register("\/serviceworker-v1.min.a64912b78d282eab1ad3715a0943da21616e5f326f8afea27034784ad445043b.js").then(function(){if(navigator.serviceWorker.controller){console.log('Assets cached by the controlling service worker.');}else{console.log('Please reload this page to allow the service worker to handle network operations.');}}).catch(function(error){console.log('ERROR: '+error);});}else{console.log('Service workers are not supported in the current browser.');}</script><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://new.halfrost.com/images/favicon.ico><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-82753806-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class="single-max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a><a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a><a id=top-icon-tablet href=# onclick="$('html, body').animate({scrollTop:0},'fast');" style=display:none><i class="fas fa-chevron-up fa-lg"></i></a><span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://new.halfrost.com/multivariate_linear_regression/><i class="fas fa-chevron-left" aria-hidden=true onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li><li><a class=icon href=https://new.halfrost.com/octave_matlab_tutorial/><i class="fas fa-chevron-right" aria-hidden=true onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li><li><a class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up" aria-hidden=true onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li><li><a class=icon href=#><i class="fas fa-share-alt" aria-hidden=true onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f"><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&text=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&title=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&is_video=false&description=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f"><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&title=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&title=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&title=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-stumbleupon" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&title=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-digg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&name=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fComputing_Parameters_Analytically.ipynb%0a%20%e4%b8%80.%20Normal%20Equation%201.%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95%e7%9b%b8%e5%af%b9%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%8c%e5%ae%83%e5%8f%af%e4%bb%a5%e4%b8%80%e6%ad%a5%e6%89%be%e5%88%b0%e6%9c%80%e5%b0%8f%e5%80%bc%e3%80%82%e8%80%8c%e4%b8%94%e5%ae%83%e4%b9%9f%e4%b8%8d%e9%9c%80%e8%a6%81%e8%bf%9b%e8%a1%8c%e7%89%b9%e5%be%81%e5%80%bc%e7%9a%84%e7%bc%a9%e6%94%be%e3%80%82%0a%e6%a0%b7%e6%9c%ac%e9%9b%86%e6%98%af%20%24%20m%20%2a%20n%20%24%20%e7%9a%84%e7%9f%a9%e9%98%b5%ef%bc%8c%e6%af%8f%e8%a1%8c%e6%a0%b7%e6%9c%ac%e8%a1%a8%e7%a4%ba%e4%b8%ba%20%24%20%5cvec%7bx%5e%7b%28i%29%7d%7d%20%24%20%2c%e7%ac%ac%20i%20%e8%a1%8c%e7%ac%ac%20n%20%e5%88%97%e5%88%86%e5%88%ab%e8%a1%a8%e7%a4%ba%e4%b8%ba%20%24%20x%5e%7b%28i%29%7d%7b0%7d%20%2c%20x%5e%7b%28i%29%7d%7b1%7d%20%2c%20x%5e%7b%28i%29%7d%7b2%7d%20%2c%20x%5e%7b%28i%29%7d%7b3%7d%20%5ccdots%20x%5e%7b%28i%29%7d_%7bn%7d%20%24%2c%20m%20%e8%a1%8c%e5%90%91%e9%87%8f%e5%88%86%e5%88%ab%e8%a1%a8%e7%a4%ba%e4%b8%ba%20%24%20%5cvec%7bx%5e%7b%281%29%7d%7d%20%2c%20%5cvec%7bx%5e%7b%282%29%7d%7d%20%2c%20%5cvec%7bx%5e%7b%283%29%7d%7d%20%2c%20%5ccdots%20%5cvec%7bx%5e%7b%28m%29%7d%7d%20%24%0a%e4%bb%a4%0a%24%24%20%5cvec%7bx%5e%7b%28i%29%7d%7d%20%3d%20%5cbegin%7bbmatrix%7d%20x%5e%7b%28i%29%7d%7b0%7d%5c%20x%5e%7b%28i%29%7d%7b1%7d%5c%20%5cvdots%20%5c%20x%5e%7b%28i%29%7d_%7bn%7d%5c%20%5cend%7bbmatrix%7d%20%24%24"><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&t=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><nav id=TableOfContents><ul><li><a href=#一-normal-equation>一. Normal Equation</a><ul><li><a href=#1-正规方程>1. 正规方程</a></li><li><a href=#2-矩阵的微分和矩阵的迹>2. 矩阵的微分和矩阵的迹</a></li><li><a href=#3-推导>3. 推导</a></li><li><a href=#4-梯度下降和正规方程法比较>4. 梯度下降和正规方程法比较：</a></li></ul></li><li><a href=#二-normal-equation-noninvertibility>二. Normal Equation Noninvertibility</a></li><li><a href=#三-linear-regression-with-multiple-variables-测试>三. Linear Regression with Multiple Variables 测试</a><ul><li><a href=#1-question-1>1. Question 1</a></li><li><a href=#2-question-2>2. Question 2</a></li><li><a href=#3-question-3>3. Question 3</a></li><li><a href=#4-question-4>4. Question 4</a></li><li><a href=#5-question-5>5. Question 5</a></li></ul></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">计算参数分析 —— 正规方程法</h1><div class=meta><div class=postdate><time datetime="2018-03-21 07:50:00 +0000 UTC" itemprop=datePublished>Mar 21</time></div><div class=article-category><i class="fas fa-archive"></i><a class=category-link href=/categories/machine-learning>Machine Learning</a>
,
<a class=category-link href=/categories/ai>AI</a></div><div class=article-tag><i class="fas fa-tag"></i><a class=tag-link href=/tags/machine-learning rel=tag>Machine Learning</a>
,
<a class=tag-link href=/tags/ai rel=tag>AI</a></div></div></header><div class=content itemprop=articleBody><blockquote><p>由于 Ghost 博客对 LateX 的识别语法和标准的 LateX 语法有差异，为了更加通用性，所以以下文章中 LateX 公式可能出现乱码，如果出现乱码，不嫌弃的话可以在笔者的 <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/contents.md>Github</a> 上看这篇无乱码的文章。笔者有空会修复这个乱码问题的。请见谅。</p><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a><br>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a><br>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Computing_Parameters_Analytically.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Computing_Parameters_Analytically.ipynb</a></p></blockquote><h2 id=一-normal-equation>一. Normal Equation</h2><h3 id=1-正规方程>1. 正规方程</h3><p>正规方程法相对梯度下降法，它可以一步找到最小值。而且它也不需要进行特征值的缩放。</p><p>样本集是 $ m * n $ 的矩阵，每行样本表示为 $ \vec{x^{(i)}} $ ,第 i 行第 n 列分别表示为 $ x^{(i)}<em>{0} , x^{(i)}</em>{1} , x^{(i)}<em>{2} , x^{(i)}</em>{3} \cdots x^{(i)}_{n} $, m 行向量分别表示为 $ \vec{x^{(1)}} , \vec{x^{(2)}} , \vec{x^{(3)}} , \cdots \vec{x^{(m)}} $</p><p>令</p><p>$$ \vec{x^{(i)}} = \begin{bmatrix} x^{(i)}<em>{0}\ x^{(i)}</em>{1}\ \vdots \ x^{(i)}_{n}\ \end{bmatrix} $$</p><p>$ \vec{x^{(i)}} $ 是这样一个 $(n+1)*1$ 维向量。每行都对应着 i 行 0-n 个变量。</p><p>再构造几个矩阵：</p><p>$$ X = \begin{bmatrix} (\vec{x^{(1)}})^{T}\ \vdots \ (\vec{x^{(m)}})^{T} \end{bmatrix} ;;;;
\Theta = \begin{bmatrix} \theta_{0}\ \theta_{1}\ \vdots \ \theta_{n}\ \end{bmatrix} ;;;;
Y = \begin{bmatrix} y^{(1)}\ y^{(2)}\ \vdots \ y^{(m)}\ \end{bmatrix}
$$</p><p>X 是一个 $ m * (n+1)$ 的矩阵，$ \Theta $ 是一个 $ (n+1) * 1$ 的向量，Y 是一个 $ m * 1$的矩阵。</p><p>对比之前代价函数中，$$ \rm{CostFunction} = \rm{F}({\theta_{0}},{\theta_{1}}) = \frac{1}{2m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2 $$</p><p>$$
\begin{align*}
X \cdot \Theta - Y =
\begin{bmatrix}
(\vec{x^{(1)}})^{T}\
\vdots \
(\vec{x^{(m)}})^{T}
\end{bmatrix} \cdot
\begin{bmatrix}
\theta_{0}\
\theta_{1}\
\vdots \
\theta_{n}\
\end{bmatrix} -
\begin{bmatrix}
y^{(1)}\
y^{(2)}\
\vdots \
y^{(m)}\
\end{bmatrix} =
\begin{bmatrix}
h_{\theta}(x^{(1)})-y^{(1)}\
h_{\theta}(x^{(2)})-y^{(2)}\
\vdots \
h_{\theta}(x^{(m)})-y^{(m)}\
\end{bmatrix}
\end{align*}$$</p><p>代入到之前代价函数中，
$$
\begin{align*}
\rm{CostFunction} = \rm{F}({\theta_{0}},{\theta_{1}}) &= \frac{1}{2m}\sum_{i = 1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2\<br>& = \frac{1}{2m} (X \cdot \Theta - Y)^{T}(X \cdot \Theta - Y)\<br>\end{align*}
$$</p><hr><h3 id=2-矩阵的微分和矩阵的迹>2. 矩阵的微分和矩阵的迹</h3><p>接下来在进行推导之前，需要引入矩阵迹的概念，因为迹是求解一阶矩阵微分的工具。</p><p>矩阵迹的定义是</p><p>$$ \rm{tr} A = \sum_{i=1}^{n}A_{ii}$$</p><p>简单的说就是左上角到右下角对角线上元素的和。</p><p>接下来有几个性质在下面推导过程中需要用到：</p><ol><li><p>$ \rm{tr};a = a $ ， a 是标量 ( $ a \in \mathbb{R} $)</p></li><li><p>$ \rm{tr};AB = \rm{tr};BA $ 更近一步 $ \rm{tr};ABC = \rm{tr};CAB = \rm{tr};BCA $<br>证明：假设 A 是 $n * m$ 矩阵， B 是 $m * n$ 矩阵，则有
$$ \rm{tr};AB = \sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji} = \sum_{j=1}^{n} \sum_{i=1}^{m}B_{ji}A_{ij}= \rm{tr};BA $$
同理：$$ \rm{tr};ABC = \rm{tr};(AB)C = \rm{tr};C(AB) = \rm{tr};CAB$$
$$ \rm{tr};ABC = \rm{tr};A(BC) = \rm{tr};(BC)A = \rm{tr};BCA$$
连起来，即 $$ \rm{tr};ABC = \rm{tr};CAB = \rm{tr};BCA $$</p></li><li><p>$ \triangledown_{A}\rm{tr};AB = \triangledown_{A}\rm{tr};BA = B^{T}$<br>证明：按照矩阵梯度的定义：
$$\triangledown_{X}f(X) = \begin{bmatrix}
\frac{\partial f(X) }{\partial x_{11}} & \cdots & \frac{\partial f(X) }{\partial x_{1n}}\
\vdots & \ddots & \vdots \
\frac{\partial f(X) }{\partial x_{m1}} & \cdots & \frac{\partial f(X) }{\partial x_{mn}}
\end{bmatrix} = \frac{\partial f(X) }{\partial X}$$
假设 A 是 $n * m$ 矩阵， B 是 $m * n$ 矩阵，则有
$$\begin{align*}\triangledown_{A}\rm{tr};AB &= \triangledown_{A} \sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji} = \frac{\partial}{\partial A}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji})\ & = \begin{bmatrix}
\frac{\partial}{\partial A_{11}}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji}) & \cdots & \frac{\partial}{\partial A_{1m}}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji})\
\vdots & \ddots & \vdots \
\frac{\partial}{\partial A_{n1}}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji}) & \cdots & \frac{\partial}{\partial A_{nm}}(\sum_{i=1}^{n}\sum_{j=1}^{m}A_{ij}B_{ji})
\end{bmatrix} \ & = \begin{bmatrix}
B_{11} & \cdots & B_{m1} \
\vdots & \ddots & \vdots \
B_{1n} & \cdots & B_{mn}
\end{bmatrix} = B^{T}\ \end{align*}$$</p><p>$$\begin{align*}\triangledown_{A}\rm{tr};BA &= \triangledown_{A} \sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji} = \frac{\partial}{\partial A}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji})\ & = \begin{bmatrix}
\frac{\partial}{\partial A_{11}}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji}) & \cdots & \frac{\partial}{\partial A_{1m}}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji})\
\vdots & \ddots & \vdots \
\frac{\partial}{\partial A_{n1}}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji}) & \cdots & \frac{\partial}{\partial A_{nm}}(\sum_{i=1}^{m}\sum_{j=1}^{n}B_{ij}A_{ji})
\end{bmatrix} \ & = \begin{bmatrix}
B_{11} & \cdots & B_{m1} \
\vdots & \ddots & \vdots \
B_{1n} & \cdots & B_{mn}
\end{bmatrix} = B^{T}\ \end{align*}$$</p><p>所以有 $ \triangledown_{A}\rm{tr};AB = \triangledown_{A}\rm{tr};BA = B^{T}$</p></li><li><p>$\triangledown_{A^{T}}a = (\triangledown_{A}a)^{T};;;; (a \in \mathbb{R})$<br>证明：假设 A 是 $n * m$ 矩阵
$$\begin{align*}\triangledown_{A^{T}}a & = \begin{bmatrix}
\frac{\partial}{\partial A_{11}}a & \cdots & \frac{\partial}{\partial A_{1n}}a\
\vdots & \ddots & \vdots \
\frac{\partial}{\partial A_{m1}}a & \cdots & \frac{\partial}{\partial A_{mn}}a
\end{bmatrix} = (\begin{bmatrix}
\frac{\partial}{\partial A_{11}}a & \cdots & \frac{\partial}{\partial A_{1m}}a\
\vdots & \ddots & \vdots \
\frac{\partial}{\partial A_{n1}}a & \cdots & \frac{\partial}{\partial A_{nm}}a
\end{bmatrix})^{T} \ & = (\triangledown_{A}a)^{T}\ \end{align*}$$</p></li><li><p>$\mathrm{d}(\rm{tr};A) = \rm{tr}(\mathrm{d}A)$
证明：<br>$$\mathrm{d}(\rm{tr};A) = \mathrm{d}(\sum_{i=1}^{n}a_{ii}) = \sum_{i=1}^{n}\mathrm{d}a_{ii} = \rm{tr}(\mathrm{d}A)$$
矩阵的迹的微分等于矩阵的微分的迹。</p></li><li><p>$\triangledown_{A}\rm{tr};ABA^{T}C = CAB + C^{T}AB^{T}$<br>证明：
根据实标量函数梯度的乘法法则：
若 f(A)、g(A)、h(A) 分别是矩阵 A 的实标量函数，则有
$$\begin{align*}\frac{\partial f(A)g(A)}{\partial A} &= g(A)\frac{\partial f(A)}{\partial A} + f(A)\frac{\partial g(A)}{\partial A}\ \frac{\partial f(A)g(A)h(A)}{\partial A} &= g(A)h(A)\frac{\partial f(A)}{\partial A} + f(A)h(A)\frac{\partial g(A)}{\partial A}+ f(A)g(A)\frac{\partial h(A)}{\partial A}\ \end{align*}$$
令 $f(A) = AB,g(A) = A^{T}C$，由性质5，矩阵的迹的微分等于矩阵的微分的迹，那么则有：
$$\begin{align*} \triangledown_{A}\rm{tr};ABA^{T}C & = \rm{tr}(\triangledown_{A}ABA^{T}C) = \rm{tr}(\triangledown_{A}f(A)g(A)) = \rm{tr}\triangledown_{A_{1}}(A_{1}BA^{T}C) + \rm{tr}\triangledown_{A_{2}}(ABA_{2}^{T}C) \ & = (BA^{T}C)^{T} + \rm{tr}\triangledown_{A_{2}}(ABA_{2}^{T}C) = C^{T}AB^{T} + \triangledown_{A_{2}}\rm{tr}(ABA_{2}^{T}C)\ & = C^{T}AB^{T} + \triangledown_{A_{2}}\rm{tr}(A_{2}^{T}CAB) = C^{T}AB^{T} + (\triangledown_{{A_{2}}^{T}};\rm{tr};A_{2}^{T}CAB)^{T} \ & = C^{T}AB^{T} + ((CAB)^{T})^{T} \ & = C^{T}AB^{T} + CAB \ \end{align*}$$</p></li></ol><hr><h3 id=3-推导>3. 推导</h3><p>回到之前的代价函数中：</p><p>$$
\rm{CostFunction} = \rm{F}({\theta_{0}},{\theta_{1}}) = \frac{1}{2m} (X \cdot \Theta - Y)^{T}(X \cdot \Theta - Y)
$$</p><p>求导：</p><p>$$
\begin{align*}
\triangledown_{\theta}\rm{F}(\theta) & = \frac{1}{2m} \triangledown_{\theta}(X \cdot \Theta - Y)^{T}(X \cdot \Theta - Y) = \frac{1}{2m}\triangledown_{\theta}(\Theta^{T}X^{T}-Y^{T})(X\Theta-Y)\<br>& = \frac{1}{2m}\triangledown_{\theta}(\Theta^{T}X^{T}X\Theta-Y^{T}X\Theta-\Theta^{T}X^{T}Y+Y^{T}Y) \ \end{align*}
$$</p><p>上式中，对 $\Theta $矩阵求导，$ Y^{T}Y $ 与 $\Theta $ 无关，所以这一项为 0 。 $Y^{T}X\Theta$ 是标量，由性质4可以知道，$Y^{T}X\Theta = (Y^{T}X\Theta)^{T} = \Theta^{T}X^{T}Y$，因为 $\Theta^{T}X^{T}X\Theta , Y^{T}X\Theta $都是标量，所以它们的也等于它们的迹，（处理矩阵微分的问题常常引入矩阵的迹），于是有</p><p>$$
\begin{align*}
\triangledown_{\theta}\rm{F}(\theta) & = \frac{1}{2m}\triangledown_{\theta}(\Theta^{T}X^{T}X\Theta-2Y^{T}X\Theta) \
& = \frac{1}{2m}\triangledown_{\theta}\rm{tr};(\Theta^{T}X^{T}X\Theta-2Y^{T}X\Theta) \ & = \frac{1}{2m}\triangledown_{\theta}\rm{tr};(\Theta\Theta^{T}X^{T}X-2Y^{T}X\Theta) \ & = \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr};\Theta\Theta^{T}X^{T}X -\triangledown_{\theta}\rm{tr};Y^{T}X\Theta) \ & = \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr};\Theta\Theta^{T}X^{T}X -(Y^{T}X)^{T}) = \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr};\Theta\Theta^{T}X^{T}X -X^{T}Y)\ \end{align*}
$$</p><p>上面第三步用的性质2矩阵迹的交换律，第五步用的性质3。</p><p>为了能进一步化简矩阵的微分，我们在矩阵的迹上面乘以一个单位矩阵，不影响结果。于是：</p><p>$$
\begin{align*}
\triangledown_{\theta}\rm{F}(\theta) & = \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr};\Theta\Theta^{T}X^{T}X -X^{T}Y) \ &= \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr};\Theta I \Theta^{T}X^{T}X -X^{T}Y) \end{align*}
$$</p><p>利用性质6 展开上面的式子，令 $ A = \Theta , B = I , C = X^{T}X $。</p><p>$$
\begin{align*}
\triangledown_{\theta}\rm{F}(\theta) &= \frac{1}{m}(\frac{1}{2}\triangledown_{\theta}\rm{tr};\Theta I \Theta^{T}X^{T}X -X^{T}Y) \ & = \frac{1}{m}(\frac{1}{2}(X^{T}X\Theta I + (X^{T}X)^{T}\Theta I^{T}) -X^{T}Y) \ & = \frac{1}{m}(\frac{1}{2}(X^{T}X\Theta I + (X^{T}X)^{T}\Theta I^{T}) -X^{T}Y) \ & = \frac{1}{m}(\frac{1}{2}(X^{T}X\Theta + X^{T}X\Theta) -X^{T}Y) = \frac{1}{m}(X^{T}X\Theta -X^{T}Y) \ \end{align*}
$$</p><p>令 $\triangledown_{\theta}\rm{F}(\theta) = 0$，即 $X^{T}X\Theta -X^{T}Y = 0$, 于是 $ X^{T}X\Theta = X^{T}Y $ ，这里假设 $ X^{T}X$ 这个矩阵是可逆的，等号两边同时左乘$ X^{T}X$的逆矩阵，得到 $\Theta = (X^{T}X)^{-1}X^{T}Y$</p><p>最终结果也就推导出来了，$$\Theta = (X^{T}X)^{-1}X^{T}Y$$</p><p>但是这里有一个<strong>前提条件是 $X^{T}X$ 是非奇异(非退化)矩阵， 即 $ \left | X^{T}X \right | \neq 0 $</strong></p><hr><h3 id=4-梯度下降和正规方程法比较>4. 梯度下降和正规方程法比较：</h3><p>优点：
梯度下降在超大数据集面前也能运行的很良好。<br>正规方程在超大数据集合面前性能会变得很差，因为需要计算 $(x^{T}x)^{-1}$,时间复杂度在 $O(n^{3})$ 这个级别。</p><p>缺点：
梯度下降需要合理的选择学习速率 $\alpha$ , 需要很多次迭代的操作去选择合理的 $\alpha$，寻找最小值的时候也需要迭代很多次才能收敛。<br>正规方程的优势相比而言，不需要选择学习速率 $\alpha$，也不需要多次的迭代或者画图检测是否收敛。</p><hr><h2 id=二-normal-equation-noninvertibility>二. Normal Equation Noninvertibility</h2><p>上一章谈到了如何利用正规方程法求解 $\Theta $,但是在线性代数中存在这样一个问题，如果是奇异(退化)矩阵，是不存在逆矩阵的。也就是说用上面正规方程的公式是不一定能求解出正确结果的。</p><p>在 Octave 软件中，存在2个求解逆矩阵的函数，一个是 pinv 和 inv。pinv (pseudo-inverse)求解的是<strong>伪逆矩阵</strong>，inv 求解的是逆矩阵，所以用 pinv 求解问题，就算是 $ X^{T}X $ 不存在逆矩阵，也一样可以得到最后的结果。</p><p>导致$ X^{T}X $ 不存在逆矩阵有2种情况：</p><ol><li>多余的特征。特征之间呈倍数关系，线性依赖。</li><li>过多的特征。当 $ m \leqslant n $ 的时候，会导致过多的特征。解决办法是删除一些特征，或者进行正则化。</li></ol><p>所以解决$ X^{T}X $ 不存在逆矩阵的办法也就是对应上面2种情况：</p><ol><li>删掉多余的特征，线性相关的，倍数关系的。直到没有多余的特征</li><li>再删除一些不影响结果的特征，或者进行正则化。</li></ol><hr><h2 id=三-linear-regression-with-multiple-variables-测试>三. Linear Regression with Multiple Variables 测试</h2><h3 id=1-question-1>1. Question 1</h3><p>Suppose m=4 students have taken some class, and the class had a midterm exam and a final exam. You have collected a dataset of their scores on the two exams, which is as follows:</p><p>midterm exam (midterm exam)2 final exam
89 7921 96
72 5184 74
94 8836 87
69 4761 78
You&rsquo;d like to use polynomial regression to predict a student&rsquo;s final exam score from their midterm exam score. Concretely, suppose you want to fit a model of the form hθ(x)=θ0+θ1x1+θ2x2, where x1 is the midterm score and x2 is (midterm score)2. Further, you plan to use both feature scaling (dividing by the &ldquo;max-min&rdquo;, or range, of a feature) and mean normalization.</p><p>What is the normalized feature x(2)2? (Hint: midterm = 72, final = 74 is training example 2.) Please round off your answer to two decimal places and enter in the text box below.</p><p>解答：
标准化 $$x = \frac{x_{2}^{2}-\frac{(7921+5184+8836+4761)}{4}}{\max - \min } = \frac{5184 - 6675.5}{8836-4761} = -0.37$$</p><h3 id=2-question-2>2. Question 2</h3><p>You run gradient descent for 15 iterations</p><p>with α=0.3 and compute J(θ) after each</p><p>iteration. You find that the value of J(θ) increases over</p><p>time. Based on this, which of the following conclusions seems</p><p>most plausible?</p><p>A. Rather than use the current value of α, it&rsquo;d be more promising to try a smaller value of α (say α=0.1).</p><p>B. α=0.3 is an effective choice of learning rate.</p><p>C. Rather than use the current value of α, it&rsquo;d be more promising to try a larger value of α (say α=1.0).</p><p>解答： A</p><p>下降太快所以a下降速率过大，a越大下降越快，a小下降慢，在本题中，代价函数快速收敛到最小值，代表此时a最合适。</p><h3 id=3-question-3>3. Question 3</h3><p>Suppose you have m=28 training examples with n=4 features (excluding the additional all-ones feature for the intercept term, which you should add). The normal equation is θ=(XTX)−1XTy. For the given values of m and n, what are the dimensions of θ, X, and y in this equation?</p><p>A. X is 28×4, y is 28×1, θ is 4×4</p><p>B. X is 28×5, y is 28×5, θ is 5×5</p><p>C. X is 28×5, y is 28×1, θ is 5×1</p><p>D. X is 28×4, y is 28×1, θ is4×1</p><p>解答： C</p><p>这里需要注意的是，题目中说了额外添加一列全部为1的，所以列数是5 。</p><h3 id=4-question-4>4. Question 4</h3><p>Suppose you have a dataset with m=50 examples and n=15 features for each example. You want to use multivariate linear regression to fit the parameters θ to our data. Should you prefer gradient descent or the normal equation?</p><p>A. Gradient descent, since it will always converge to the optimal θ.</p><p>B. Gradient descent, since (XTX)−1 will be very slow to compute in the normal equation.</p><p>C. The normal equation, since it provides an efficient way to directly find the solution.</p><p>D. The normal equation, since gradient descent might be unable to find the optimal θ.</p><p>解答： C</p><p>数据量少，选择正规方程法更加高效</p><h3 id=5-question-5>5. Question 5</h3><p>Which of the following are reasons for using feature scaling?</p><p>A. It prevents the matrix XTX (used in the normal equation) from being non-invertable (singular/degenerate).</p><p>B. It is necessary to prevent the normal equation from getting stuck in local optima.</p><p>C. It speeds up gradient descent by making it require fewer iterations to get to a good solution.</p><p>D. It speeds up gradient descent by making each iteration of gradient descent less expensive to compute.</p><p>解答： C</p><p>normal equation 不需要 Feature Scaling，排除AB， 特征缩放减少迭代数量，加快梯度下降，然而不能防止梯度下降陷入局部最优。</p><hr><blockquote><p>GitHub Repo：<a href=https://github.com/halfrost/Halfrost-Field>Halfrost-Field</a></p><p>Follow: <a href=https://github.com/halfrost>halfrost · GitHub</a></p><p>Source: <a href=https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Computing_Parameters_Analytically.ipynb>https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Computing_Parameters_Analytically.ipynb</a></p></blockquote><img src=https://img.halfrost.com/wechat-qr-code.png></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=https://books.halfrost.com/>Books</a></li><li><a href=https://github.com/halfrost/Halfrost-Field>Github</a></li><li><a href=https://halfrost.me/>About</a></li><li><a href=/index.xml>RSS</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#一-normal-equation>一. Normal Equation</a><ul><li><a href=#1-正规方程>1. 正规方程</a></li><li><a href=#2-矩阵的微分和矩阵的迹>2. 矩阵的微分和矩阵的迹</a></li><li><a href=#3-推导>3. 推导</a></li><li><a href=#4-梯度下降和正规方程法比较>4. 梯度下降和正规方程法比较：</a></li></ul></li><li><a href=#二-normal-equation-noninvertibility>二. Normal Equation Noninvertibility</a></li><li><a href=#三-linear-regression-with-multiple-variables-测试>三. Linear Regression with Multiple Variables 测试</a><ul><li><a href=#1-question-1>1. Question 1</a></li><li><a href=#2-question-2>2. Question 2</a></li><li><a href=#3-question-3>3. Question 3</a></li><li><a href=#4-question-4>4. Question 4</a></li><li><a href=#5-question-5>5. Question 5</a></li></ul></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f"><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&text=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&title=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&is_video=false&description=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95&body=Check out this article: https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f"><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&title=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&title=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.stumbleupon.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&title=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-stumbleupon fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://digg.com/submit?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&title=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-digg fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&name=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95&description=%e7%94%b1%e4%ba%8e%20Ghost%20%e5%8d%9a%e5%ae%a2%e5%af%b9%20LateX%20%e7%9a%84%e8%af%86%e5%88%ab%e8%af%ad%e6%b3%95%e5%92%8c%e6%a0%87%e5%87%86%e7%9a%84%20LateX%20%e8%af%ad%e6%b3%95%e6%9c%89%e5%b7%ae%e5%bc%82%ef%bc%8c%e4%b8%ba%e4%ba%86%e6%9b%b4%e5%8a%a0%e9%80%9a%e7%94%a8%e6%80%a7%ef%bc%8c%e6%89%80%e4%bb%a5%e4%bb%a5%e4%b8%8b%e6%96%87%e7%ab%a0%e4%b8%ad%20LateX%20%e5%85%ac%e5%bc%8f%e5%8f%af%e8%83%bd%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e5%a6%82%e6%9e%9c%e5%87%ba%e7%8e%b0%e4%b9%b1%e7%a0%81%ef%bc%8c%e4%b8%8d%e5%ab%8c%e5%bc%83%e7%9a%84%e8%af%9d%e5%8f%af%e4%bb%a5%e5%9c%a8%e7%ac%94%e8%80%85%e7%9a%84%20Github%20%e4%b8%8a%e7%9c%8b%e8%bf%99%e7%af%87%e6%97%a0%e4%b9%b1%e7%a0%81%e7%9a%84%e6%96%87%e7%ab%a0%e3%80%82%e7%ac%94%e8%80%85%e6%9c%89%e7%a9%ba%e4%bc%9a%e4%bf%ae%e5%a4%8d%e8%bf%99%e4%b8%aa%e4%b9%b1%e7%a0%81%e9%97%ae%e9%a2%98%e7%9a%84%e3%80%82%e8%af%b7%e8%a7%81%e8%b0%85%e3%80%82%0aGitHub%20Repo%ef%bc%9aHalfrost-Field%0aFollow%3a%20halfrost%20%c2%b7%20GitHub%0aSource%3a%20https%3a%2f%2fgithub.com%2fhalfrost%2fHalfrost-Field%2fblob%2fmaster%2fcontents%2fMachine_Learning%2fComputing_Parameters_Analytically.ipynb%0a%20%e4%b8%80.%20Normal%20Equation%201.%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95%e7%9b%b8%e5%af%b9%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%ef%bc%8c%e5%ae%83%e5%8f%af%e4%bb%a5%e4%b8%80%e6%ad%a5%e6%89%be%e5%88%b0%e6%9c%80%e5%b0%8f%e5%80%bc%e3%80%82%e8%80%8c%e4%b8%94%e5%ae%83%e4%b9%9f%e4%b8%8d%e9%9c%80%e8%a6%81%e8%bf%9b%e8%a1%8c%e7%89%b9%e5%be%81%e5%80%bc%e7%9a%84%e7%bc%a9%e6%94%be%e3%80%82%0a%e6%a0%b7%e6%9c%ac%e9%9b%86%e6%98%af%20%24%20m%20%2a%20n%20%24%20%e7%9a%84%e7%9f%a9%e9%98%b5%ef%bc%8c%e6%af%8f%e8%a1%8c%e6%a0%b7%e6%9c%ac%e8%a1%a8%e7%a4%ba%e4%b8%ba%20%24%20%5cvec%7bx%5e%7b%28i%29%7d%7d%20%24%20%2c%e7%ac%ac%20i%20%e8%a1%8c%e7%ac%ac%20n%20%e5%88%97%e5%88%86%e5%88%ab%e8%a1%a8%e7%a4%ba%e4%b8%ba%20%24%20x%5e%7b%28i%29%7d%7b0%7d%20%2c%20x%5e%7b%28i%29%7d%7b1%7d%20%2c%20x%5e%7b%28i%29%7d%7b2%7d%20%2c%20x%5e%7b%28i%29%7d%7b3%7d%20%5ccdots%20x%5e%7b%28i%29%7d_%7bn%7d%20%24%2c%20m%20%e8%a1%8c%e5%90%91%e9%87%8f%e5%88%86%e5%88%ab%e8%a1%a8%e7%a4%ba%e4%b8%ba%20%24%20%5cvec%7bx%5e%7b%281%29%7d%7d%20%2c%20%5cvec%7bx%5e%7b%282%29%7d%7d%20%2c%20%5cvec%7bx%5e%7b%283%29%7d%7d%20%2c%20%5ccdots%20%5cvec%7bx%5e%7b%28m%29%7d%7d%20%24%0a%e4%bb%a4%0a%24%24%20%5cvec%7bx%5e%7b%28i%29%7d%7d%20%3d%20%5cbegin%7bbmatrix%7d%20x%5e%7b%28i%29%7d%7b0%7d%5c%20x%5e%7b%28i%29%7d%7b1%7d%5c%20%5cvdots%20%5c%20x%5e%7b%28i%29%7d_%7bn%7d%5c%20%5cend%7bbmatrix%7d%20%24%24"><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnew.halfrost.com%2fcomputing_parameters_analytically%2f&t=%e8%ae%a1%e7%ae%97%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90%20%e2%80%94%e2%80%94%20%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e6%b3%95"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu class=icon href=# onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden=true></i>Menu</a>
<a id=toc class=icon href=# onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden=true></i>TOC</a>
<a id=share class=icon href=# onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden=true></i>share</a>
<a id=top style=display:none class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i>Top</a></div></div></div><footer id=footer><div class=footer-left><p class=copyright style=float:left;margin-bottom:0><a href=https://github.com/halfrost/Halfrost-Field class=github-repo style=height:18px><span class=gadget-github></span>Star</a>
Copyright &copy;halfrost 2016 - 2021
<a href=http://www.miit.gov.cn/>鄂ICP备16014744号</a></p><br><p class="copyright statistics" style=margin-bottom:20px><span id=busuanzi_container_site_pv>Cumulative Page Views <span id=busuanzi_value_site_pv></span>| Unique Visitors <span id=busuanzi_value_site_uv></span></span></p></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script><script src=/main.min.f870a4d110314b9e50e65f8ac982dc1c9c376c8f1a5083d39c62cfc49073f011.js></script><script async src=/prism.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}};</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>